{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dbc3421-5ad4-4445-bdb8-001db667b842",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id=\"table-of-contents\"></a>Table of Contents\n",
    "\n",
    "1. [Portfolio_marginal_attributes](#portfolio_marginal_attributes)\n",
    "2. [Portfolio_attributes](#portfolio_attributes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe810a4-6a80-4dbc-809a-ea8daa73e804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import trim_mean\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d5b24-ce76-4457-aeff-a966bf8fb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL INPUTS\n",
    "\n",
    "#List portfolio securities here:\n",
    "portfolio_with_candidate = [\"SPY_history.csv\", \"XLU_history.csv\", \"XLF_history.csv\"]\n",
    "portfolio_without_candidate= [\"XLF_history.csv\",\"SPY_history.csv\"]\n",
    "candidate = [\"XLU_history.csv\"]\n",
    "\n",
    "# Define weights_with_candidate (make sure the keys match the column names):\n",
    "weights_with_candidate = {\n",
    "    'Return_SPY': 0.3,  # 50% weight to SPY\n",
    "    'Return_XLU': 0.5,  # 30% weight to XLU\n",
    "    'Return_XLF': 0.2   # 20% weight to XLF\n",
    "}\n",
    "\n",
    "weights_without_candidate = {\n",
    "    \"Return_SPY\": 0.6,  # Adjust weights to exclude candidate\n",
    "    \"Return_XLF\": 0.4\n",
    "}\n",
    "\n",
    "observations_to_keep= 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a0c8e6-85e3-4e63-a8f3-2c5e9da2bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_portfolio_data(securities, weights_with_candidate):\n",
    "    # Define the Data folder path\n",
    "    data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "    \n",
    "    # Initialize an empty list to store the DataFrames\n",
    "    data_frames = []\n",
    "    \n",
    "    # Iterate over the list of security files\n",
    "    for csv in securities:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(data_folder, csv)\n",
    "        \n",
    "        # set \"data\" equal to a pandas dataframe created from the csv file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert 'Date' column to datetime format\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        \n",
    "        # Extract security name from file name (e.g., \"SPY\" from \"SPY_history.csv\")\n",
    "        security_name = csv.split('_')[0]\n",
    "        \n",
    "        # Rename columns to include security name\n",
    "        data = data.add_suffix(f'_{security_name}')\n",
    "        \n",
    "        # Calculate day-over-day price change based on 'Close' column\n",
    "        data[f'Return_{security_name}'] = data[f'Close/Last_{security_name}'].pct_change()\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        data_frames.append(data)\n",
    "    \n",
    "    # Combine all pandas DataFrames you just made into a single DataFrame\n",
    "    combined_data = pd.concat(data_frames, axis=1)\n",
    "    \n",
    "    # Calculate the portfolio's uniform daily return (average of all securities' daily returns)\n",
    "    return_columns = [col for col in combined_data.columns if col.startswith(\"Return_\")]\n",
    "    combined_data[\"portfolio_uniform_daily_return\"] = combined_data[return_columns].mean(axis=1)\n",
    "    \n",
    "    # Normalize weights to ensure they sum to 1 (just in case)\n",
    "    total_weight = sum(weights_with_candidate.values())\n",
    "    normalized_weights = {col: weight / total_weight for col, weight in weights_with_candidate.items()}\n",
    "    \n",
    "    # Calculate the portfolio's weighted daily return\n",
    "    combined_data[\"portfolio_weighted_daily_return\"] = sum(\n",
    "        combined_data[col] * weight for col, weight in normalized_weights.items() if col in combined_data.columns\n",
    "    )\n",
    "    \n",
    "    # Add Original_Index column to capture current index\n",
    "    combined_data['Original_Index'] = combined_data.index\n",
    "    \n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d11c3-392c-448a-9f4b-a6224ce27a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data for different portfolios\n",
    "portfolio_with_candidate_df = process_portfolio_data(portfolio_with_candidate, weights_with_candidate)\n",
    "portfolio_without_candidate_df = process_portfolio_data(portfolio_without_candidate, weights_with_candidate)\n",
    "candidate_df = process_portfolio_data(candidate, weights_with_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97a5e2-60d0-4592-b0fe-74e3f68dbaee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_with_candidate_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d6fc7-4f5f-4f7e-904a-aaaa30674c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_with_candidate_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9797b-d326-4020-83cd-d0955ec2b2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_with_candidate_df = pd.DataFrame(portfolio_with_candidate_df)\n",
    "\n",
    "combined_returns_df = portfolio_with_candidate_df[[\n",
    "    \"Return_SPY\", \"Return_XLU\", \"Return_XLF\"\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502f078-dc43-43ae-9acf-ebad33f53b25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add function to sort DataFrame by Original_Index\n",
    "def sort_by_original_index(data):\n",
    "    return data.sort_values(by='Original_Index', ascending=False)\n",
    "\n",
    "# Sort the DataFrames by Original_Index\n",
    "portfolio_with_candidate_df = sort_by_original_index(portfolio_with_candidate_df)\n",
    "portfolio_without_candidate_df = sort_by_original_index(portfolio_without_candidate_df)\n",
    "candidate_df = sort_by_original_index(candidate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ec02b-8f1d-42e4-bdfc-67accfa9af75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to drop observations after a specified number\n",
    "def drop_excess_observations(dataframe, observations_to_keep):\n",
    "    \"\"\"\n",
    "    Returns a new DataFrame with only the first 'observations_to_keep' rows.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Input DataFrame to truncate.\n",
    "        observations_to_keep (int): Number of rows to retain.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Truncated DataFrame.\n",
    "    \"\"\"\n",
    "    return dataframe.iloc[:observations_to_keep].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "portfolio_with_candidate_df_truncated = drop_excess_observations(portfolio_with_candidate_df, observations_to_keep)\n",
    "portfolio_without_candidate_df_truncated = drop_excess_observations(portfolio_without_candidate_df, observations_to_keep)\n",
    "candidate_df_truncated = drop_excess_observations(candidate_df, observations_to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034c92d-ca45-4f44-aca3-ac1983eb192a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_weighted_returns(dataframe, weights_with_candidate, weights_without_candidate):\n",
    "    \"\"\"\n",
    "    Calculates weighted portfolio returns for two scenarios: \n",
    "    with candidate and without candidate.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame containing security return columns.\n",
    "        weights_with_candidate (dict): Weights for the portfolio with candidate included.\n",
    "        weights_without_candidate (dict): Weights for the portfolio without candidate included.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with two new columns for the weighted portfolio returns.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # Calculate weighted return with candidate\n",
    "    dataframe[\"weighted_return_with_candidate\"] = sum(\n",
    "        dataframe[security] * weight\n",
    "        for security, weight in weights_with_candidate.items()\n",
    "        if security in dataframe.columns\n",
    "    )\n",
    "\n",
    "    # Calculate weighted return without candidate\n",
    "    dataframe[\"weighted_return_without_candidate\"] = sum(\n",
    "        dataframe[security] * weight\n",
    "        for security, weight in weights_without_candidate.items()\n",
    "        if security in dataframe.columns\n",
    "    )\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21f78a-6097-47b4-8e99-0ce41f78f4eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_returns_df = calculate_weighted_returns(\n",
    "    dataframe=portfolio_with_candidate_df,\n",
    "    weights_with_candidate=weights_with_candidate,\n",
    "    weights_without_candidate=weights_without_candidate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2c2ac-c844-4f6a-bbd9-d2d6750703dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot both distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(\n",
    "    combined_returns_df[\"weighted_return_with_candidate\"].dropna(),\n",
    "    bins=50, kde=True, color=\"blue\", label=\"With Candidate\", alpha=0.4\n",
    ")\n",
    "sns.histplot(\n",
    "    combined_returns_df[\"weighted_return_without_candidate\"].dropna(),\n",
    "    bins=50, kde=True, color=\"green\", label=\"Without Candidate\", alpha=0.4\n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "plt.legend(fontsize=10)\n",
    "plt.title(\"Comparison of Portfolio Weighted Return Distributions\")\n",
    "plt.xlabel(\"Daily Weighted Return\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07fdc36-9323-4046-90fa-295fd55009c0",
   "metadata": {},
   "source": [
    "## Portfolio_marginal_attributes <a id=\"portfolio_marginal_attributes\"></a>\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456fd1e-f605-4fb6-afd8-1015fc340f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_weighted_statistics(dataframe, portfolio_list, weights_dictionary):\n",
    "    \"\"\"\n",
    "    Calculates the weighted mean, standard deviation, skewness, and kurtosis \n",
    "    for the portfolio, rescaling weights to exclude cash holdings.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the rescaled weighted statistics for the portfolio.\n",
    "    \"\"\"\n",
    "    # Validate input types\n",
    "    if not isinstance(portfolio_list, list):\n",
    "        raise ValueError(\"Expected portfolio_list to be a list of filenames.\")\n",
    "\n",
    "    # Initialize variables to store weighted statistics\n",
    "    weighted_mean = 0\n",
    "    weighted_std_dev = 0\n",
    "    weighted_skewness = 0\n",
    "    weighted_kurtosis = 0\n",
    "\n",
    "    # Extract security names dynamically from the portfolio list\n",
    "    securities = [csv.split('_')[0] for csv in portfolio_list]  # Extract 'SPY', 'XLU', etc.\n",
    "\n",
    "    # Map the securities to their weight keys in the dictionary\n",
    "    weight_keys = [f\"Return_{security}\" for security in securities]\n",
    "\n",
    "    # Rescale the weights to exclude cash (normalize weights to sum to 1)\n",
    "    invested_weights = {key: weights_dictionary.get(key, 0) for key in weight_keys}  # Match weights using correct keys\n",
    "\n",
    "    total_invested_weight = sum(invested_weights.values())  # Calculate total invested weight\n",
    "\n",
    "\n",
    "    # Avoid division by zero (e.g., no invested securities)\n",
    "    if total_invested_weight > 0:\n",
    "        normalized_weights = {key: weight / total_invested_weight for key, weight in invested_weights.items()}\n",
    "        print(\"Normalized Weights:\", normalized_weights)  # Print the normalized weights\n",
    "    else:\n",
    "        print(\"Warning: Total invested weight is zero. Cannot rescale weights.\")\n",
    "        return None  # Return None or raise an exception\n",
    "    \n",
    "    # Iterate over securities and calculate weighted statistics\n",
    "    for security, weight_key in zip(securities, weight_keys):\n",
    "        # Ensure the Close/Last column for the security exists\n",
    "        close_col = f\"Close/Last_{security}\"\n",
    "        \n",
    "        if close_col in dataframe.columns:\n",
    "            # Drop NaN values from the close column\n",
    "            close_data = dataframe[close_col].dropna()\n",
    "\n",
    "            # Calculate individual statistics\n",
    "            mean = close_data.mean()\n",
    "            std_dev = close_data.std()\n",
    "            skewness = skew(close_data)\n",
    "            kurt = kurtosis(close_data)\n",
    "\n",
    "            # Fetch the normalized weight for this security\n",
    "            weight = normalized_weights.get(weight_key, 0)  # Default to 0 if not found\n",
    "\n",
    "            # Apply weights to the metrics\n",
    "            weighted_mean += mean * weight\n",
    "            weighted_std_dev += std_dev * weight\n",
    "            weighted_skewness += skewness * weight\n",
    "            weighted_kurtosis += kurt * weight\n",
    "        else:\n",
    "            print(f\"Warning: Column {close_col} not found in the DataFrame.\")\n",
    "    \n",
    "    # Combine all weighted metrics into a dictionary\n",
    "    weighted_statistics = {\n",
    "        'weighted_mean': weighted_mean,\n",
    "        'weighted_std_dev': weighted_std_dev,\n",
    "        'weighted_skewness': weighted_skewness,\n",
    "        'weighted_kurtosis': weighted_kurtosis\n",
    "    }\n",
    "\n",
    "    return weighted_statistics\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0b147e3-b0c4-48de-a575-353014583d79",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(calculate_weighted_statistics(portfolio_with_candidate_df, portfolio_with_candidate, weights_with_candidate))\n",
    "print(calculate_weighted_statistics(portfolio_without_candidate_df, portfolio_with_candidate, weights_with_candidate))\n",
    "print(calculate_weighted_statistics(candidate_df, portfolio_with_candidate, weights_with_candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f620a-1dd6-4d19-8b80-dda45230945e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_weighted_return_distributions(returns_with_candidate, returns_without_candidate, stats):\n",
    "    \"\"\"\n",
    "    Plots the PDF (kernel density estimate) with histograms underneath for two distributions: \n",
    "    \"With Candidate\" and \"Without Candidate\".\n",
    "\n",
    "    Args:\n",
    "        returns_with_candidate (pd.Series): Weighted returns for \"With Candidate\" portfolio.\n",
    "        returns_without_candidate (pd.Series): Weighted returns for \"Without Candidate\" portfolio.\n",
    "        stats (dict): Dictionary containing mean, std dev, kurtosis, and skew for each portfolio.\n",
    "    \"\"\"\n",
    "    # Create a dictionary for the data\n",
    "    returns_data = {\n",
    "        \"With Candidate\": returns_with_candidate.dropna(),\n",
    "        \"Without Candidate\": returns_without_candidate.dropna()\n",
    "    }\n",
    "    \n",
    "    # Initialize a figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = [\"blue\", \"green\"]\n",
    "    \n",
    "    # Overlay histograms and density plots\n",
    "    for (label, data), color in zip(returns_data.items(), colors):\n",
    "        sns.histplot(data, bins=50, kde=True, color=color, label=f\"{label}\\n\"\n",
    "            f\"Mean: {stats[label]['Mean']:.4f}\\n\"\n",
    "            f\"Std Dev: {stats[label]['Std Dev']:.4f}\\n\"\n",
    "            f\"Kurtosis: {stats[label]['Kurtosis']:.4f}\\n\"\n",
    "            f\"Skew: {stats[label]['Skew']:.4f}\",\n",
    "            alpha=0.4)  # Adjust opacity for histograms\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.title(\"Portfolio Weighted Return Distributions\")\n",
    "    plt.xlabel(\"Daily Weighted Return\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70f12963-dd32-49c2-905a-b5b530e3bbbd",
   "metadata": {},
   "source": [
    "THIS IS STILL NOT USING THE calculate_weighted_statistics FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b5f08-0042-466b-a558-3bf1aea02a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weighted_returns_data = {\n",
    "    \"With Candidate\": portfolio_with_candidate_df[\"portfolio_weighted_daily_return\"],\n",
    "    \"Without Candidate\": portfolio_without_candidate_df[\"portfolio_weighted_daily_return\"]\n",
    "}\n",
    "\n",
    "stats = {\n",
    "    label: {\n",
    "        \"Mean\": np.mean(data.dropna()),\n",
    "        \"Std Dev\": np.std(data.dropna()),\n",
    "        \"Kurtosis\": kurtosis(data.dropna()),\n",
    "        \"Skew\": skew(data.dropna())\n",
    "    }\n",
    "    for label, data in weighted_returns_data.items()\n",
    "}\n",
    "\n",
    "# Call the adjusted function\n",
    "plot_weighted_return_distributions(\n",
    "    returns_with_candidate=portfolio_with_candidate_df[\"portfolio_weighted_daily_return\"],\n",
    "    returns_without_candidate=portfolio_without_candidate_df[\"portfolio_weighted_daily_return\"],\n",
    "    stats=stats\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32c63a-d741-4962-8513-13253b084f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trimmed_std_dev(data, trim_percent=0.02):\n",
    "    \"\"\"\n",
    "    Calculate the trimmed standard deviation for a portfolio's average daily return.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing daily returns of all securities\n",
    "    - trim_percent (float): Percentage of observations to trim from each end (default 10%)\n",
    "\n",
    "    Returns:\n",
    "    - float: Trimmed standard deviation of the portfolio\n",
    "    \"\"\"\n",
    "    # Select only return columns (avoid including other numerical data)\n",
    "    return_columns = [col for col in data.columns if col.startswith(\"Return_\")]\n",
    "    if not return_columns:\n",
    "        raise ValueError(\"No return columns found in the dataset!\")\n",
    "\n",
    "    # Compute the portfolio's daily return (average of all securities' daily returns)\n",
    "    data[\"portfolio_uniform_daily_return\"] = data[return_columns].mean(axis=1)\n",
    "\n",
    "    # Extract the portfolio daily returns as a series\n",
    "    portfolio_returns = data[\"portfolio_uniform_daily_return\"].dropna().values\n",
    "\n",
    "    # Trim the extreme observations\n",
    "    trim_count = int(len(portfolio_returns) * trim_percent)\n",
    "    sorted_returns = np.sort(portfolio_returns)\n",
    "    trimmed_returns = sorted_returns[trim_count:-trim_count]  # Trim bottom & top values\n",
    "\n",
    "    # Compute and return standard deviation of the trimmed dataset\n",
    "    return np.std(trimmed_returns, ddof=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6fa6f-7d8e-4297-972a-f82018a24103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute trimmed standard deviations\n",
    "trimmed_std_portfolio_without_candidate = trimmed_std_dev(portfolio_without_candidate_df, trim_percent=0.02)\n",
    "trimmed_std_portfolio_with_candidate = trimmed_std_dev(portfolio_with_candidate_df, trim_percent=0.02)\n",
    "trimmed_std_candidate = trimmed_std_dev(candidate_df, trim_percent=0.02)\n",
    "\n",
    "# Print results\n",
    "print(\"Trimmed Std Dev (Portfolio without Candidate):\", trimmed_std_portfolio_without_candidate)\n",
    "print(\"Trimmed Std Dev (Candidate Security):\", trimmed_std_candidate)\n",
    "print(\"Trimmed Std Dev (Portfolio with Candidate):\", trimmed_std_portfolio_with_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd344087-7377-4f38-a36d-034d8055c877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gini_mean_difference(data, column):\n",
    "    \"\"\"\n",
    "    Calculate the Gini mean difference for a specified column in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        column (str): The column name for which to calculate the Gini mean difference.\n",
    "        \n",
    "    Returns:\n",
    "        float: The Gini mean difference.\n",
    "    \"\"\"\n",
    "    # Extract the specified column values\n",
    "    values = data[column].dropna().values\n",
    "    \n",
    "    # Calculate the absolute differences between all pairs of elements\n",
    "    diff_matrix = np.abs(np.subtract.outer(values, values))\n",
    "    \n",
    "    # Calculate the mean of the absolute differences\n",
    "    gini_mean_diff = np.mean(diff_matrix)\n",
    "    \n",
    "    return gini_mean_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e05d77-931a-4c84-9de3-a3db4e0480aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gini_portfolio_with_candidate = gini_mean_difference(portfolio_with_candidate_df, 'portfolio_uniform_daily_return')\n",
    "gini_portfolio_without_candidate = gini_mean_difference(portfolio_without_candidate_df, 'portfolio_uniform_daily_return')\n",
    "gini_candidate = gini_mean_difference(candidate_df, 'portfolio_uniform_daily_return')\n",
    "\n",
    "print(f'Gini Mean Difference (without candidate): {gini_portfolio_without_candidate}')\n",
    "print(f'Gini Mean Difference (candidate): {gini_candidate}')\n",
    "print(f'Gini Mean Difference (with candidate): {gini_portfolio_with_candidate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a3c34-cb41-4fa5-9e7d-64a89a3fa51f",
   "metadata": {},
   "source": [
    "DOUBLE CHECK THE STATIONARITY OF GINI MEAN COEFFICENT AND TRIMMED STANDARD DEVIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f7aeb-2bcc-49e4-bfec-5fc2bc164842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_clustered_bar_chart_with_labels(gini_values, trimmed_std_values, labels):\n",
    "    \"\"\"\n",
    "    Plots a clustered bar chart with value labels for Gini mean difference\n",
    "    and trimmed standard deviation for three portfolios.\n",
    "\n",
    "    Args:\n",
    "        gini_values (list): A list of Gini mean differences for the portfolios.\n",
    "        trimmed_std_values (list): A list of trimmed standard deviations for the portfolios.\n",
    "        labels (list): A list of labels for the portfolios.\n",
    "    \"\"\"\n",
    "    # Number of portfolios\n",
    "    n_portfolios = len(labels)\n",
    "\n",
    "    # Bar positions\n",
    "    x = np.arange(n_portfolios)  # X-axis positions for the groups\n",
    "    bar_width = 0.35  # Width of each bar\n",
    "\n",
    "    # Plot bars\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    gini_bars = plt.bar(x - bar_width / 2, gini_values, width=bar_width, label='Gini Mean Difference', color='skyblue')\n",
    "    std_bars = plt.bar(x + bar_width / 2, trimmed_std_values, width=bar_width, label='Trimmed Std Dev', color='lightcoral')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title('Comparison of Portfolio Statistics')\n",
    "    plt.xlabel('Portfolios')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xticks(x, labels)  # Set portfolio labels for x-axis ticks\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # Add value labels to the bars\n",
    "    for bar in gini_bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    for bar in std_bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define the calculated values\n",
    "gini_values = [gini_portfolio_with_candidate, gini_portfolio_without_candidate, gini_candidate]\n",
    "trimmed_std_values = [trimmed_std_portfolio_with_candidate, trimmed_std_portfolio_without_candidate, trimmed_std_candidate]\n",
    "labels = [\"With Candidate\", \"Without Candidate\", \"Candidate Only\"]\n",
    "\n",
    "# Call the function to plot the bar chart\n",
    "plot_clustered_bar_chart_with_labels(gini_values, trimmed_std_values, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fb35d-d136-454e-9d56-44b0d3e34b84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Portfolio_attributes <a id=\"portfolio_attributes\"></a>\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bfd51-81da-42da-8fc2-d04933265b1b",
   "metadata": {},
   "source": [
    "Shape of the Distribution (Skewness, Kurtosis, Standard Deviation, Mean):\n",
    "Since you're working with returns, which are essentially first-differenced prices, you're correct that they've already been detrended to some extent. This makes the assumption of mean-variance stationarity less critical.\n",
    "\n",
    "Even though financial returns can still exhibit non-stationary behavior (e.g., volatility clustering), their distributional properties (like skewness and kurtosis) are relatively stable over time if calculated over a large enough sample.\n",
    "\n",
    "In this case, you don't necessarily need to account for stationarity explicitly unless your analysis spans vastly different market conditions (like a bull market vs. a bear market).\n",
    "\n",
    "You're absolutely right that for metrics like beta and covariance, which depend on relationships between securities, stationarity is more crucial. If the underlying data isn't stationary, these metrics could fluctuate unpredictably over time, making them unreliable.\n",
    "\n",
    "\n",
    "You're absolutely correct that if your focus is on the stability of variance, covariance, and beta, the concept of mean stationarity isn't particularly relevant.Don't use dickey-fuller or ADF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc161c7-ac90-48a5-aae2-b1f4e2a01604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the \"Close/Last_\" columns\n",
    "close_columns = [col for col in portfolio_with_candidate_df.columns if col.startswith('Close/Last_')]\n",
    "\n",
    "# Calculate daily percentage returns\n",
    "returns = portfolio_with_candidate_df[close_columns].pct_change()\n",
    "\n",
    "# Drop NaN values (from the first row caused by pct_change)\n",
    "returns = returns.dropna()\n",
    "\n",
    "# 1. Correlation Matrix\n",
    "correlation_matrix = returns.corr()\n",
    "\n",
    "# 2. Covariance Matrix\n",
    "covariance_matrix = returns.cov()\n",
    "\n",
    "# 3. Beta Matrix - Adjust to have the same shape as correlation and covariance matrices\n",
    "benchmark = close_columns[0]\n",
    "betas = {}\n",
    "\n",
    "# Initialize beta_matrix with NaN values to match the size of the correlation and covariance matrices\n",
    "beta_matrix = pd.DataFrame(np.nan, index=close_columns, columns=close_columns)\n",
    "\n",
    "for col in close_columns:\n",
    "    for row in close_columns:\n",
    "        if col == row:\n",
    "            beta_matrix.loc[row, col] = 1\n",
    "        else:\n",
    "            beta = covariance_matrix.loc[col, benchmark] / covariance_matrix.loc[benchmark, benchmark] \n",
    "            beta_matrix.loc[col, benchmark] = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ffb56-cecf-481a-8488-a03536f3211c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a conversion factor for covariance (e.g., multiplying by 1e6 for 'mu' units)\n",
    "covariance_conversion_factor = 1e6  # Adjust this based on what \"mu\" represents\n",
    "\n",
    "# Apply conversion factor to covariance matrix\n",
    "covariance_matrix_mu = covariance_matrix * covariance_conversion_factor\n",
    "\n",
    "# Store matrices in a dictionary for dynamic plotting\n",
    "matrices = {\n",
    "    \"Correlation Matrix\": correlation_matrix,\n",
    "    \"Covariance Matrix (mu)\": covariance_matrix_mu,  # Use converted covariance matrix\n",
    "    \"Beta Matrix\": beta_matrix\n",
    "}\n",
    "\n",
    "num_matrices = len(matrices)  # Adjust based on how many matrices you have\n",
    "\n",
    "# Create subplots dynamically\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=num_matrices,  \n",
    "    subplot_titles=list(matrices.keys()),  # Dynamically set titles\n",
    "    column_widths=[1/num_matrices] * num_matrices,  \n",
    "    shared_yaxes=True,\n",
    "    shared_xaxes=True\n",
    ")\n",
    "\n",
    "# Adjust x positions for colorbars below each heatmap, adding some spacing\n",
    "x_positions = np.linspace(0.15, 0.85, num_matrices)  # Spread them evenly from left to right with spacing\n",
    "\n",
    "# Add each matrix as a heatmap dynamically\n",
    "for i, (title, matrix) in enumerate(matrices.items(), start=1):\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=matrix.values,\n",
    "            x=matrix.columns,\n",
    "            y=matrix.columns,\n",
    "            colorscale=\"RdBu\",\n",
    "            colorbar=dict(\n",
    "                title=title.split()[0],  # Use first word of title (Correlation, Covariance, Beta)\n",
    "                tickvals=[matrix.values.min(), 0, matrix.values.max()],\n",
    "                yanchor=\"top\",\n",
    "                y=-0.25,  # Move colorbar slightly below the heatmap\n",
    "                x=x_positions[i - 1],  # Align it under each respective heatmap with spacing\n",
    "                xanchor=\"center\",\n",
    "                orientation=\"v\"  # Make the colorbar vertical\n",
    "            ),\n",
    "            text=matrix.values.round(2),\n",
    "            texttemplate=\"%{text}\",\n",
    "            showscale=True,\n",
    "            hoverinfo=\"skip\"\n",
    "        ),\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Correlation, Covariance (mu), and Beta Matrices\",\n",
    "    height=750,  # Increased height to accommodate vertical legends\n",
    "    showlegend=False,\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "# Show the interactive heatmap\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
