{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9ed79e-112a-418f-b955-5c13dd46dd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import gaussian_kde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2982057a-32e3-43e6-b761-6240815efa25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tickers from CSV cache...\n",
      "(503, 8)\n",
      "Index(['symbol', 'name', 'sector', 'subSector', 'headQuarter',\n",
      "       'dateFirstAdded', 'cik', 'founded'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "\n",
    "# THIS pulls sector and subsector info either localy if it's cached or from the api\n",
    "\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "\n",
    "# --- LOAD FROM CACHE OR FETCH ---\n",
    "if os.path.exists(tickers_csv_file):\n",
    "    print(\"Loading tickers from CSV cache...\")\n",
    "    df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "else:\n",
    "    print(\"Fetching tickers from API...\")\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/sp500_constituent?apikey={API_KEY}\"\n",
    "    df_sp500 = pd.DataFrame(requests.get(url).json())\n",
    "\n",
    "    # Save to CSV\n",
    "    df_sp500.to_csv(tickers_csv_file, index=False)\n",
    "    print(f\"Saved {len(df_sp500)} tickers to CSV cache.\")\n",
    "\n",
    "    \n",
    "# --- PREVIEW ---\n",
    "print(df_sp500.shape)\n",
    "print(df_sp500.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d2a811-89dd-4a1d-b195-4f3af35f41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85f8ff54-08c1-4898-b7cc-5d6264566d2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GETS THE PE DATA\n",
    "\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "# Load tickers\n",
    "df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()\n",
    "\n",
    "# Output file\n",
    "output_file = os.path.join(data_folder, \"price_and_earnings.json\")\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Get current price\n",
    "            quote_url = f\"https://financialmodelingprep.com/api/v3/quote/{ticker}?apikey={API_KEY}\"\n",
    "            price_data = requests.get(quote_url).json()\n",
    "            if not price_data:\n",
    "                continue\n",
    "            price = price_data[0][\"price\"]\n",
    "\n",
    "            # Get latest annual income statement (EPS or netIncome)\n",
    "            income_url = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?limit=1&apikey={API_KEY}\"\n",
    "            income_data = requests.get(income_url).json()\n",
    "            if not income_data:\n",
    "                continue\n",
    "            eps = income_data[0].get(\"eps\")\n",
    "            net_income = income_data[0].get(\"netIncome\")\n",
    "\n",
    "            records.append({\n",
    "                \"symbol\": ticker,\n",
    "                \"price\": price,\n",
    "                \"eps\": eps,\n",
    "                \"netIncome\": net_income\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "        time.sleep(0.2)  # polite rate limit\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "price_earnings_df = fetch_price_and_earnings(tickers, output_file)\n",
    "\n",
    "price_earnings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d8a247-31e3-486c-a090-35f4860c348f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/price_and_earnings.json\n"
     ]
    }
   ],
   "source": [
    "# GETS THE PE DATA\n",
    "\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "# Load tickers\n",
    "df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()\n",
    "\n",
    "# Output file\n",
    "output_file = os.path.join(data_folder, \"price_and_earnings.json\")\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Get current price\n",
    "            quote_url = f\"https://financialmodelingprep.com/api/v3/quote/{ticker}?apikey={API_KEY}\"\n",
    "            price_data = requests.get(quote_url).json()\n",
    "            if not price_data:\n",
    "                continue\n",
    "            price = price_data[0][\"price\"]\n",
    "            price_date = price_data[0].get(\"date\")  # trading date\n",
    "\n",
    "            # Get latest annual income statement\n",
    "            income_url = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?limit=1&apikey={API_KEY}\"\n",
    "            income_data = requests.get(income_url).json()\n",
    "            if not income_data:\n",
    "                continue\n",
    "            eps = income_data[0].get(\"eps\")\n",
    "            net_income = income_data[0].get(\"netIncome\")\n",
    "            report_date = income_data[0].get(\"date\")  # fiscal period end date\n",
    "\n",
    "            records.append({\n",
    "                \"symbol\": ticker,\n",
    "                \"price\": price,\n",
    "                \"price_date\": price_date,\n",
    "                \"eps\": eps,\n",
    "                \"netIncome\": net_income,\n",
    "                \"date\": report_date\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "        time.sleep(0.2)  # polite rate limit\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "price_earnings_df = fetch_price_and_earnings(tickers, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb737c07-dc25-4514-858c-f5178907b5be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'price_earnings_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#CLEANS AND TAKES LOG OF THE PE DATA\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Only keep rows with positive EPS\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pe_data \u001b[38;5;241m=\u001b[39m \u001b[43mprice_earnings_df\u001b[49m[price_earnings_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Compute log(PE)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m pe_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_PE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(pe_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m pe_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'price_earnings_df' is not defined"
     ]
    }
   ],
   "source": [
    "#CLEANS AND TAKES LOG OF THE PE DATA\n",
    "\n",
    "# Only keep rows with positive EPS\n",
    "pe_data = price_earnings_df[price_earnings_df[\"eps\"] > 0].copy()\n",
    "\n",
    "# Compute log(PE)\n",
    "pe_data[\"log_PE\"] = np.log(pe_data[\"price\"] / pe_data[\"eps\"])\n",
    "\n",
    "# Optional: store just the series if you want\n",
    "pe_series = pe_data[\"log_PE\"]\n",
    "\n",
    "# Print row count for reference\n",
    "print(f\"Rows used: {len(pe_series)}\")\n",
    "print(pe_series.head())\n",
    "print(pe_data.shape)\n",
    "\n",
    "# we are only pulling the eps and takingthe log here.. the code that follows doesn't immediately and directly build on this code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031400b-6a30-4ff6-ab53-72963cfe5a96",
   "metadata": {},
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1844560d-77dc-4f2a-8993-96bb263662be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FETCHES INCOME STATEMENT AND BS STATEMENT AND CF STATEMENT\n",
    "def fetch_statement(endpoint, tickers, period, limit, data_folder):\n",
    "    \"\"\"Fetch statements with unique JSON filename based on endpoint, period, limit.\"\"\"\n",
    "    output_file = os.path.join(\n",
    "        data_folder,\n",
    "        f\"{endpoint}_{period}_limit{limit}.json\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        with open(output_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        url = f\"https://financialmodelingprep.com/api/v3/{endpoint}/{ticker}?period={period}&limit={limit}&apikey={API_KEY}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                for row in data:\n",
    "                    row[\"symbol\"] = ticker\n",
    "                records.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker} ({endpoint}): {e}\")\n",
    "        time.sleep(.2)  # API polite rate limit\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "    # \"RECORDS HERE IS A LIST OF STRINGS NOT A DF\"\n",
    "    return records\n",
    "\n",
    "# AT THIS POINT INCOME_DATA_2_YEARS IS STILL A LIST OF STRINGS STORED AS A VAR\n",
    "income_data_2_years   = fetch_statement(\"income-statement\", tickers, \"annual\", 2, data_folder)\n",
    "balance_data_2_years  = fetch_statement(\"balance-sheet-statement\", tickers, \"annual\", 2, data_folder)\n",
    "cashflow_data_2_years = fetch_statement(\"cash-flow-statement\", tickers, \"annual\", 2, data_folder)\n",
    "\n",
    "# INCOME_DATA_2_YEARS IS CONVERTED TO A DF\n",
    "income_data_2_years   = pd.DataFrame(income_data_2_years)\n",
    "balance_data_2_years  = pd.DataFrame(balance_data_2_years)\n",
    "cashflow_data_2_years = pd.DataFrame(cashflow_data_2_years)\n",
    "\n",
    "print(\"Income shape:\", income_data_2_years.shape)\n",
    "print(\"Balance shape:\", balance_data_2_years.shape)\n",
    "print(\"Cash flow shape:\", cashflow_data_2_years.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e9e1d-7967-4d9d-bedd-c234025a3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_symbol_date(df):\n",
    "    return df.sort_values([\"symbol\", \"date\"])\n",
    "\n",
    "\n",
    "def compute_yoy_growth(df, exclude_cols=[\"symbol\", \"date\",\"link\",\"finalLink\"]):\n",
    "    numeric_cols = df.select_dtypes(include=[float, int]).columns\n",
    "    numeric_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "    \n",
    "    df_growth = df.copy()\n",
    "    for col in numeric_cols:\n",
    "        df_growth[col + \"_yoy\"] = df.groupby(\"symbol\")[col].pct_change()\n",
    "    \n",
    "    return df_growth\n",
    "\n",
    "income_sorted = sort_by_symbol_date(income_data_2_years)\n",
    "balance_sorted = sort_by_symbol_date(balance_data_2_years)\n",
    "cashflow_sorted = sort_by_symbol_date(cashflow_data_2_years)\n",
    "\n",
    "income_growth = compute_yoy_growth(income_sorted)\n",
    "balance_growth = compute_yoy_growth(balance_sorted)\n",
    "cashflow_growth = compute_yoy_growth(cashflow_sorted)\n",
    "\n",
    "#we are sorting to make sure that hte dates are properly aligned before we perform out pct chagne method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f17b0-b137-4386-b95c-58b08ed4a839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_zeros_nans_yoy(df):\n",
    "    # Keep only numeric columns that end with \"_yoy\"\n",
    "    numeric_cols = [c for c in df.select_dtypes(include=[float, int]).columns if c.endswith(\"_yoy\")]\n",
    "    \n",
    "    # Count zeros and NaNs\n",
    "    zero_counts = (df[numeric_cols] == 0).sum()\n",
    "    nan_counts = df[numeric_cols].isna().sum()\n",
    "    \n",
    "    # Combine into a single DataFrame\n",
    "    summary = pd.DataFrame({\n",
    "        \"zeros\": zero_counts,\n",
    "        \"nans\": nan_counts\n",
    "    }).sort_values(by=[\"zeros\", \"nans\"], ascending=True)\n",
    "    \n",
    "    # Force full display\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(summary)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "print(\"Income 0/NaN counts per YoY column:\")\n",
    "income_summary = count_zeros_nans_yoy(income_growth)\n",
    "\n",
    "print(\"\\nBalance 0/NaN counts per YoY column:\")\n",
    "balance_summary = count_zeros_nans_yoy(balance_growth)\n",
    "\n",
    "print(\"\\nCashflow 0/NaN counts per YoY column:\")\n",
    "cashflow_summary = count_zeros_nans_yoy(cashflow_growth)\n",
    "\n",
    "# the purpose here is to make it easy to identify which line items (columns) are fully filled out from our sample so that we are only grabbing columns (features)\n",
    "# that are likely to be filled out by the stock under consideration, cause ultimately after we find a regression that has explanatory power... we can still only apply it \n",
    "# to the stock under consideration if it has the same line items filled out \n",
    "# the 503 nans is a result of the .pct change method that we used which creates a nan on every other row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f2825-768b-4ae3-bb05-0fba40fcac2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_single_period_features(df, nan_threshold=503, keep_cols=[\"symbol\",\"date\"], name=\"\"):\n",
    "    \"\"\"Select _yoy numeric columns, replace inf, drop rows/columns with too many NaNs.\"\"\"\n",
    "    yoy_cols = [c for c in df.select_dtypes(include=[float, int]).columns if c.endswith(\"_yoy\")]\n",
    "    sub_df = df[yoy_cols + keep_cols].copy()  # Keep symbol/date columns\n",
    "    \n",
    "    # Drop columns with too many NaNs (only apply to yoy columns)\n",
    "    valid_yoy_cols = [c for c in yoy_cols if sub_df[c].isna().sum() <= nan_threshold]\n",
    "    sub_df = sub_df[valid_yoy_cols + keep_cols]\n",
    "    \n",
    "    # Replace infinities with NaN and drop rows with any remaining NaNs (only apply to yoy columns)\n",
    "    sub_df[valid_yoy_cols] = sub_df[valid_yoy_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    sub_df.dropna(subset=valid_yoy_cols, inplace=True)\n",
    "    \n",
    "    print(f\"{name} cleaned shape: {sub_df.shape}\")\n",
    "    return sub_df\n",
    "\n",
    "# Step 0: Clean _yoy columns and print shapes\n",
    "income_post_nans = remove_single_period_features(income_growth, name=\"Income\")\n",
    "balance_post_nans = remove_single_period_features(balance_growth, name=\"Balance\")\n",
    "cashflow_post_nans = remove_single_period_features(cashflow_growth, name=\"Cashflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65861d96-adb2-47b1-80e2-88785926295f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_density_scatter(df, title):\n",
    "    # Flatten all numeric columns into one long vector\n",
    "    numeric_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns\n",
    "    values = df[numeric_cols].values.flatten()\n",
    "    values = values[~np.isnan(values)]  # drop NaNs\n",
    "\n",
    "    # Scatter vs. index, colored by density\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(values, fill=True, color=\"lightblue\", alpha=0.3, linewidth=0)  # background density\n",
    "    plt.scatter(range(len(values)), values, \n",
    "                c=values, cmap=\"viridis\", s=5, alpha=0.6)\n",
    "\n",
    "    plt.title(f\"Density Scatterplot: {title}\", fontsize=14)\n",
    "    plt.xlabel(\"Index\", fontsize=12)\n",
    "    plt.ylabel(\"Value\", fontsize=12)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Call for each of your cleaned DataFrames\n",
    "plot_density_scatter(income_post_nans, \"Income\")\n",
    "plot_density_scatter(balance_post_nans, \"Balance\")\n",
    "plot_density_scatter(cashflow_post_nans, \"Cashflow\")\n",
    "\n",
    "#demonstrates the necessity for addressing outlier concerns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b204d-3254-4ffd-92ac-288c1a6b9c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_distribution(df, title):\n",
    "    # Flatten numeric columns into one vector\n",
    "    numeric_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns\n",
    "    values = df[numeric_cols].values.flatten()\n",
    "    values = values[~np.isnan(values)]  # remove NaNs\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(values, bins=200, density=True, alpha=0.6, color=\"steelblue\")\n",
    "    \n",
    "    # KDE overlay\n",
    "    kde = gaussian_kde(values)\n",
    "    xs = np.linspace(-8, 8, 400)\n",
    "    plt.plot(xs, kde(xs), color=\"darkred\", lw=2)\n",
    "\n",
    "    plt.title(f\"Distribution: {title}\", fontsize=14)\n",
    "    plt.xlabel(\"Value\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.xlim(-8, 8)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Call for each dataset\n",
    "plot_distribution(income_post_nans, \"Income\")\n",
    "plot_distribution(balance_post_nans, \"Balance\")\n",
    "plot_distribution(cashflow_post_nans, \"Cashflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7de5d-cb6c-4ffb-90d8-21e3ee14d871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Keep only common (symbol, date) pairs\n",
    "for df in [income_post_nans, balance_post_nans, cashflow_post_nans]:\n",
    "    df[\"symbol_date\"] = list(zip(df[\"symbol\"], df[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b96a7-2d78-40a1-a8d8-5de193100403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_pairs = (\n",
    "    set(income_post_nans[\"symbol_date\"])\n",
    "    & set(balance_post_nans[\"symbol_date\"])\n",
    "    & set(cashflow_post_nans[\"symbol_date\"])\n",
    ")\n",
    "\n",
    "def filter_by_common_pairs(df, common_pairs):\n",
    "    \"\"\"\n",
    "    Keep only rows where the 'symbol_date' is in common_pairs.\n",
    "    Returns a new DataFrame.\n",
    "    \"\"\"\n",
    "    return df[df[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "\n",
    "income_post_nans_overlapped = filter_by_common_pairs(income_post_nans, common_pairs)\n",
    "balance_post_nans_overlapped = filter_by_common_pairs(balance_post_nans, common_pairs)\n",
    "cashflow_post_nans_overlapped = filter_by_common_pairs(cashflow_post_nans, common_pairs)\n",
    "\n",
    "# we only want to include a ticker if we have all the financial statement items for all three financial statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ff5cc-0343-463b-a89d-32c3f8b744a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Sort consistently\n",
    "income_post_nans_overlapped = income_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "balance_post_nans_overlapped = balance_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "cashflow_post_nans_overlapped = cashflow_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "print(\n",
    "    \"Cleaned and aligned shapes:\", \n",
    "    income_post_nans_overlapped.shape, \n",
    "    \n",
    "    \n",
    "    balance_post_nans_overlapped.shape, \n",
    "    cashflow_post_nans_overlapped.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed1b04-d0c6-4593-a018-e0f53440299b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, df in {\"income\": income_post_nans_overlapped, \"balance\": balance_post_nans_overlapped, \"cashflow\": cashflow_post_nans_overlapped}.items():\n",
    "    print(f\"\\n{name.upper()} FIRST 5 SYMBOLS:\")\n",
    "    print(df[\"symbol\"].head(5).to_list())\n",
    "    print(f\"\\n{name.upper()} LAST 5 SYMBOLS:\")\n",
    "    print(df[\"symbol\"].tail(5).to_list())\n",
    "    \n",
    "    \n",
    "# just a check that the symbols are the same and aligned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c759955-9178-4ce7-a017-584a39a18e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_univariate_regressions(X, y):\n",
    "    \"\"\"\n",
    "    Runs univariate OLS regressions of y on each column of X.\n",
    "    \n",
    "    Returns a DataFrame with coefficients, t-values, raw p-values, \n",
    "    FDR-adjusted p-values, and R-squared.\n",
    "    \"\"\"\n",
    "    # --- Drop duplicate columns by name ---\n",
    "    X = X.loc[:, ~X.columns.duplicated()].copy()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for col in X.columns:\n",
    "        X_const = sm.add_constant(X[[col]])\n",
    "        model = sm.OLS(y, X_const).fit()\n",
    "        coef = model.params[col]\n",
    "        t_value = model.tvalues[col]\n",
    "        pval = model.pvalues[col]\n",
    "        r2 = model.rsquared\n",
    "        results.append((col, coef, t_value, pval, r2))\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        results, columns=[\"feature\", \"coef\", \"t_value\", \"pval\", \"r2\"]\n",
    "    )\n",
    "\n",
    "    # Apply Benjamini-Hochberg FDR correction\n",
    "    reject, pvals_corrected, _, _ = multipletests(\n",
    "        results_df[\"pval\"], alpha=0.05, method=\"fdr_bh\"\n",
    "    )\n",
    "    results_df[\"pval_fdr\"] = pvals_corrected\n",
    "    results_df[\"reject_null\"] = reject\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3feb244f-141f-44f5-87e6-afb7b095c809",
   "metadata": {
    "tags": []
   },
   "source": [
    "def run_univariate_regressions(X, y):\n",
    "    \"\"\"\n",
    "    Runs univariate OLS regressions of y on each column of X.\n",
    "    \n",
    "    Returns a DataFrame with coefficients, t-values, raw p-values, \n",
    "    FDR-adjusted p-values, and R-squared.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for col in X.columns:\n",
    "        X_const = sm.add_constant(X[[col]])\n",
    "        model = sm.OLS(y, X_const).fit()\n",
    "        coef = model.params[col]\n",
    "        t_value = model.tvalues[col]  # <-- store t-value\n",
    "        pval = model.pvalues[col]\n",
    "        r2 = model.rsquared\n",
    "        results.append((col, coef, t_value, pval, r2))\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=[\"feature\", \"coef\", \"t_value\", \"pval\", \"r2\"])\n",
    "\n",
    "    # Apply Benjamini-Hochberg FDR correction\n",
    "    reject, pvals_corrected, _, _ = multipletests(results_df[\"pval\"], alpha=0.05, method=\"fdr_bh\")\n",
    "    results_df[\"pval_fdr\"] = pvals_corrected\n",
    "    results_df[\"reject_null\"] = reject\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e14874-9a2e-4f1c-af29-1cb659d8ae6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pe_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85347437-8761-4dc8-a3d6-28b5a2be3e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Prepare the dependent variable (only symbol + log_PE, no date)\n",
    "log_pe_df = pe_data[[\"symbol\", \"log_PE\"]].copy()\n",
    "\n",
    "# 2. Merge income with log_pe\n",
    "income_merged = income_post_nans_overlapped.merge(\n",
    "    log_pe_df, on=\"symbol\", how=\"inner\"\n",
    ")\n",
    "X_income = income_merged.select_dtypes(include=[\"number\"]).drop(columns=[\"log_PE\"])\n",
    "y_income = income_merged[\"log_PE\"]\n",
    "\n",
    "income_univariate = run_univariate_regressions(X_income, y_income)\n",
    "print(\"Income statement results:\")\n",
    "print(income_univariate.head())\n",
    "\n",
    "# 3. Merge balance with log_pe\n",
    "balance_merged = balance_post_nans_overlapped.merge(\n",
    "    log_pe_df, on=\"symbol\", how=\"inner\"\n",
    ")\n",
    "X_balance = balance_merged.select_dtypes(include=[\"number\"]).drop(columns=[\"log_PE\"])\n",
    "y_balance = balance_merged[\"log_PE\"]\n",
    "\n",
    "balance_univariate = run_univariate_regressions(X_balance, y_balance)\n",
    "print(\"\\nBalance sheet results:\")\n",
    "print(balance_univariate.head())\n",
    "\n",
    "# 4. Merge cashflow with log_pe\n",
    "cashflow_merged = cashflow_post_nans_overlapped.merge(\n",
    "    log_pe_df, on=\"symbol\", how=\"inner\"\n",
    ")\n",
    "X_cashflow = cashflow_merged.select_dtypes(include=[\"number\"]).drop(columns=[\"log_PE\"])\n",
    "y_cashflow = cashflow_merged[\"log_PE\"]\n",
    "\n",
    "cashflow_univariate = run_univariate_regressions(X_cashflow, y_cashflow)\n",
    "print(\"\\nCashflow statement results:\")\n",
    "print(cashflow_univariate.head(2))\n",
    "\n",
    "\n",
    "print(income_merged.shape)\n",
    "print(balance_merged.shape)\n",
    "print(cashflow_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84aa717-384a-4aad-81a7-48eb136b9d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Combine the three DataFrames temporarily for plotting\n",
    "plot_df = pd.concat([\n",
    "    income_univariate.assign(statement_type='Income'),\n",
    "    balance_univariate.assign(statement_type='Balance'),\n",
    "    cashflow_univariate.assign(statement_type='Cashflow')\n",
    "], ignore_index=True)\n",
    "\n",
    "# 2. Sort within each statement by absolute t-value descending\n",
    "plot_df = plot_df.sort_values(['statement_type', 't_value'], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# 3. Normalize r2 for color mapping\n",
    "norm = mpl.colors.Normalize(vmin=plot_df['r2'].min(), vmax=plot_df['r2'].max())\n",
    "cmap = mpl.cm.viridis\n",
    "colors = [cmap(norm(val)) for val in plot_df['r2']]\n",
    "\n",
    "# 4. Prepare the y positions for grouping by statement\n",
    "statement_order = ['Income', 'Balance', 'Cashflow']\n",
    "plot_df['group_num'] = plot_df['statement_type'].map({s: i for i, s in enumerate(statement_order)})\n",
    "\n",
    "# Add a small offset between groups\n",
    "group_offsets = plot_df['group_num'] * 0.5\n",
    "y_pos = range(len(plot_df))\n",
    "y_pos = [i + offset for i, offset in zip(y_pos, group_offsets)]\n",
    "\n",
    "# 5. Create horizontal bar chart\n",
    "plt.figure(figsize=(12, max(8, len(plot_df) * 0.25)))\n",
    "bars = plt.barh(\n",
    "    y=y_pos,\n",
    "    width=plot_df['t_value'],\n",
    "    color=colors,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# 6. Highlight significant features with a red star\n",
    "for idx, row in enumerate(plot_df.itertuples()):\n",
    "    if row.reject_null:\n",
    "        plt.text(\n",
    "            x=row.t_value + (0.05 if row.t_value>0 else -0.05),\n",
    "            y=y_pos[idx],\n",
    "            s='*',\n",
    "            va='center',\n",
    "            ha='left' if row.t_value>0 else 'right',\n",
    "            color='red',\n",
    "            fontsize=12\n",
    "        )\n",
    "\n",
    "# 7. Y-axis labels\n",
    "plt.yticks(y_pos, plot_df['feature'])\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.xlabel('t-value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Univariate Regression t-values by Feature and Statement Type (Color = R²)')\n",
    "\n",
    "# 8. Add colorbar for R²\n",
    "sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "plt.colorbar(sm, label='R²')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1d039-4460-4571-9527-d7aba05e3a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_threshold = 2\n",
    "r2_threshold = 0.1\n",
    "p_threshold = 0.05\n",
    "\n",
    "# --- Income ---\n",
    "signif_income = income_univariate[\n",
    "    (income_univariate[\"t_value\"].abs() > t_threshold) &\n",
    "    (income_univariate[\"r2\"] > r2_threshold) &\n",
    "    (income_univariate[\"pval\"] < p_threshold)\n",
    "]\n",
    "\n",
    "income_features = signif_income[\"feature\"].tolist()\n",
    "income_selected = income_merged[[\"symbol\", \"log_PE\"] + income_features]\n",
    "\n",
    "# --- Balance ---\n",
    "signif_balance = balance_univariate[\n",
    "    (balance_univariate[\"t_value\"].abs() > t_threshold) &\n",
    "    (balance_univariate[\"r2\"] > r2_threshold) &\n",
    "    (balance_univariate[\"pval\"] < p_threshold)\n",
    "]\n",
    "\n",
    "balance_features = signif_balance[\"feature\"].tolist()\n",
    "balance_selected = balance_merged[[\"symbol\", \"log_PE\"] + balance_features]\n",
    "\n",
    "# --- Cashflow ---\n",
    "signif_cashflow = cashflow_univariate[\n",
    "    (cashflow_univariate[\"t_value\"].abs() > t_threshold) &\n",
    "    (cashflow_univariate[\"r2\"] > r2_threshold) &\n",
    "    (cashflow_univariate[\"pval\"] < p_threshold)\n",
    "]\n",
    "\n",
    "cashflow_features = signif_cashflow[\"feature\"].tolist()\n",
    "cashflow_selected = cashflow_merged[[\"symbol\", \"log_PE\"] + cashflow_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f12562-b05e-4743-886d-802d4de0c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a line of code here to grab the column names from the \"univariate\" dataframes and hten go back tot he merged dataframes and grab the actual\n",
    "# data because that data is what we will run the pca.. the univeriate regresisons are just a filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f820d-231d-41e1-a154-6871519e7d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pca_with_prefix(df, n_components=None, prefix=\"\", columns=None):\n",
    "    \"\"\"\n",
    "    Run PCA on a DataFrame with optional column selection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input data.\n",
    "    n_components : int or None\n",
    "        Number of PCA components. If None, use all available features.\n",
    "    prefix : str\n",
    "        Prefix for the PCA component column names.\n",
    "    columns : list of str or None\n",
    "        Subset of columns to run PCA on. If None, use all numeric columns\n",
    "        except common ID columns like symbol/date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with PCA component columns added.\n",
    "    PCA\n",
    "        The fitted PCA object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop common ID columns if present\n",
    "    exclude_cols = [\"symbol\", \"date\", \"symbol_date\"]\n",
    "\n",
    "    if columns is None:\n",
    "        feature_df = df.drop(columns=[c for c in exclude_cols if c in df.columns], errors=\"ignore\")\n",
    "    else:\n",
    "        feature_df = df[columns]\n",
    "\n",
    "    # Make sure it's numeric\n",
    "    feature_df = feature_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    # Handle n_components safely\n",
    "    max_components = min(feature_df.shape[0], feature_df.shape[1])\n",
    "    if n_components is None:\n",
    "        n_components = max_components\n",
    "    else:\n",
    "        n_components = min(n_components, max_components)\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(feature_df)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = pca.fit_transform(scaled)\n",
    "\n",
    "    comp_df = pd.DataFrame(\n",
    "        components,\n",
    "        columns=[f\"{prefix}{i+1}\" for i in range(components.shape[1])],\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    return pd.concat([df, comp_df], axis=1), pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fcdd7-57d2-4bb2-bf2a-f3c0355ae096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Income statement PCA\n",
    "income_df_with_pca, income_pca_model = run_pca_with_prefix(\n",
    "    income_post_nans_overlapped.drop(columns=[\"symbol\", \"date\", \"symbol_date\"], errors=\"ignore\"),\n",
    "    n_components=3,\n",
    "    prefix=\"income_\"\n",
    ")\n",
    "\n",
    "# Balance sheet PCA\n",
    "balance_df_with_pca, balance_pca_model = run_pca_with_prefix(\n",
    "    balance_post_nans_overlapped.drop(columns=[\"symbol\", \"date\", \"symbol_date\"], errors=\"ignore\"),\n",
    "    n_components=3,\n",
    "    prefix=\"balance_\"\n",
    ")\n",
    "\n",
    "# Cash flow PCA\n",
    "cashflow_df_with_pca, cashflow_pca_model = run_pca_with_prefix(\n",
    "    cashflow_post_nans_overlapped.drop(columns=[\"symbol\", \"date\", \"symbol_date\"], errors=\"ignore\"),\n",
    "    n_components=3,\n",
    "    prefix=\"cashflow_\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4b15a-dc38-4987-98af-029cf9d16292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_log_pe_on_pca(pca_df, log_pe_df, prefix=\"income_\"):\n",
    "    \"\"\"\n",
    "    Merge PCA outputs with log(PE) and run regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pca_df : pd.DataFrame\n",
    "        DataFrame with PCA components (from run_pca_with_prefix).\n",
    "    log_pe_df : pd.DataFrame\n",
    "        DataFrame containing ['symbol','date','log_pe'] at least.\n",
    "    prefix : str\n",
    "        Prefix used in PCA component names (e.g., 'income_', 'balance_', 'cashflow_').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    statsmodels RegressionResults\n",
    "        The fitted regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. Select PCA columns\n",
    "    X = merged[[c for c in merged.columns if c.startswith(prefix)]]\n",
    "    y = merged[\"log_pe\"]\n",
    "\n",
    "    # 3. Add intercept\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # 4. Run regression\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04f486-970a-4958-b919-0723c66180f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
