{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9ed79e-112a-418f-b955-5c13dd46dd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2982057a-32e3-43e6-b761-6240815efa25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tickers from CSV cache...\n",
      "(503, 8)\n",
      "Index(['symbol', 'name', 'sector', 'subSector', 'headQuarter',\n",
      "       'dateFirstAdded', 'cik', 'founded'],\n",
      "      dtype='object')\n",
      "  symbol                  name                  sector  \\\n",
      "0    XYZ           Block, Inc.              Technology   \n",
      "1    TTD  The Trade Desk, Inc.              Technology   \n",
      "2   DDOG               Datadog              Technology   \n",
      "3   COIN       Coinbase Global      Financial Services   \n",
      "4   DASH              DoorDash  Communication Services   \n",
      "\n",
      "                            subSector              headQuarter dateFirstAdded  \\\n",
      "0           Software - Infrastructure      Oakland, California     2025-07-23   \n",
      "1              Software - Application      Ventura, California     2025-07-18   \n",
      "2              Software - Application  New York City, New York     2025-07-09   \n",
      "3  Financial - Data & Stock Exchanges     Wilmington, Delaware     2025-05-19   \n",
      "4      Internet Content & Information        San Francisco, CA     2025-03-24   \n",
      "\n",
      "       cik founded  \n",
      "0  1512673    2009  \n",
      "1  1671933    2009  \n",
      "2  1561550    2010  \n",
      "3  1679788    2012  \n",
      "4  1792789    2013  \n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "# --- LOAD FROM CACHE OR FETCH ---\n",
    "if os.path.exists(tickers_csv_file):\n",
    "    print(\"Loading tickers from CSV cache...\")\n",
    "    df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "else:\n",
    "    print(\"Fetching tickers from API...\")\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/sp500_constituent?apikey={API_KEY}\"\n",
    "    df_sp500 = pd.DataFrame(requests.get(url).json())\n",
    "\n",
    "    # Save to CSV\n",
    "    df_sp500.to_csv(tickers_csv_file, index=False)\n",
    "    print(f\"Saved {len(df_sp500)} tickers to CSV cache.\")\n",
    "\n",
    "    \n",
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()\n",
    "# --- PREVIEW ---\n",
    "print(df_sp500.shape)\n",
    "print(df_sp500.columns)\n",
    "print(df_sp500.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7840d0-3280-4f6b-8164-3d1996487ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/price_and_earnings.json\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "# Load tickers\n",
    "df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()\n",
    "\n",
    "# Output file\n",
    "output_file = os.path.join(data_folder, \"price_and_earnings.json\")\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Get current price\n",
    "            quote_url = f\"https://financialmodelingprep.com/api/v3/quote/{ticker}?apikey={API_KEY}\"\n",
    "            price_data = requests.get(quote_url).json()\n",
    "            if not price_data:\n",
    "                continue\n",
    "            price = price_data[0][\"price\"]\n",
    "\n",
    "            # Get latest annual income statement (EPS or netIncome)\n",
    "            income_url = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?limit=1&apikey={API_KEY}\"\n",
    "            income_data = requests.get(income_url).json()\n",
    "            if not income_data:\n",
    "                continue\n",
    "            eps = income_data[0].get(\"eps\")\n",
    "            net_income = income_data[0].get(\"netIncome\")\n",
    "\n",
    "            records.append({\n",
    "                \"symbol\": ticker,\n",
    "                \"price\": price,\n",
    "                \"eps\": eps,\n",
    "                \"netIncome\": net_income\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "        time.sleep(0.2)  # polite rate limit\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "price_earnings_df = fetch_price_and_earnings(tickers, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb737c07-dc25-4514-858c-f5178907b5be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows used: 480\n",
      "0    2.741115\n",
      "1    4.033134\n",
      "2    5.514162\n",
      "3    3.434049\n",
      "4    6.756855\n",
      "Name: log_PE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Only keep rows with positive EPS\n",
    "pe_data = price_earnings_df[price_earnings_df[\"eps\"] > 0].copy()\n",
    "\n",
    "# Compute log(PE)\n",
    "pe_data[\"log_PE\"] = np.log(pe_data[\"price\"] / pe_data[\"eps\"])\n",
    "\n",
    "# Optional: store just the series if you want\n",
    "pe_series = pe_data[\"log_PE\"]\n",
    "\n",
    "# Print row count for reference\n",
    "print(f\"Rows used: {len(pe_series)}\")\n",
    "print(pe_series.head())\n",
    "\n",
    "# we are only pulling the eps and takingthe log here.. the code that follows doesn't immediately and directly build on this code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031400b-6a30-4ff6-ab53-72963cfe5a96",
   "metadata": {},
   "source": [
    "## break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1844560d-77dc-4f2a-8993-96bb263662be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/income-statement_annual_limit2.json\n",
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/balance-sheet-statement_annual_limit2.json\n",
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/cash-flow-statement_annual_limit2.json\n",
      "Income shape: (1006, 38)\n",
      "Balance shape: (1006, 54)\n",
      "Cash flow shape: (1006, 40)\n",
      "Index(['date', 'symbol', 'reportedCurrency', 'cik', 'fillingDate',\n",
      "       'acceptedDate', 'calendarYear', 'period', 'revenue', 'costOfRevenue',\n",
      "       'grossProfit', 'grossProfitRatio', 'researchAndDevelopmentExpenses',\n",
      "       'generalAndAdministrativeExpenses', 'sellingAndMarketingExpenses',\n",
      "       'sellingGeneralAndAdministrativeExpenses', 'otherExpenses',\n",
      "       'operatingExpenses', 'costAndExpenses', 'interestIncome',\n",
      "       'interestExpense', 'depreciationAndAmortization', 'ebitda',\n",
      "       'ebitdaratio', 'operatingIncome', 'operatingIncomeRatio',\n",
      "       'totalOtherIncomeExpensesNet', 'incomeBeforeTax',\n",
      "       'incomeBeforeTaxRatio', 'incomeTaxExpense', 'netIncome',\n",
      "       'netIncomeRatio', 'eps', 'epsdiluted', 'weightedAverageShsOut',\n",
      "       'weightedAverageShsOutDil', 'link', 'finalLink'],\n",
      "      dtype='object')\n",
      "Index(['date', 'symbol', 'reportedCurrency', 'cik', 'fillingDate',\n",
      "       'acceptedDate', 'calendarYear', 'period', 'cashAndCashEquivalents',\n",
      "       'shortTermInvestments', 'cashAndShortTermInvestments', 'netReceivables',\n",
      "       'inventory', 'otherCurrentAssets', 'totalCurrentAssets',\n",
      "       'propertyPlantEquipmentNet', 'goodwill', 'intangibleAssets',\n",
      "       'goodwillAndIntangibleAssets', 'longTermInvestments', 'taxAssets',\n",
      "       'otherNonCurrentAssets', 'totalNonCurrentAssets', 'otherAssets',\n",
      "       'totalAssets', 'accountPayables', 'shortTermDebt', 'taxPayables',\n",
      "       'deferredRevenue', 'otherCurrentLiabilities', 'totalCurrentLiabilities',\n",
      "       'longTermDebt', 'deferredRevenueNonCurrent',\n",
      "       'deferredTaxLiabilitiesNonCurrent', 'otherNonCurrentLiabilities',\n",
      "       'totalNonCurrentLiabilities', 'otherLiabilities',\n",
      "       'capitalLeaseObligations', 'totalLiabilities', 'preferredStock',\n",
      "       'commonStock', 'retainedEarnings',\n",
      "       'accumulatedOtherComprehensiveIncomeLoss',\n",
      "       'othertotalStockholdersEquity', 'totalStockholdersEquity',\n",
      "       'totalEquity', 'totalLiabilitiesAndStockholdersEquity',\n",
      "       'minorityInterest', 'totalLiabilitiesAndTotalEquity',\n",
      "       'totalInvestments', 'totalDebt', 'netDebt', 'link', 'finalLink'],\n",
      "      dtype='object')\n",
      "Index(['date', 'symbol', 'reportedCurrency', 'cik', 'fillingDate',\n",
      "       'acceptedDate', 'calendarYear', 'period', 'netIncome',\n",
      "       'depreciationAndAmortization', 'deferredIncomeTax',\n",
      "       'stockBasedCompensation', 'changeInWorkingCapital',\n",
      "       'accountsReceivables', 'inventory', 'accountsPayables',\n",
      "       'otherWorkingCapital', 'otherNonCashItems',\n",
      "       'netCashProvidedByOperatingActivities',\n",
      "       'investmentsInPropertyPlantAndEquipment', 'acquisitionsNet',\n",
      "       'purchasesOfInvestments', 'salesMaturitiesOfInvestments',\n",
      "       'otherInvestingActivites', 'netCashUsedForInvestingActivites',\n",
      "       'debtRepayment', 'commonStockIssued', 'commonStockRepurchased',\n",
      "       'dividendsPaid', 'otherFinancingActivites',\n",
      "       'netCashUsedProvidedByFinancingActivities',\n",
      "       'effectOfForexChangesOnCash', 'netChangeInCash', 'cashAtEndOfPeriod',\n",
      "       'cashAtBeginningOfPeriod', 'operatingCashFlow', 'capitalExpenditure',\n",
      "       'freeCashFlow', 'link', 'finalLink'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def fetch_statement(endpoint, tickers, period, limit, data_folder):\n",
    "    \"\"\"Fetch statements with unique JSON filename based on endpoint, period, limit.\"\"\"\n",
    "    output_file = os.path.join(\n",
    "        data_folder,\n",
    "        f\"{endpoint}_{period}_limit{limit}.json\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        with open(output_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        url = f\"https://financialmodelingprep.com/api/v3/{endpoint}/{ticker}?period={period}&limit={limit}&apikey={API_KEY}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                for row in data:\n",
    "                    row[\"symbol\"] = ticker\n",
    "                records.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker} ({endpoint}): {e}\")\n",
    "        time.sleep(.2)  # API polite rate limit\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "    return records\n",
    "\n",
    "\n",
    "income_data_2_years   = fetch_statement(\"income-statement\", tickers, \"annual\", 2, data_folder)\n",
    "balance_data_2_years  = fetch_statement(\"balance-sheet-statement\", tickers, \"annual\", 2, data_folder)\n",
    "cashflow_data_2_years = fetch_statement(\"cash-flow-statement\", tickers, \"annual\", 2, data_folder)\n",
    "\n",
    "# Convert to DataFrames\n",
    "income_data_2_years   = pd.DataFrame(income_data_2_years)\n",
    "balance_data_2_years  = pd.DataFrame(balance_data_2_years)\n",
    "cashflow_data_2_years = pd.DataFrame(cashflow_data_2_years)\n",
    "\n",
    "print(\"Income shape:\", income_data_2_years.shape)\n",
    "print(\"Balance shape:\", balance_data_2_years.shape)\n",
    "print(\"Cash flow shape:\", cashflow_data_2_years.shape)\n",
    "\n",
    "print(income_data_2_years.columns)\n",
    "print(balance_data_2_years.columns)\n",
    "print(cashflow_data_2_years.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6e9e1d-7967-4d9d-bedd-c234025a3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_symbol_date(df):\n",
    "    return df.sort_values([\"symbol\", \"date\"])\n",
    "\n",
    "\n",
    "def compute_yoy_growth(df, exclude_cols=[\"symbol\", \"date\",\"link\",\"finalLink\"]):\n",
    "    numeric_cols = df.select_dtypes(include=[float, int]).columns\n",
    "    numeric_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "    \n",
    "    df_growth = df.copy()\n",
    "    for col in numeric_cols:\n",
    "        df_growth[col + \"_yoy\"] = df.groupby(\"symbol\")[col].pct_change()\n",
    "    \n",
    "    return df_growth\n",
    "\n",
    "\n",
    "income_sorted = sort_by_symbol_date(income_data_2_years)\n",
    "balance_sorted = sort_by_symbol_date(balance_data_2_years)\n",
    "cashflow_sorted = sort_by_symbol_date(cashflow_data_2_years)\n",
    "\n",
    "income_growth = compute_yoy_growth(income_sorted)\n",
    "balance_growth = compute_yoy_growth(balance_sorted)\n",
    "cashflow_growth = compute_yoy_growth(cashflow_sorted)\n",
    "\n",
    "\n",
    "#we are sorting to make sure that hte dates are properly aligned before we perform out pct chagne method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3379f-f655-4b95-bf6e-9eef3d845b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130f17b0-b137-4386-b95c-58b08ed4a839",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income 0/NaN counts per YoY column:\n",
      "                                             zeros  nans\n",
      "revenue_yoy                                      0   503\n",
      "grossProfit_yoy                                  0   503\n",
      "costAndExpenses_yoy                              0   503\n",
      "ebitdaratio_yoy                                  0   503\n",
      "operatingIncome_yoy                              0   503\n",
      "operatingIncomeRatio_yoy                         0   503\n",
      "incomeBeforeTax_yoy                              0   503\n",
      "incomeBeforeTaxRatio_yoy                         0   503\n",
      "netIncomeRatio_yoy                               0   503\n",
      "ebitda_yoy                                       0   504\n",
      "operatingExpenses_yoy                            0   507\n",
      "costOfRevenue_yoy                                0   510\n",
      "totalOtherIncomeExpensesNet_yoy                  0   535\n",
      "generalAndAdministrativeExpenses_yoy             0   756\n",
      "netIncome_yoy                                    1   503\n",
      "eps_yoy                                          1   503\n",
      "epsdiluted_yoy                                   1   503\n",
      "depreciationAndAmortization_yoy                  1   508\n",
      "incomeTaxExpense_yoy                             1   508\n",
      "sellingGeneralAndAdministrativeExpenses_yoy      1   530\n",
      "otherExpenses_yoy                                1   642\n",
      "researchAndDevelopmentExpenses_yoy               1   792\n",
      "sellingAndMarketingExpenses_yoy                  3   841\n",
      "interestIncome_yoy                               4   667\n",
      "weightedAverageShsOut_yoy                        7   503\n",
      "grossProfitRatio_yoy                             9   503\n",
      "weightedAverageShsOutDil_yoy                     9   503\n",
      "interestExpense_yoy                              9   530\n",
      "\n",
      "Balance 0/NaN counts per YoY column:\n",
      "                                             zeros  nans\n",
      "otherNonCurrentAssets_yoy                        0   503\n",
      "totalAssets_yoy                                  0   503\n",
      "totalLiabilities_yoy                             0   503\n",
      "totalStockholdersEquity_yoy                      0   503\n",
      "totalEquity_yoy                                  0   503\n",
      "totalLiabilitiesAndStockholdersEquity_yoy        0   503\n",
      "totalLiabilitiesAndTotalEquity_yoy               0   503\n",
      "totalNonCurrentAssets_yoy                        0   504\n",
      "totalCurrentAssets_yoy                           0   505\n",
      "longTermDebt_yoy                                 0   505\n",
      "totalNonCurrentLiabilities_yoy                   0   505\n",
      "retainedEarnings_yoy                             0   505\n",
      "otherNonCurrentLiabilities_yoy                   0   508\n",
      "netReceivables_yoy                               0   513\n",
      "accountPayables_yoy                              0   530\n",
      "shortTermInvestments_yoy                         0   780\n",
      "otherAssets_yoy                                  0   971\n",
      "otherLiabilities_yoy                             0   985\n",
      "cashAndCashEquivalents_yoy                       1   503\n",
      "cashAndShortTermInvestments_yoy                  1   503\n",
      "totalDebt_yoy                                    1   504\n",
      "totalCurrentLiabilities_yoy                      1   508\n",
      "otherCurrentLiabilities_yoy                      1   510\n",
      "propertyPlantEquipmentNet_yoy                    1   511\n",
      "otherCurrentAssets_yoy                           1   536\n",
      "othertotalStockholdersEquity_yoy                 1   547\n",
      "totalInvestments_yoy                             1   602\n",
      "longTermInvestments_yoy                          1   658\n",
      "deferredTaxLiabilitiesNonCurrent_yoy             1   665\n",
      "taxPayables_yoy                                  1   674\n",
      "deferredRevenue_yoy                              1   731\n",
      "netDebt_yoy                                      2   503\n",
      "capitalLeaseObligations_yoy                      2   605\n",
      "taxAssets_yoy                                    2   752\n",
      "inventory_yoy                                    3   666\n",
      "deferredRevenueNonCurrent_yoy                    3   869\n",
      "accumulatedOtherComprehensiveIncomeLoss_yoy      5   515\n",
      "intangibleAssets_yoy                             5   586\n",
      "shortTermDebt_yoy                                6   539\n",
      "minorityInterest_yoy                            13   719\n",
      "goodwillAndIntangibleAssets_yoy                 24   539\n",
      "preferredStock_yoy                              27   949\n",
      "goodwill_yoy                                    73   553\n",
      "commonStock_yoy                                244   534\n",
      "\n",
      "Cashflow 0/NaN counts per YoY column:\n",
      "                                              zeros  nans\n",
      "netCashProvidedByOperatingActivities_yoy          0   503\n",
      "netCashUsedForInvestingActivites_yoy              0   503\n",
      "netCashUsedProvidedByFinancingActivities_yoy      0   503\n",
      "cashAtBeginningOfPeriod_yoy                       0   503\n",
      "operatingCashFlow_yoy                             0   503\n",
      "freeCashFlow_yoy                                  0   503\n",
      "otherWorkingCapital_yoy                           0   506\n",
      "investmentsInPropertyPlantAndEquipment_yoy        0   529\n",
      "deferredIncomeTax_yoy                             0   567\n",
      "effectOfForexChangesOnCash_yoy                    0   636\n",
      "inventory_yoy                                     0   660\n",
      "purchasesOfInvestments_yoy                        0   672\n",
      "commonStockIssued_yoy                             0   791\n",
      "netIncome_yoy                                     1   503\n",
      "netChangeInCash_yoy                               1   503\n",
      "cashAtEndOfPeriod_yoy                             1   503\n",
      "depreciationAndAmortization_yoy                   1   507\n",
      "capitalExpenditure_yoy                            1   527\n",
      "accountsReceivables_yoy                           1   561\n",
      "accountsPayables_yoy                              1   574\n",
      "acquisitionsNet_yoy                               1   618\n",
      "salesMaturitiesOfInvestments_yoy                  1   665\n",
      "changeInWorkingCapital_yoy                        2   503\n",
      "otherNonCashItems_yoy                             2   504\n",
      "debtRepayment_yoy                                 2   536\n",
      "dividendsPaid_yoy                                 2   597\n",
      "otherFinancingActivites_yoy                       3   511\n",
      "otherInvestingActivites_yoy                       5   532\n",
      "stockBasedCompensation_yoy                        5   580\n",
      "commonStockRepurchased_yoy                        8   568\n"
     ]
    }
   ],
   "source": [
    "def count_zeros_nans_yoy(df):\n",
    "    # Keep only numeric columns that end with \"_yoy\"\n",
    "    numeric_cols = [c for c in df.select_dtypes(include=[float, int]).columns if c.endswith(\"_yoy\")]\n",
    "    \n",
    "    # Count zeros and NaNs\n",
    "    zero_counts = (df[numeric_cols] == 0).sum()\n",
    "    nan_counts = df[numeric_cols].isna().sum()\n",
    "    \n",
    "    # Combine into a single DataFrame\n",
    "    summary = pd.DataFrame({\n",
    "        \"zeros\": zero_counts,\n",
    "        \"nans\": nan_counts\n",
    "    }).sort_values(by=[\"zeros\", \"nans\"], ascending=True)\n",
    "    \n",
    "    # Force full display\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(summary)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "print(\"Income 0/NaN counts per YoY column:\")\n",
    "income_summary = count_zeros_nans_yoy(income_growth)\n",
    "\n",
    "print(\"\\nBalance 0/NaN counts per YoY column:\")\n",
    "balance_summary = count_zeros_nans_yoy(balance_growth)\n",
    "\n",
    "print(\"\\nCashflow 0/NaN counts per YoY column:\")\n",
    "cashflow_summary = count_zeros_nans_yoy(cashflow_growth)\n",
    "\n",
    "# the purpose here is to make it easy to identify which line items (columns) are fully filled out from our sample so that we are only grabbing columns (features)\n",
    "# that are likely to be filled out by the stock under consideration, cause ultimately after we find a regression that has explanatory power... we can still only apply it \n",
    "# to the stock under consideration if it has the same line items filled out \n",
    "# the 503 nans is a result of the .pct change method that we used which creates a nan on every other row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "676f2825-768b-4ae3-bb05-0fba40fcac2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income cleaned shape: (502, 17)\n",
      "Balance cleaned shape: (502, 12)\n",
      "Cashflow cleaned shape: (501, 12)\n"
     ]
    }
   ],
   "source": [
    "def remove_single_period_features(df, nan_threshold=503, keep_cols=[\"symbol\",\"date\"], name=\"\"):\n",
    "    \"\"\"Select _yoy numeric columns, replace inf, drop rows/columns with too many NaNs.\"\"\"\n",
    "    yoy_cols = [c for c in df.select_dtypes(include=[float, int]).columns if c.endswith(\"_yoy\")]\n",
    "    sub_df = df[yoy_cols + keep_cols].copy()  # Keep symbol/date columns\n",
    "    \n",
    "    # Drop columns with too many NaNs (only apply to yoy columns)\n",
    "    valid_yoy_cols = [c for c in yoy_cols if sub_df[c].isna().sum() <= nan_threshold]\n",
    "    sub_df = sub_df[valid_yoy_cols + keep_cols]\n",
    "    \n",
    "    # Replace infinities with NaN and drop rows with any remaining NaNs (only apply to yoy columns)\n",
    "    sub_df[valid_yoy_cols] = sub_df[valid_yoy_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    sub_df.dropna(subset=valid_yoy_cols, inplace=True)\n",
    "    \n",
    "    print(f\"{name} cleaned shape: {sub_df.shape}\")\n",
    "    return sub_df\n",
    "\n",
    "# Step 0: Clean _yoy columns and print shapes\n",
    "income_clean = remove_single_period_features(income_growth, name=\"Income\")\n",
    "balance_clean = remove_single_period_features(balance_growth, name=\"Balance\")\n",
    "cashflow_clean = remove_single_period_features(cashflow_growth, name=\"Cashflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f7de5d-cb6c-4ffb-90d8-21e3ee14d871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Keep only common (symbol, date) pairs\n",
    "for df in [income_clean, balance_clean, cashflow_clean]:\n",
    "    df[\"symbol_date\"] = list(zip(df[\"symbol\"], df[\"date\"]))\n",
    "\n",
    "common_pairs = (\n",
    "    set(income_clean[\"symbol_date\"])\n",
    "    & set(balance_clean[\"symbol_date\"])\n",
    "    & set(cashflow_clean[\"symbol_date\"])\n",
    ")\n",
    "\n",
    "income_clean = income_clean[income_clean[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "balance_clean = balance_clean[balance_clean[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "cashflow_clean = cashflow_clean[cashflow_clean[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "\n",
    "# we only want to include a ticker if we have all the financial statement items for all three financial statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45ff5cc-0343-463b-a89d-32c3f8b744a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and aligned shapes: (491, 18) (491, 13) (491, 13)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Sort consistently\n",
    "income_clean = income_clean.sort_values([\"symbol\",\"date\"]).reset_index(drop=True)\n",
    "balance_clean = balance_clean.sort_values([\"symbol\",\"date\"]).reset_index(drop=True)\n",
    "cashflow_clean = cashflow_clean.sort_values([\"symbol\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Cleaned and aligned shapes:\", income_clean.shape, balance_clean.shape, cashflow_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47abcfd7-fcfc-44ee-bf39-194b62583559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Clean & align by (symbol, date)\n",
    "def clean_and_align(dfs, nan_threshold=503, keep_cols=[\"symbol\",\"date\"]):\n",
    "    \"\"\"\n",
    "    1) Drop columns with too many NaNs\n",
    "    2) Replace inf with NaN\n",
    "    3) Drop rows with any remaining NaNs\n",
    "    4) Align multiple DataFrames by shared (symbol, date) rows\n",
    "    \"\"\"\n",
    "    cleaned = {}\n",
    "    for name, df in dfs.items():\n",
    "        # Select _yoy numeric columns\n",
    "        yoy_cols = [c for c in df.select_dtypes(include=[float,int]).columns if c.endswith(\"_yoy\")]\n",
    "        # Keep only valid columns under nan_threshold\n",
    "        valid_cols = [c for c in yoy_cols if df[c].isna().sum() <= nan_threshold]\n",
    "        sub_df = df[valid_cols + keep_cols].copy()\n",
    "        \n",
    "        # Replace infs with NaN and drop rows with any remaining NaNs\n",
    "        sub_df[valid_cols] = sub_df[valid_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        before = sub_df.shape[0]\n",
    "        sub_df.dropna(subset=valid_cols, inplace=True)\n",
    "        after = sub_df.shape[0]\n",
    "        print(f\"{name}: {before - after} rows dropped due to NaNs\")\n",
    "        \n",
    "        # Create composite key\n",
    "        sub_df[\"symbol_date\"] = list(zip(sub_df[\"symbol\"], sub_df[\"date\"]))\n",
    "        cleaned[name] = sub_df\n",
    "        print(f\"{name} shape after cleaning: {sub_df.shape}\")\n",
    "    \n",
    "    # Find shared (symbol, date) across all dfs\n",
    "    common_pairs = set.intersection(*(set(df[\"symbol_date\"]) for df in cleaned.values()))\n",
    "    print(f\"Shared (symbol, date) rows across all dfs: {len(common_pairs)}\")\n",
    "    \n",
    "    # Filter each df to shared rows\n",
    "    aligned = {name: df[df[\"symbol_date\"].isin(common_pairs)].copy().reset_index(drop=True) \n",
    "               for name, df in cleaned.items()}\n",
    "    \n",
    "    # Drop helper column\n",
    "    for df in aligned.values():\n",
    "        df.drop(columns=[\"symbol_date\"], inplace=True)\n",
    "    \n",
    "    return aligned, common_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae15f3d-85b7-4b9c-8dc4-9e2495ffa557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income: 0 rows dropped due to NaNs\n",
      "income shape after cleaning: (491, 18)\n",
      "balance: 0 rows dropped due to NaNs\n",
      "balance shape after cleaning: (491, 13)\n",
      "cashflow: 0 rows dropped due to NaNs\n",
      "cashflow shape after cleaning: (491, 13)\n",
      "Shared (symbol, date) rows across all dfs: 491\n"
     ]
    }
   ],
   "source": [
    "aligned, common_pairs = clean_and_align({\n",
    "    \"income\": income_clean,\n",
    "    \"balance\": balance_clean,\n",
    "    \"cashflow\": cashflow_clean\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd99266-2982-49c8-85e5-4683cdd83aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['revenue_yoy', 'grossProfit_yoy', 'grossProfitRatio_yoy',\n",
      "       'costAndExpenses_yoy', 'ebitdaratio_yoy', 'operatingIncome_yoy',\n",
      "       'operatingIncomeRatio_yoy', 'incomeBeforeTax_yoy',\n",
      "       'incomeBeforeTaxRatio_yoy', 'netIncome_yoy', 'netIncomeRatio_yoy',\n",
      "       'eps_yoy', 'epsdiluted_yoy', 'weightedAverageShsOut_yoy',\n",
      "       'weightedAverageShsOutDil_yoy', 'symbol', 'date', 'symbol_date'],\n",
      "      dtype='object')\n",
      "Index(['cashAndCashEquivalents_yoy', 'cashAndShortTermInvestments_yoy',\n",
      "       'otherNonCurrentAssets_yoy', 'totalAssets_yoy', 'totalLiabilities_yoy',\n",
      "       'totalStockholdersEquity_yoy', 'totalEquity_yoy',\n",
      "       'totalLiabilitiesAndStockholdersEquity_yoy',\n",
      "       'totalLiabilitiesAndTotalEquity_yoy', 'netDebt_yoy', 'symbol', 'date',\n",
      "       'symbol_date'],\n",
      "      dtype='object')\n",
      "Index(['netIncome_yoy', 'changeInWorkingCapital_yoy',\n",
      "       'netCashProvidedByOperatingActivities_yoy',\n",
      "       'netCashUsedForInvestingActivites_yoy',\n",
      "       'netCashUsedProvidedByFinancingActivities_yoy', 'netChangeInCash_yoy',\n",
      "       'cashAtEndOfPeriod_yoy', 'cashAtBeginningOfPeriod_yoy',\n",
      "       'operatingCashFlow_yoy', 'freeCashFlow_yoy', 'symbol', 'date',\n",
      "       'symbol_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(income_clean.columns)\n",
    "print(balance_clean.columns)\n",
    "print(cashflow_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fed1b04-d0c6-4593-a018-e0f53440299b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INCOME FIRST 5 SYMBOLS:\n",
      "['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACGL']\n",
      "\n",
      "INCOME LAST 5 SYMBOLS:\n",
      "['XYZ', 'YUM', 'ZBH', 'ZBRA', 'ZTS']\n",
      "\n",
      "BALANCE FIRST 5 SYMBOLS:\n",
      "['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACGL']\n",
      "\n",
      "BALANCE LAST 5 SYMBOLS:\n",
      "['XYZ', 'YUM', 'ZBH', 'ZBRA', 'ZTS']\n",
      "\n",
      "CASHFLOW FIRST 5 SYMBOLS:\n",
      "['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACGL']\n",
      "\n",
      "CASHFLOW LAST 5 SYMBOLS:\n",
      "['XYZ', 'YUM', 'ZBH', 'ZBRA', 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "for name, df in {\"income\": income_clean, \"balance\": balance_clean, \"cashflow\": cashflow_clean}.items():\n",
    "    print(f\"\\n{name.upper()} FIRST 5 SYMBOLS:\")\n",
    "    print(df[\"symbol\"].head(5).to_list())\n",
    "    print(f\"\\n{name.upper()} LAST 5 SYMBOLS:\")\n",
    "    print(df[\"symbol\"].tail(5).to_list())\n",
    "    \n",
    "# just a check that the symbols are the same and aligned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd4fef13-9c56-4801-86b3-1d7904fc3e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pca(df, selected_cols, n_components=5, scale=True):\n",
    "    \"\"\"\n",
    "    Run PCA on selected columns with optional scaling and visualize:\n",
    "      - Column variances\n",
    "      - Explained variance ratio of PCs\n",
    "    \"\"\"\n",
    "    sub_df = df[selected_cols].copy()\n",
    "    \n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        scaled = scaler.fit_transform(sub_df)\n",
    "        data_to_use = scaled\n",
    "        title_suffix = \" (post-standardization)\"\n",
    "    else:\n",
    "        data_to_use = sub_df.values\n",
    "        title_suffix = \" (raw)\"\n",
    "    \n",
    "    # Compute PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(data_to_use)\n",
    "    pcs_df = pd.DataFrame(pcs, index=sub_df.index, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "    \n",
    "    # Print some info\n",
    "    print(f\"PCA shape: {pcs_df.shape}\")\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "    return pcs_df, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512f12a1-643f-458b-ae19-f8fb82a8b909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA shape: (491, 3)\n",
      "Explained variance ratio: [0.34710423 0.34377133 0.30912445]\n",
      "PCA shape: (491, 3)\n",
      "Explained variance ratio: [0.646486  0.3168122 0.0367018]\n",
      "PCA shape: (491, 3)\n",
      "Explained variance ratio: [6.80308141e-01 3.19691688e-01 1.71405147e-07]\n"
     ]
    }
   ],
   "source": [
    "income_cols = [\"revenue_yoy\",\"netIncome_yoy\",\"operatingIncomeRatio_yoy\"]\n",
    "balance_cols = [\"totalAssets_yoy\",\"totalLiabilities_yoy\",\"totalStockholdersEquity_yoy\"]\n",
    "cashflow_cols = [\"operatingCashFlow_yoy\",\"freeCashFlow_yoy\",\"netCashProvidedByOperatingActivities_yoy\"]\n",
    "\n",
    "income_pcs, income_pca_model = run_pca(aligned[\"income\"], income_cols, n_components=3)\n",
    "balance_pcs, balance_pca_model = run_pca(aligned[\"balance\"], balance_cols, n_components=3)\n",
    "cashflow_pcs, cashflow_pca_model = run_pca(aligned[\"cashflow\"], cashflow_cols, n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61229223-2e01-45a1-be31-ff084145b6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491, 4)\n",
      "(491, 4)\n",
      "(491, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(480, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_pcs[\"symbol\"] = aligned[\"income\"][\"symbol\"].values\n",
    "balance_pcs[\"symbol\"] = aligned[\"balance\"][\"symbol\"].values\n",
    "cashflow_pcs[\"symbol\"] = aligned[\"cashflow\"][\"symbol\"].values\n",
    "\n",
    "print(cashflow_pcs.shape)\n",
    "print(balance_pcs.shape)\n",
    "print(income_pcs.shape)\n",
    "\n",
    "cashflow_pcs.columns   \n",
    "pe_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d13adc9-7452-4516-93d4-c67b94e457e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (468, 5)\n"
     ]
    }
   ],
   "source": [
    "all_pcs = pd.concat(\n",
    "    [income_pcs, balance_pcs, cashflow_pcs], axis=1\n",
    ")\n",
    "\n",
    "all_pcs = all_pcs.loc[:, ~all_pcs.columns.duplicated()]\n",
    "\n",
    "\n",
    "merged = pd.merge(all_pcs, pe_data[[\"symbol\", \"log_PE\"]], on=\"symbol\")\n",
    "print(\"Merged shape:\", merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a5538ec-a122-47a6-854a-868ff37d0512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       variable      coef   std_err  R_squared  quartile\n",
      "0         const  2.512168  0.025053   0.026304         1\n",
      "1           PC1 -0.017309  0.016511   0.026304         1\n",
      "2           PC2 -0.013928  0.014614   0.026304         1\n",
      "3           PC3  0.022537  0.016594   0.026304         1\n",
      "4   PE_quartile  0.000000  0.000000   0.026304         1\n",
      "5         const  1.521359  0.006073   0.010191         2\n",
      "6           PC1 -0.003696  0.041033   0.010191         2\n",
      "7           PC2 -0.127523  0.154594   0.010191         2\n",
      "8           PC3 -0.088242  0.098536   0.010191         2\n",
      "9   PE_quartile  1.521359  0.006073   0.010191         2\n",
      "10        const  0.677555  0.002584   0.003285         3\n",
      "11          PC1  0.054271  0.093405   0.003285         3\n",
      "12          PC2  0.022564  0.099015   0.003285         3\n",
      "13          PC3  0.092381  0.170751   0.003285         3\n",
      "14  PE_quartile  1.355111  0.005169   0.003285         3\n",
      "15        const  0.394228  0.009205   0.212034         4\n",
      "16          PC1 -2.504433  0.655548   0.212034         4\n",
      "17          PC2 -0.994669  1.376141   0.212034         4\n",
      "18          PC3 -3.989013  0.825462   0.212034         4\n",
      "19  PE_quartile  1.182684  0.027616   0.212034         4\n"
     ]
    }
   ],
   "source": [
    "# Assume merged has all the features and log_PE\n",
    "X = merged.drop(columns=[\"symbol\", \"log_PE\"])\n",
    "y = merged[\"log_PE\"]\n",
    "\n",
    "# Add a constant for intercept\n",
    "X_const = sm.add_constant(X)\n",
    "\n",
    "# Create quartiles based on log_PE\n",
    "merged[\"PE_quartile\"] = pd.qcut(merged[\"log_PE\"], 4, labels=False)  # 0=lowest, 3=highest\n",
    "\n",
    "# Prepare a DataFrame to store results\n",
    "results_df = pd.DataFrame()a\n",
    "\n",
    "# Loop through quartiles\n",
    "for q in range(4):\n",
    "    mask = merged[\"PE_quartile\"] == q\n",
    "    X_q = X_const[mask]\n",
    "    y_q = y[mask]\n",
    "    \n",
    "    model_q = sm.OLS(y_q, X_q).fit()\n",
    "    \n",
    "    # Store coefficients and standard errors\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"variable\": model_q.params.index,\n",
    "        \"coef\": model_q.params.values,\n",
    "        \"std_err\": model_q.bse.values,\n",
    "        \"R_squared\": model_q.rsquared,\n",
    "        \"quartile\": q+1  # human-friendly 1-4\n",
    "    })\n",
    "    \n",
    "    results_df = pd.concat([results_df, coef_df], ignore_index=True)\n",
    "\n",
    "# Show results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6841963a-74ea-4215-8852-bbd2b5cd669f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income 1, Balance 1, Cashflow 1, R²: 0.011\n",
      "Income 1, Balance 1, Cashflow 2, R²: 0.013\n",
      "Income 1, Balance 1, Cashflow 3, R²: 0.044\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \"AAPL\" at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2369\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to parse string \"AAPL\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m X_subset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[1;32m     15\u001b[0m     merged[income_pcs_cols[:k_income]],\n\u001b[1;32m     16\u001b[0m     merged[balance_pcs_cols[:k_balance]],\n\u001b[1;32m     17\u001b[0m     merged[cashflow_pcs_cols[:k_cashflow]]\n\u001b[1;32m     18\u001b[0m ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert only numeric columns to float\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m X_subset \u001b[38;5;241m=\u001b[39m \u001b[43mX_subset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraise\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Add intercept\u001b[39;00m\n\u001b[1;32m     24\u001b[0m X_subset \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39madd_constant(X_subset)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9557\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9559\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9561\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9566\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9567\u001b[0m )\n\u001b[0;32m-> 9568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    909\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    910\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    911\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/apply.py:142\u001b[0m, in \u001b[0;36mApply.__init__.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/tools/numeric.py:185\u001b[0m, in \u001b[0;36mto_numeric\u001b[0;34m(arg, errors, downcast)\u001b[0m\n\u001b[1;32m    183\u001b[0m coerce_numeric \u001b[38;5;241m=\u001b[39m errors \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     values, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_numeric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoerce_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_numeric\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2411\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to parse string \"AAPL\" at position 0"
     ]
    }
   ],
   "source": [
    "# --- Define PCA column groups ---\n",
    "income_pcs_cols = [c for c in merged.columns if c in income_pcs.columns]\n",
    "balance_pcs_cols = [c for c in merged.columns if c in balance_pcs.columns]\n",
    "cashflow_pcs_cols = [c for c in merged.columns if c in cashflow_pcs.columns]\n",
    "\n",
    "# --- Hyperparameter loop ---\n",
    "results = []\n",
    "\n",
    "for k_income in range(1, len(income_pcs_cols)+1):\n",
    "    for k_balance in range(1, len(balance_pcs_cols)+1):\n",
    "        for k_cashflow in range(1, len(cashflow_pcs_cols)+1):\n",
    "            \n",
    "            # Subset first k PCs for each category\n",
    "            X_subset = pd.concat([\n",
    "                merged[income_pcs_cols[:k_income]],\n",
    "                merged[balance_pcs_cols[:k_balance]],\n",
    "                merged[cashflow_pcs_cols[:k_cashflow]]\n",
    "            ], axis=1)\n",
    "            \n",
    "            # Drop non-numeric identifiers\n",
    "            X_subset = X_subset.drop(columns=[\"symbol\"], errors=\"ignore\")\n",
    "\n",
    "            # Ensure numeric\n",
    "            X_subset = X_subset.astype(float)\n",
    "         \n",
    "            # Convert only numeric columns to float\n",
    "            X_subset = X_subset.apply(pd.to_numeric, errors='raise')\n",
    "            \n",
    "            # Add intercept\n",
    "            X_subset = sm.add_constant(X_subset)\n",
    "\n",
    "            # Response variable\n",
    "            y = pd.to_numeric(merged[\"log_PE\"], errors='raise')\n",
    "            \n",
    "            # Fit OLS\n",
    "            model = sm.OLS(y, X_subset).fit()\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"k_income\": k_income,\n",
    "                \"k_balance\": k_balance,\n",
    "                \"k_cashflow\": k_cashflow,\n",
    "                \"R_squared\": model.rsquared\n",
    "            })\n",
    "            \n",
    "            print(f\"Income {k_income}, Balance {k_balance}, Cashflow {k_cashflow}, R²: {model.rsquared:.3f}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4b15a-dc38-4987-98af-029cf9d16292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
