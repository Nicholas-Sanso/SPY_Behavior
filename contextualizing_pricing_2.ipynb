{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9ed79e-112a-418f-b955-5c13dd46dd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3d2046-ea29-42a8-94a4-d33602247506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from JSON cache...\n",
      "         date symbol reportedCurrency         cik fillingDate  \\\n",
      "0  2024-09-28   AAPL              USD  0000320193  2024-11-01   \n",
      "1  2024-06-30   MSFT              USD  0000789019  2024-07-30   \n",
      "2  2024-12-31   AMZN              USD  0001018724  2025-02-07   \n",
      "3  2025-01-26   NVDA              USD  0001045810  2025-02-26   \n",
      "4  2024-12-31  GOOGL              USD  0001652044  2025-02-05   \n",
      "\n",
      "          acceptedDate calendarYear period       revenue  costOfRevenue  ...  \\\n",
      "0  2024-11-01 06:01:36         2024     FY  391035000000   210352000000  ...   \n",
      "1  2024-07-30 16:06:22         2024     FY  245122000000    74114000000  ...   \n",
      "2  2025-02-06 18:40:29         2024     FY  637959000000   326288000000  ...   \n",
      "3  2025-02-26 16:48:33         2025     FY  130497000000    32639000000  ...   \n",
      "4  2025-02-04 20:41:40         2024     FY  350018000000   146306000000  ...   \n",
      "\n",
      "   incomeBeforeTaxRatio  incomeTaxExpense     netIncome  netIncomeRatio  \\\n",
      "0              0.315790       29749000000   93736000000        0.239713   \n",
      "1              0.439728       19651000000   88136000000        0.359560   \n",
      "2              0.107394        9265000000   59248000000        0.092871   \n",
      "3              0.643892       11146000000   72880000000        0.558480   \n",
      "4              0.342311       19697000000  100118000000        0.286037   \n",
      "\n",
      "     eps  epsdiluted  weightedAverageShsOut  weightedAverageShsOutDil  \\\n",
      "0   6.11        6.08            15343783000               15408095000   \n",
      "1  11.86       11.80             7431000000                7469000000   \n",
      "2   5.66        5.53            10473000000               10721000000   \n",
      "3   2.97        2.94            24555000000               24804000000   \n",
      "4   8.04        8.04            12319000000               12447000000   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://www.sec.gov/Archives/edgar/data/320193...   \n",
      "1  https://www.sec.gov/Archives/edgar/data/789019...   \n",
      "2  https://www.sec.gov/Archives/edgar/data/101872...   \n",
      "3  https://www.sec.gov/Archives/edgar/data/104581...   \n",
      "4  https://www.sec.gov/Archives/edgar/data/165204...   \n",
      "\n",
      "                                           finalLink  \n",
      "0  https://www.sec.gov/Archives/edgar/data/320193...  \n",
      "1  https://www.sec.gov/Archives/edgar/data/789019...  \n",
      "2  https://www.sec.gov/Archives/edgar/data/101872...  \n",
      "3  https://www.sec.gov/Archives/edgar/data/104581...  \n",
      "4  https://www.sec.gov/Archives/edgar/data/165204...  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# Set up data folder and file paths\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "csv_file = os.path.join(data_folder, \"spy_tickers_sample.csv\")\n",
    "json_file = os.path.join(data_folder, \"income_statements.json\")\n",
    "\n",
    "# Define which fields to keep\n",
    "desired_fields = [\n",
    "    'symbol',\n",
    "    'date',\n",
    "    'revenue',\n",
    "    'grossProfit',\n",
    "    'operatingIncome',\n",
    "    'netIncome',\n",
    "    'eps',\n",
    "    'ebitda',\n",
    "    'costOfRevenue',\n",
    "    'operatingExpenses'\n",
    "]\n",
    "\n",
    "# Try loading from cached JSON if it exists\n",
    "if os.path.exists(json_file):\n",
    "    print(\"Loading data from JSON cache...\")\n",
    "    with open(json_file, 'r') as f:\n",
    "        records = json.load(f)\n",
    "else:\n",
    "    # Load tickers\n",
    "    df = pd.read_csv(csv_file)\n",
    "    tickers = df['Ticker'].dropna().tolist()\n",
    "\n",
    "    API_KEY = 'YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6'\n",
    "    records = []\n",
    "\n",
    "    # Fetch most recent income statement per ticker\n",
    "    for ticker in tickers:\n",
    "        url = f'https://financialmodelingprep.com/api/v3/income-statement/{ticker}?limit=1&apikey={API_KEY}'\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                row = data[0]\n",
    "                row['symbol'] = ticker\n",
    "                filtered_row = {k: row.get(k, None) for k in desired_fields}\n",
    "                records.append(filtered_row)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "        time.sleep(1)  # Rate limiting\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(\"Saved data to JSON cache.\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "income_df = pd.DataFrame(records)\n",
    "\n",
    "# Show sample\n",
    "print(income_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc5f342-6179-42f1-9306-36d6c6a41ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading growth data from JSON cache...\n",
      "Columns in growth DataFrame: ['symbol', 'date', 'fiscalYear', 'period', 'reportedCurrency', 'growthRevenue', 'growthCostOfRevenue', 'growthGrossProfit', 'growthGrossProfitRatio', 'growthResearchAndDevelopmentExpenses', 'growthGeneralAndAdministrativeExpenses', 'growthSellingAndMarketingExpenses', 'growthOtherExpenses', 'growthOperatingExpenses', 'growthCostAndExpenses', 'growthInterestIncome', 'growthInterestExpense', 'growthDepreciationAndAmortization', 'growthEBITDA', 'growthOperatingIncome', 'growthIncomeBeforeTax', 'growthIncomeTaxExpense', 'growthNetIncome', 'growthEPS', 'growthEPSDiluted', 'growthWeightedAverageShsOut', 'growthWeightedAverageShsOutDil', 'growthEBIT', 'growthNonOperatingIncomeExcludingInterest', 'growthNetInterestIncome', 'growthTotalOtherIncomeExpensesNet', 'growthNetIncomeFromContinuingOperations', 'growthOtherAdjustmentsToNetIncome', 'growthNetIncomeDeductions']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 34)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "data_folder  = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "csv_file     = os.path.join(data_folder, \"spy_tickers_sample.csv\")\n",
    "output_file  = os.path.join(data_folder, \"income_growth.json\")\n",
    "\n",
    "# API setup\n",
    "API_KEY  = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "BASE_URL = \"https://financialmodelingprep.com/stable/income-statement-growth\"\n",
    "\n",
    "# Try loading from cache\n",
    "if os.path.exists(output_file):\n",
    "    print(\"Loading growth data from JSON cache...\")\n",
    "    with open(output_file, \"r\") as f:\n",
    "        growth_records = json.load(f)\n",
    "\n",
    "else:\n",
    "    print(\"No cache found — fetching growth data from API...\")\n",
    "    # Load tickers\n",
    "    df       = pd.read_csv(csv_file)\n",
    "    tickers  = df['Ticker'].tolist()\n",
    "    growth_records = []\n",
    "\n",
    "    # Fetch and flatten all periods (Solution B)\n",
    "    for ticker in tickers:\n",
    "        url = f\"{BASE_URL}?symbol={ticker}&apikey={API_KEY}\"\n",
    "        try:\n",
    "            resp = requests.get(url)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "\n",
    "            if isinstance(data, list) and data:\n",
    "                for period in data:\n",
    "                    period['symbol'] = ticker\n",
    "                    growth_records.append(period)\n",
    "            else:\n",
    "                print(f\"  • no growth data for {ticker}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "\n",
    "        time.sleep(1)  # respect rate limits\n",
    "\n",
    "    # Save to JSON cache\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(growth_records, f, indent=2)\n",
    "    print(f\"Saved {len(growth_records)} records to JSON cache.\")\n",
    "\n",
    "# Convert to DataFrame and inspect\n",
    "df_growth = pd.DataFrame(growth_records)\n",
    "print(\"Columns in growth DataFrame:\", df_growth.columns.tolist())\n",
    "df_growth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fffff96-d96e-45dc-a7d7-47616b48c94b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ratios from JSON cache...\n",
      "  symbol        date calendarYear period  currentRatio  quickRatio  cashRatio  \\\n",
      "0   AAPL  2024-09-28         2024     FY      0.867313    0.826007   0.169753   \n",
      "1   MSFT  2025-06-30         2025     FY      1.353446    1.346804   0.214151   \n",
      "2   AMZN  2024-12-31         2024     FY      1.063735    0.873054   0.439049   \n",
      "3   NVDA  2025-01-26         2025     FY      4.439851    3.881310   0.475924   \n",
      "4  GOOGL  2024-12-31         2024     FY      1.836931    1.836931   0.263302   \n",
      "\n",
      "   daysOfSalesOutstanding  daysOfInventoryOutstanding  operatingCycle  ...  \\\n",
      "0               61.832560                   12.642571       74.475130  ...   \n",
      "1               90.568517                    3.898054       94.466572  ...   \n",
      "2               31.725573                   38.273274       69.998847  ...   \n",
      "3               64.512786                  112.724042      177.236828  ...   \n",
      "4               54.580336                    0.000000       54.580336  ...   \n",
      "\n",
      "   priceToSalesRatio  priceEarningsRatio  priceToFreeCashFlowsRatio  \\\n",
      "0           8.938229           37.287278                  32.122569   \n",
      "1          13.123655           36.307335                  51.629617   \n",
      "2           3.601597           38.780574                  69.884770   \n",
      "3          22.282528           39.898506                  47.784055   \n",
      "4           6.662477           23.292382                  32.048633   \n",
      "\n",
      "   priceToOperatingCashFlowsRatio  priceCashFlowRatio  \\\n",
      "0                       29.556381           29.556381   \n",
      "1                       27.153307           27.153307   \n",
      "2                       19.828538           19.828538   \n",
      "3                       45.371329           45.371329   \n",
      "4                       18.611375           18.611375   \n",
      "\n",
      "   priceEarningsToGrowthRatio  priceSalesRatio  dividendYield  \\\n",
      "0                  -45.937927         8.938229       0.004359   \n",
      "1                    2.340245        13.123655       0.006513   \n",
      "2                    0.422150         3.601597       0.000000   \n",
      "3                    0.274302        22.282528       0.000287   \n",
      "4                    0.618307         6.662477       0.003157   \n",
      "\n",
      "   enterpriseValueMultiple  priceFairValue  \n",
      "0                26.617033       61.372438  \n",
      "1                23.273465       10.764118  \n",
      "2                18.978254        8.034659  \n",
      "3                33.777402       36.655907  \n",
      "4                17.238443        7.173490  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "tickers_csv_path = os.path.join(data_folder, \"spy_tickers_sample.csv\")\n",
    "output_json_path = os.path.join(data_folder, \"ratios.json\")\n",
    "\n",
    "# Try loading from cache\n",
    "if os.path.exists(output_json_path):\n",
    "    print(\"Loading ratios from JSON cache...\")\n",
    "    with open(output_json_path, \"r\") as f:\n",
    "        all_ratios = json.load(f)\n",
    "else:\n",
    "    print(\"No cache found — fetching ratios from API...\")\n",
    "    tickers_df = pd.read_csv(tickers_csv_path)\n",
    "    tickers = tickers_df['Ticker'].dropna().unique().tolist()\n",
    "\n",
    "    api_key = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "    base_url = \"https://financialmodelingprep.com/api/v3/ratios/{}?limit=1&apikey={}\"\n",
    "\n",
    "    all_ratios = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            url = base_url.format(ticker, api_key)\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                all_ratios.append(data[0])\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "\n",
    "    # Save to cache\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(all_ratios, f, indent=2)\n",
    "    print(f\"Saved {len(all_ratios)} records to JSON cache.\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_ratios = pd.DataFrame(all_ratios)\n",
    "print(df_ratios.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17ffdf80-2c08-4501-b116-0ca68b785655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache found — fetching company info from API...\n",
      "Found 100 unique tickers.\n",
      "Saved 100 company records to cache.\n",
      "Columns returned: ['symbol', 'companyName', 'marketCap', 'sector', 'industry', 'beta', 'price', 'lastAnnualDividend', 'volume', 'exchange', 'exchangeShortName', 'country', 'isEtf', 'isFund']\n",
      "   symbol                   companyName marketCap                  sector  \\\n",
      "0    AAPL                    Apple Inc.      None              Technology   \n",
      "1    MSFT         Microsoft Corporation      None              Technology   \n",
      "2    AMZN              Amazon.com, Inc.      None       Consumer Cyclical   \n",
      "3    NVDA            NVIDIA Corporation      None              Technology   \n",
      "4   GOOGL                 Alphabet Inc.      None  Communication Services   \n",
      "..    ...                           ...       ...                     ...   \n",
      "95     WM        Waste Management, Inc.      None             Industrials   \n",
      "96    PSX                   Phillips 66      None                  Energy   \n",
      "97      D         Dominion Energy, Inc.      None               Utilities   \n",
      "98    HCA          HCA Healthcare, Inc.      None              Healthcare   \n",
      "99   MNST  Monster Beverage Corporation      None      Consumer Defensive   \n",
      "\n",
      "                          industry   beta   price lastAnnualDividend volume  \\\n",
      "0             Consumer Electronics  1.165  202.92               None   None   \n",
      "1        Software - Infrastructure  1.055  527.75               None   None   \n",
      "2                 Specialty Retail  1.314  213.75               None   None   \n",
      "3                   Semiconductors  2.145  178.26               None   None   \n",
      "4   Internet Content & Information  1.014  194.67               None   None   \n",
      "..                             ...    ...     ...                ...    ...   \n",
      "95                Waste Management  0.644  229.78               None   None   \n",
      "96  Oil & Gas Refining & Marketing  1.064  122.07               None   None   \n",
      "97              Regulated Electric  0.560   61.08               None   None   \n",
      "98       Medical - Care Facilities  1.400  363.81               None   None   \n",
      "99       Beverages - Non-Alcoholic  0.551   59.04               None   None   \n",
      "\n",
      "                   exchange exchangeShortName country  isEtf  isFund  \n",
      "0      NASDAQ Global Select            NASDAQ      US  False   False  \n",
      "1      NASDAQ Global Select            NASDAQ      US  False   False  \n",
      "2      NASDAQ Global Select            NASDAQ      US  False   False  \n",
      "3      NASDAQ Global Select            NASDAQ      US  False   False  \n",
      "4      NASDAQ Global Select            NASDAQ      US  False   False  \n",
      "..                      ...               ...     ...    ...     ...  \n",
      "95  New York Stock Exchange              NYSE      US  False   False  \n",
      "96  New York Stock Exchange              NYSE      US  False   False  \n",
      "97  New York Stock Exchange              NYSE      US  False   False  \n",
      "98  New York Stock Exchange              NYSE      US  False   False  \n",
      "99     NASDAQ Global Select            NASDAQ      US  False   False  \n",
      "\n",
      "[100 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, requests\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Set up paths\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "csv_file    = os.path.join(data_folder, \"spy_tickers_sample.csv\")\n",
    "json_file   = os.path.join(data_folder, \"company_info.json\")\n",
    "\n",
    "# 2. Define the fields you want to keep\n",
    "desired_fields = [\n",
    "    \"symbol\", \"companyName\", \"marketCap\", \"sector\", \"industry\", \"beta\",\n",
    "    \"price\", \"lastAnnualDividend\", \"volume\", \"exchange\", \"exchangeShortName\",\n",
    "    \"country\", \"isEtf\", \"isFund\"\n",
    "]\n",
    "\n",
    "# 3. Load or fetch company info\n",
    "records = []\n",
    "\n",
    "if os.path.exists(json_file):\n",
    "    print(\"Loading company info from JSON cache...\")\n",
    "    with open(json_file, \"r\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "else:\n",
    "    print(\"No cache found — fetching company info from API...\")\n",
    "    df      = pd.read_csv(csv_file)\n",
    "    tickers = df['Ticker'].dropna().unique().tolist()\n",
    "    print(f\"Found {len(tickers)} unique tickers.\")\n",
    "\n",
    "    API_KEY  = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "    BASE_URL = \"https://financialmodelingprep.com/api/v3/profile/{}?apikey={}\"\n",
    "\n",
    "    failed = []\n",
    "\n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        url = BASE_URL.format(ticker, API_KEY)\n",
    "        try:\n",
    "            resp = requests.get(url)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            if isinstance(data, list) and data:\n",
    "                row = data[0]\n",
    "                filtered = {k: row.get(k, None) for k in desired_fields}\n",
    "                records.append(filtered)\n",
    "            else:\n",
    "                print(f\"  • No profile data for {ticker}\")\n",
    "                failed.append(ticker)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "            failed.append(ticker)\n",
    "        time.sleep(1)\n",
    "\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} company records to cache.\")\n",
    "    if failed:\n",
    "        print(f\"{len(failed)} tickers failed to fetch: {failed[:5]}...\")\n",
    "\n",
    "# 4. Convert to DataFrame and inspect\n",
    "company_df = pd.DataFrame(records)\n",
    "company_df.drop_duplicates(subset=\"symbol\", inplace=True)\n",
    "\n",
    "print(\"Columns returned:\", company_df.columns.tolist())\n",
    "print(company_df.head(101))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b14bd7-484b-4c26-8c10-5477475b67a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'P_E'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'P_E'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert columns to numeric, coercing errors to NaN\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_growth[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_E\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(\u001b[43mdf_growth\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP_E\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_growth[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrowthEPSDiluted\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df_growth[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrowthEPSDiluted\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Drop rows with missing values in P_E or growthEPSDiluted\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'P_E'"
     ]
    }
   ],
   "source": [
    "# Convert columns to numeric, coercing errors to NaN\n",
    "df_growth['priceEarningsRatio'] = pd.to_numeric(df_growth['priceEarningsRatio'], errors='coerce')\n",
    "df_growth['growthEPSDiluted'] = pd.to_numeric(df_growth['growthEPSDiluted'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in P_E or growthEPSDiluted\n",
    "reg_df = df_growth.dropna(subset=['priceEarningsRatio', 'growthEPSDiluted']).copy()\n",
    "\n",
    "# Log-transform the P/E ratio\n",
    "reg_df['ln_PE'] = np.log(reg_df['P_E'])\n",
    "\n",
    "# Setup regression\n",
    "X = sm.add_constant(reg_df['growthEPSDiluted'])\n",
    "y = reg_df['ln_PE']\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "print(''' the coefficient is the expected p/e when the growth is 0. the coefficient beta is the expected percentage increase in the p/e per unit increase in the independent var, aka the growth  ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
