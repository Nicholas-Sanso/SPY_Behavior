{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9ed79e-112a-418f-b955-5c13dd46dd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2982057a-32e3-43e6-b761-6240815efa25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tickers from CSV cache...\n",
      "(503, 8)\n",
      "Index(['symbol', 'name', 'sector', 'subSector', 'headQuarter',\n",
      "       'dateFirstAdded', 'cik', 'founded'],\n",
      "      dtype='object')\n",
      "  symbol                  name                  sector  \\\n",
      "0    XYZ           Block, Inc.              Technology   \n",
      "1    TTD  The Trade Desk, Inc.              Technology   \n",
      "2   DDOG               Datadog              Technology   \n",
      "3   COIN       Coinbase Global      Financial Services   \n",
      "4   DASH              DoorDash  Communication Services   \n",
      "\n",
      "                            subSector              headQuarter dateFirstAdded  \\\n",
      "0           Software - Infrastructure      Oakland, California     2025-07-23   \n",
      "1              Software - Application      Ventura, California     2025-07-18   \n",
      "2              Software - Application  New York City, New York     2025-07-09   \n",
      "3  Financial - Data & Stock Exchanges     Wilmington, Delaware     2025-05-19   \n",
      "4      Internet Content & Information        San Francisco, CA     2025-03-24   \n",
      "\n",
      "       cik founded  \n",
      "0  1512673    2009  \n",
      "1  1671933    2009  \n",
      "2  1561550    2010  \n",
      "3  1679788    2012  \n",
      "4  1792789    2013  \n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "# --- LOAD FROM CACHE OR FETCH ---\n",
    "if os.path.exists(tickers_csv_file):\n",
    "    print(\"Loading tickers from CSV cache...\")\n",
    "    df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "else:\n",
    "    print(\"Fetching tickers from API...\")\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/sp500_constituent?apikey={API_KEY}\"\n",
    "    df_sp500 = pd.DataFrame(requests.get(url).json())\n",
    "\n",
    "    # Save to CSV\n",
    "    df_sp500.to_csv(tickers_csv_file, index=False)\n",
    "    print(f\"Saved {len(df_sp500)} tickers to CSV cache.\")\n",
    "\n",
    "    \n",
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()\n",
    "# --- PREVIEW ---\n",
    "print(df_sp500.shape)\n",
    "print(df_sp500.columns)\n",
    "print(df_sp500.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7840d0-3280-4f6b-8164-3d1996487ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/price_and_earnings.json\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "# Load tickers\n",
    "df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()\n",
    "\n",
    "# Output file\n",
    "output_file = os.path.join(data_folder, \"price_and_earnings.json\")\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Get current price\n",
    "            quote_url = f\"https://financialmodelingprep.com/api/v3/quote/{ticker}?apikey={API_KEY}\"\n",
    "            price_data = requests.get(quote_url).json()\n",
    "            if not price_data:\n",
    "                continue\n",
    "            price = price_data[0][\"price\"]\n",
    "\n",
    "            # Get latest annual income statement (EPS or netIncome)\n",
    "            income_url = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?limit=1&apikey={API_KEY}\"\n",
    "            income_data = requests.get(income_url).json()\n",
    "            if not income_data:\n",
    "                continue\n",
    "            eps = income_data[0].get(\"eps\")\n",
    "            net_income = income_data[0].get(\"netIncome\")\n",
    "\n",
    "            records.append({\n",
    "                \"symbol\": ticker,\n",
    "                \"price\": price,\n",
    "                \"eps\": eps,\n",
    "                \"netIncome\": net_income\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "        time.sleep(0.2)  # polite rate limit\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "price_earnings_df = fetch_price_and_earnings(tickers, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb737c07-dc25-4514-858c-f5178907b5be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows used: 480\n",
      "0    2.741115\n",
      "1    4.033134\n",
      "2    5.514162\n",
      "3    3.434049\n",
      "4    6.756855\n",
      "Name: log_PE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Only keep rows with positive EPS\n",
    "pe_data = price_earnings_df[price_earnings_df[\"eps\"] > 0].copy()\n",
    "\n",
    "# Compute log(PE)\n",
    "pe_data[\"log_PE\"] = np.log(pe_data[\"price\"] / pe_data[\"eps\"])\n",
    "\n",
    "# Optional: store just the series if you want\n",
    "pe_series = pe_data[\"log_PE\"]\n",
    "\n",
    "# Print row count for reference\n",
    "print(f\"Rows used: {len(pe_series)}\")\n",
    "print(pe_series.head())\n",
    "\n",
    "# we are only pulling the eps and takingthe log here.. the code that follows doesn't immediately and directly build on this code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031400b-6a30-4ff6-ab53-72963cfe5a96",
   "metadata": {},
   "source": [
    "## break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1844560d-77dc-4f2a-8993-96bb263662be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/income-statement_annual_limit2.json\n",
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/balance-sheet-statement_annual_limit2.json\n",
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/cash-flow-statement_annual_limit2.json\n",
      "Income shape: (1006, 38)\n",
      "Balance shape: (1006, 54)\n",
      "Cash flow shape: (1006, 40)\n",
      "Index(['date', 'symbol', 'reportedCurrency', 'cik', 'fillingDate',\n",
      "       'acceptedDate', 'calendarYear', 'period', 'revenue', 'costOfRevenue',\n",
      "       'grossProfit', 'grossProfitRatio', 'researchAndDevelopmentExpenses',\n",
      "       'generalAndAdministrativeExpenses', 'sellingAndMarketingExpenses',\n",
      "       'sellingGeneralAndAdministrativeExpenses', 'otherExpenses',\n",
      "       'operatingExpenses', 'costAndExpenses', 'interestIncome',\n",
      "       'interestExpense', 'depreciationAndAmortization', 'ebitda',\n",
      "       'ebitdaratio', 'operatingIncome', 'operatingIncomeRatio',\n",
      "       'totalOtherIncomeExpensesNet', 'incomeBeforeTax',\n",
      "       'incomeBeforeTaxRatio', 'incomeTaxExpense', 'netIncome',\n",
      "       'netIncomeRatio', 'eps', 'epsdiluted', 'weightedAverageShsOut',\n",
      "       'weightedAverageShsOutDil', 'link', 'finalLink'],\n",
      "      dtype='object')\n",
      "Index(['date', 'symbol', 'reportedCurrency', 'cik', 'fillingDate',\n",
      "       'acceptedDate', 'calendarYear', 'period', 'cashAndCashEquivalents',\n",
      "       'shortTermInvestments', 'cashAndShortTermInvestments', 'netReceivables',\n",
      "       'inventory', 'otherCurrentAssets', 'totalCurrentAssets',\n",
      "       'propertyPlantEquipmentNet', 'goodwill', 'intangibleAssets',\n",
      "       'goodwillAndIntangibleAssets', 'longTermInvestments', 'taxAssets',\n",
      "       'otherNonCurrentAssets', 'totalNonCurrentAssets', 'otherAssets',\n",
      "       'totalAssets', 'accountPayables', 'shortTermDebt', 'taxPayables',\n",
      "       'deferredRevenue', 'otherCurrentLiabilities', 'totalCurrentLiabilities',\n",
      "       'longTermDebt', 'deferredRevenueNonCurrent',\n",
      "       'deferredTaxLiabilitiesNonCurrent', 'otherNonCurrentLiabilities',\n",
      "       'totalNonCurrentLiabilities', 'otherLiabilities',\n",
      "       'capitalLeaseObligations', 'totalLiabilities', 'preferredStock',\n",
      "       'commonStock', 'retainedEarnings',\n",
      "       'accumulatedOtherComprehensiveIncomeLoss',\n",
      "       'othertotalStockholdersEquity', 'totalStockholdersEquity',\n",
      "       'totalEquity', 'totalLiabilitiesAndStockholdersEquity',\n",
      "       'minorityInterest', 'totalLiabilitiesAndTotalEquity',\n",
      "       'totalInvestments', 'totalDebt', 'netDebt', 'link', 'finalLink'],\n",
      "      dtype='object')\n",
      "Index(['date', 'symbol', 'reportedCurrency', 'cik', 'fillingDate',\n",
      "       'acceptedDate', 'calendarYear', 'period', 'netIncome',\n",
      "       'depreciationAndAmortization', 'deferredIncomeTax',\n",
      "       'stockBasedCompensation', 'changeInWorkingCapital',\n",
      "       'accountsReceivables', 'inventory', 'accountsPayables',\n",
      "       'otherWorkingCapital', 'otherNonCashItems',\n",
      "       'netCashProvidedByOperatingActivities',\n",
      "       'investmentsInPropertyPlantAndEquipment', 'acquisitionsNet',\n",
      "       'purchasesOfInvestments', 'salesMaturitiesOfInvestments',\n",
      "       'otherInvestingActivites', 'netCashUsedForInvestingActivites',\n",
      "       'debtRepayment', 'commonStockIssued', 'commonStockRepurchased',\n",
      "       'dividendsPaid', 'otherFinancingActivites',\n",
      "       'netCashUsedProvidedByFinancingActivities',\n",
      "       'effectOfForexChangesOnCash', 'netChangeInCash', 'cashAtEndOfPeriod',\n",
      "       'cashAtBeginningOfPeriod', 'operatingCashFlow', 'capitalExpenditure',\n",
      "       'freeCashFlow', 'link', 'finalLink'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def fetch_statement(endpoint, tickers, period, limit, data_folder):\n",
    "    \"\"\"Fetch statements with unique JSON filename based on endpoint, period, limit.\"\"\"\n",
    "    output_file = os.path.join(\n",
    "        data_folder,\n",
    "        f\"{endpoint}_{period}_limit{limit}.json\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        with open(output_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        url = f\"https://financialmodelingprep.com/api/v3/{endpoint}/{ticker}?period={period}&limit={limit}&apikey={API_KEY}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                for row in data:\n",
    "                    row[\"symbol\"] = ticker\n",
    "                records.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker} ({endpoint}): {e}\")\n",
    "        time.sleep(.2)  # API polite rate limit\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "    return records\n",
    "\n",
    "\n",
    "income_data_2_years   = fetch_statement(\"income-statement\", tickers, \"annual\", 2, data_folder)\n",
    "balance_data_2_years  = fetch_statement(\"balance-sheet-statement\", tickers, \"annual\", 2, data_folder)\n",
    "cashflow_data_2_years = fetch_statement(\"cash-flow-statement\", tickers, \"annual\", 2, data_folder)\n",
    "\n",
    "# Convert to DataFrames\n",
    "income_data_2_years   = pd.DataFrame(income_data_2_years)\n",
    "balance_data_2_years  = pd.DataFrame(balance_data_2_years)\n",
    "cashflow_data_2_years = pd.DataFrame(cashflow_data_2_years)\n",
    "\n",
    "print(\"Income shape:\", income_data_2_years.shape)\n",
    "print(\"Balance shape:\", balance_data_2_years.shape)\n",
    "print(\"Cash flow shape:\", cashflow_data_2_years.shape)\n",
    "\n",
    "print(income_data_2_years.columns)\n",
    "print(balance_data_2_years.columns)\n",
    "print(cashflow_data_2_years.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6e9e1d-7967-4d9d-bedd-c234025a3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_symbol_date(df):\n",
    "    return df.sort_values([\"symbol\", \"date\"])\n",
    "\n",
    "\n",
    "def compute_yoy_growth(df, exclude_cols=[\"symbol\", \"date\",\"link\",\"finalLink\"]):\n",
    "    numeric_cols = df.select_dtypes(include=[float, int]).columns\n",
    "    numeric_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "    \n",
    "    df_growth = df.copy()\n",
    "    for col in numeric_cols:\n",
    "        df_growth[col + \"_yoy\"] = df.groupby(\"symbol\")[col].pct_change()\n",
    "    \n",
    "    return df_growth\n",
    "\n",
    "\n",
    "income_sorted = sort_by_symbol_date(income_data_2_years)\n",
    "balance_sorted = sort_by_symbol_date(balance_data_2_years)\n",
    "cashflow_sorted = sort_by_symbol_date(cashflow_data_2_years)\n",
    "\n",
    "income_growth = compute_yoy_growth(income_sorted)\n",
    "balance_growth = compute_yoy_growth(balance_sorted)\n",
    "cashflow_growth = compute_yoy_growth(cashflow_sorted)\n",
    "\n",
    "\n",
    "#we are sorting to make sure that hte dates are properly aligned before we perform out pct chagne method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130f17b0-b137-4386-b95c-58b08ed4a839",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income 0/NaN counts per YoY column:\n",
      "                                             zeros  nans\n",
      "revenue_yoy                                      0   503\n",
      "grossProfit_yoy                                  0   503\n",
      "costAndExpenses_yoy                              0   503\n",
      "ebitdaratio_yoy                                  0   503\n",
      "operatingIncome_yoy                              0   503\n",
      "operatingIncomeRatio_yoy                         0   503\n",
      "incomeBeforeTax_yoy                              0   503\n",
      "incomeBeforeTaxRatio_yoy                         0   503\n",
      "netIncomeRatio_yoy                               0   503\n",
      "ebitda_yoy                                       0   504\n",
      "operatingExpenses_yoy                            0   507\n",
      "costOfRevenue_yoy                                0   510\n",
      "totalOtherIncomeExpensesNet_yoy                  0   535\n",
      "generalAndAdministrativeExpenses_yoy             0   756\n",
      "netIncome_yoy                                    1   503\n",
      "eps_yoy                                          1   503\n",
      "epsdiluted_yoy                                   1   503\n",
      "depreciationAndAmortization_yoy                  1   508\n",
      "incomeTaxExpense_yoy                             1   508\n",
      "sellingGeneralAndAdministrativeExpenses_yoy      1   530\n",
      "otherExpenses_yoy                                1   642\n",
      "researchAndDevelopmentExpenses_yoy               1   792\n",
      "sellingAndMarketingExpenses_yoy                  3   841\n",
      "interestIncome_yoy                               4   667\n",
      "weightedAverageShsOut_yoy                        7   503\n",
      "grossProfitRatio_yoy                             9   503\n",
      "weightedAverageShsOutDil_yoy                     9   503\n",
      "interestExpense_yoy                              9   530\n",
      "\n",
      "Balance 0/NaN counts per YoY column:\n",
      "                                             zeros  nans\n",
      "otherNonCurrentAssets_yoy                        0   503\n",
      "totalAssets_yoy                                  0   503\n",
      "totalLiabilities_yoy                             0   503\n",
      "totalStockholdersEquity_yoy                      0   503\n",
      "totalEquity_yoy                                  0   503\n",
      "totalLiabilitiesAndStockholdersEquity_yoy        0   503\n",
      "totalLiabilitiesAndTotalEquity_yoy               0   503\n",
      "totalNonCurrentAssets_yoy                        0   504\n",
      "totalCurrentAssets_yoy                           0   505\n",
      "longTermDebt_yoy                                 0   505\n",
      "totalNonCurrentLiabilities_yoy                   0   505\n",
      "retainedEarnings_yoy                             0   505\n",
      "otherNonCurrentLiabilities_yoy                   0   508\n",
      "netReceivables_yoy                               0   513\n",
      "accountPayables_yoy                              0   530\n",
      "shortTermInvestments_yoy                         0   780\n",
      "otherAssets_yoy                                  0   971\n",
      "otherLiabilities_yoy                             0   985\n",
      "cashAndCashEquivalents_yoy                       1   503\n",
      "cashAndShortTermInvestments_yoy                  1   503\n",
      "totalDebt_yoy                                    1   504\n",
      "totalCurrentLiabilities_yoy                      1   508\n",
      "otherCurrentLiabilities_yoy                      1   510\n",
      "propertyPlantEquipmentNet_yoy                    1   511\n",
      "otherCurrentAssets_yoy                           1   536\n",
      "othertotalStockholdersEquity_yoy                 1   547\n",
      "totalInvestments_yoy                             1   602\n",
      "longTermInvestments_yoy                          1   658\n",
      "deferredTaxLiabilitiesNonCurrent_yoy             1   665\n",
      "taxPayables_yoy                                  1   674\n",
      "deferredRevenue_yoy                              1   731\n",
      "netDebt_yoy                                      2   503\n",
      "capitalLeaseObligations_yoy                      2   605\n",
      "taxAssets_yoy                                    2   752\n",
      "inventory_yoy                                    3   666\n",
      "deferredRevenueNonCurrent_yoy                    3   869\n",
      "accumulatedOtherComprehensiveIncomeLoss_yoy      5   515\n",
      "intangibleAssets_yoy                             5   586\n",
      "shortTermDebt_yoy                                6   539\n",
      "minorityInterest_yoy                            13   719\n",
      "goodwillAndIntangibleAssets_yoy                 24   539\n",
      "preferredStock_yoy                              27   949\n",
      "goodwill_yoy                                    73   553\n",
      "commonStock_yoy                                244   534\n",
      "\n",
      "Cashflow 0/NaN counts per YoY column:\n",
      "                                              zeros  nans\n",
      "netCashProvidedByOperatingActivities_yoy          0   503\n",
      "netCashUsedForInvestingActivites_yoy              0   503\n",
      "netCashUsedProvidedByFinancingActivities_yoy      0   503\n",
      "cashAtBeginningOfPeriod_yoy                       0   503\n",
      "operatingCashFlow_yoy                             0   503\n",
      "freeCashFlow_yoy                                  0   503\n",
      "otherWorkingCapital_yoy                           0   506\n",
      "investmentsInPropertyPlantAndEquipment_yoy        0   529\n",
      "deferredIncomeTax_yoy                             0   567\n",
      "effectOfForexChangesOnCash_yoy                    0   636\n",
      "inventory_yoy                                     0   660\n",
      "purchasesOfInvestments_yoy                        0   672\n",
      "commonStockIssued_yoy                             0   791\n",
      "netIncome_yoy                                     1   503\n",
      "netChangeInCash_yoy                               1   503\n",
      "cashAtEndOfPeriod_yoy                             1   503\n",
      "depreciationAndAmortization_yoy                   1   507\n",
      "capitalExpenditure_yoy                            1   527\n",
      "accountsReceivables_yoy                           1   561\n",
      "accountsPayables_yoy                              1   574\n",
      "acquisitionsNet_yoy                               1   618\n",
      "salesMaturitiesOfInvestments_yoy                  1   665\n",
      "changeInWorkingCapital_yoy                        2   503\n",
      "otherNonCashItems_yoy                             2   504\n",
      "debtRepayment_yoy                                 2   536\n",
      "dividendsPaid_yoy                                 2   597\n",
      "otherFinancingActivites_yoy                       3   511\n",
      "otherInvestingActivites_yoy                       5   532\n",
      "stockBasedCompensation_yoy                        5   580\n",
      "commonStockRepurchased_yoy                        8   568\n"
     ]
    }
   ],
   "source": [
    "def count_zeros_nans_yoy(df):\n",
    "    # Keep only numeric columns that end with \"_yoy\"\n",
    "    numeric_cols = [c for c in df.select_dtypes(include=[float, int]).columns if c.endswith(\"_yoy\")]\n",
    "    \n",
    "    # Count zeros and NaNs\n",
    "    zero_counts = (df[numeric_cols] == 0).sum()\n",
    "    nan_counts = df[numeric_cols].isna().sum()\n",
    "    \n",
    "    # Combine into a single DataFrame\n",
    "    summary = pd.DataFrame({\n",
    "        \"zeros\": zero_counts,\n",
    "        \"nans\": nan_counts\n",
    "    }).sort_values(by=[\"zeros\", \"nans\"], ascending=True)\n",
    "    \n",
    "    # Force full display\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(summary)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "print(\"Income 0/NaN counts per YoY column:\")\n",
    "income_summary = count_zeros_nans_yoy(income_growth)\n",
    "\n",
    "print(\"\\nBalance 0/NaN counts per YoY column:\")\n",
    "balance_summary = count_zeros_nans_yoy(balance_growth)\n",
    "\n",
    "print(\"\\nCashflow 0/NaN counts per YoY column:\")\n",
    "cashflow_summary = count_zeros_nans_yoy(cashflow_growth)\n",
    "\n",
    "# the purpose here is to make it easy to identify which line items (columns) are fully filled out from our sample so that we are only grabbing columns (features)\n",
    "# that are likely to be filled out by the stock under consideration, cause ultimately after we find a regression that has explanatory power... we can still only apply it \n",
    "# to the stock under consideration if it has the same line items filled out \n",
    "# the 503 nans is a result of the .pct change method that we used which creates a nan on every other row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "676f2825-768b-4ae3-bb05-0fba40fcac2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income cleaned shape: (502, 17)\n",
      "Balance cleaned shape: (502, 12)\n",
      "Cashflow cleaned shape: (501, 12)\n"
     ]
    }
   ],
   "source": [
    "def remove_single_period_features(df, nan_threshold=503, keep_cols=[\"symbol\",\"date\"], name=\"\"):\n",
    "    \"\"\"Select _yoy numeric columns, replace inf, drop rows/columns with too many NaNs.\"\"\"\n",
    "    yoy_cols = [c for c in df.select_dtypes(include=[float, int]).columns if c.endswith(\"_yoy\")]\n",
    "    sub_df = df[yoy_cols + keep_cols].copy()  # Keep symbol/date columns\n",
    "    \n",
    "    # Drop columns with too many NaNs (only apply to yoy columns)\n",
    "    valid_yoy_cols = [c for c in yoy_cols if sub_df[c].isna().sum() <= nan_threshold]\n",
    "    sub_df = sub_df[valid_yoy_cols + keep_cols]\n",
    "    \n",
    "    # Replace infinities with NaN and drop rows with any remaining NaNs (only apply to yoy columns)\n",
    "    sub_df[valid_yoy_cols] = sub_df[valid_yoy_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    sub_df.dropna(subset=valid_yoy_cols, inplace=True)\n",
    "    \n",
    "    print(f\"{name} cleaned shape: {sub_df.shape}\")\n",
    "    return sub_df\n",
    "\n",
    "# Step 0: Clean _yoy columns and print shapes\n",
    "income_clean = remove_single_period_features(income_growth, name=\"Income\")\n",
    "balance_clean = remove_single_period_features(balance_growth, name=\"Balance\")\n",
    "cashflow_clean = remove_single_period_features(cashflow_growth, name=\"Cashflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f7de5d-cb6c-4ffb-90d8-21e3ee14d871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Keep only common (symbol, date) pairs\n",
    "for df in [income_clean, balance_clean, cashflow_clean]:\n",
    "    df[\"symbol_date\"] = list(zip(df[\"symbol\"], df[\"date\"]))\n",
    "\n",
    "common_pairs = (\n",
    "    set(income_clean[\"symbol_date\"])\n",
    "    & set(balance_clean[\"symbol_date\"])\n",
    "    & set(cashflow_clean[\"symbol_date\"])\n",
    ")\n",
    "\n",
    "income_clean = income_clean[income_clean[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "balance_clean = balance_clean[balance_clean[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "cashflow_clean = cashflow_clean[cashflow_clean[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "\n",
    "# we only want to include a ticker if we have all the financial statement items for all three financial statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e45ff5cc-0343-463b-a89d-32c3f8b744a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and aligned shapes: (491, 18) (491, 13) (491, 13)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Sort consistently\n",
    "income_clean = income_clean.sort_values([\"symbol\",\"date\"]).reset_index(drop=True)\n",
    "balance_clean = balance_clean.sort_values([\"symbol\",\"date\"]).reset_index(drop=True)\n",
    "cashflow_clean = cashflow_clean.sort_values([\"symbol\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Cleaned and aligned shapes:\", income_clean.shape, balance_clean.shape, cashflow_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47abcfd7-fcfc-44ee-bf39-194b62583559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Clean & align by (symbol, date)\n",
    "def clean_and_align(dfs, nan_threshold=503, keep_cols=[\"symbol\",\"date\"]):\n",
    "    \"\"\"\n",
    "    1) Drop columns with too many NaNs\n",
    "    2) Replace inf with NaN\n",
    "    3) Drop rows with any remaining NaNs\n",
    "    4) Align multiple DataFrames by shared (symbol, date) rows\n",
    "    \"\"\"\n",
    "    cleaned = {}\n",
    "    for name, df in dfs.items():\n",
    "        # Select _yoy numeric columns\n",
    "        yoy_cols = [c for c in df.select_dtypes(include=[float,int]).columns if c.endswith(\"_yoy\")]\n",
    "        # Keep only valid columns under nan_threshold\n",
    "        valid_cols = [c for c in yoy_cols if df[c].isna().sum() <= nan_threshold]\n",
    "        sub_df = df[valid_cols + keep_cols].copy()\n",
    "        \n",
    "        # Replace infs with NaN and drop rows with any remaining NaNs\n",
    "        sub_df[valid_cols] = sub_df[valid_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        before = sub_df.shape[0]\n",
    "        sub_df.dropna(subset=valid_cols, inplace=True)\n",
    "        after = sub_df.shape[0]\n",
    "        print(f\"{name}: {before - after} rows dropped due to NaNs\")\n",
    "        \n",
    "        # Create composite key\n",
    "        sub_df[\"symbol_date\"] = list(zip(sub_df[\"symbol\"], sub_df[\"date\"]))\n",
    "        cleaned[name] = sub_df\n",
    "        print(f\"{name} shape after cleaning: {sub_df.shape}\")\n",
    "    \n",
    "    # Find shared (symbol, date) across all dfs\n",
    "    common_pairs = set.intersection(*(set(df[\"symbol_date\"]) for df in cleaned.values()))\n",
    "    print(f\"Shared (symbol, date) rows across all dfs: {len(common_pairs)}\")\n",
    "    \n",
    "    # Filter each df to shared rows\n",
    "    aligned = {name: df[df[\"symbol_date\"].isin(common_pairs)].copy().reset_index(drop=True) \n",
    "               for name, df in cleaned.items()}\n",
    "    \n",
    "    # Drop helper column\n",
    "    for df in aligned.values():\n",
    "        df.drop(columns=[\"symbol_date\"], inplace=True)\n",
    "    \n",
    "    return aligned, common_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae15f3d-85b7-4b9c-8dc4-9e2495ffa557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income: 0 rows dropped due to NaNs\n",
      "income shape after cleaning: (491, 18)\n",
      "balance: 0 rows dropped due to NaNs\n",
      "balance shape after cleaning: (491, 13)\n",
      "cashflow: 0 rows dropped due to NaNs\n",
      "cashflow shape after cleaning: (491, 13)\n",
      "Shared (symbol, date) rows across all dfs: 491\n"
     ]
    }
   ],
   "source": [
    "aligned, common_pairs = clean_and_align({\n",
    "    \"income\": income_clean,\n",
    "    \"balance\": balance_clean,\n",
    "    \"cashflow\": cashflow_clean\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd99266-2982-49c8-85e5-4683cdd83aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['revenue_yoy', 'grossProfit_yoy', 'grossProfitRatio_yoy',\n",
      "       'costAndExpenses_yoy', 'ebitdaratio_yoy', 'operatingIncome_yoy',\n",
      "       'operatingIncomeRatio_yoy', 'incomeBeforeTax_yoy',\n",
      "       'incomeBeforeTaxRatio_yoy', 'netIncome_yoy', 'netIncomeRatio_yoy',\n",
      "       'eps_yoy', 'epsdiluted_yoy', 'weightedAverageShsOut_yoy',\n",
      "       'weightedAverageShsOutDil_yoy', 'symbol', 'date', 'symbol_date'],\n",
      "      dtype='object')\n",
      "Index(['cashAndCashEquivalents_yoy', 'cashAndShortTermInvestments_yoy',\n",
      "       'otherNonCurrentAssets_yoy', 'totalAssets_yoy', 'totalLiabilities_yoy',\n",
      "       'totalStockholdersEquity_yoy', 'totalEquity_yoy',\n",
      "       'totalLiabilitiesAndStockholdersEquity_yoy',\n",
      "       'totalLiabilitiesAndTotalEquity_yoy', 'netDebt_yoy', 'symbol', 'date',\n",
      "       'symbol_date'],\n",
      "      dtype='object')\n",
      "Index(['netIncome_yoy', 'changeInWorkingCapital_yoy',\n",
      "       'netCashProvidedByOperatingActivities_yoy',\n",
      "       'netCashUsedForInvestingActivites_yoy',\n",
      "       'netCashUsedProvidedByFinancingActivities_yoy', 'netChangeInCash_yoy',\n",
      "       'cashAtEndOfPeriod_yoy', 'cashAtBeginningOfPeriod_yoy',\n",
      "       'operatingCashFlow_yoy', 'freeCashFlow_yoy', 'symbol', 'date',\n",
      "       'symbol_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(income_clean.columns)\n",
    "print(balance_clean.columns)\n",
    "print(cashflow_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fed1b04-d0c6-4593-a018-e0f53440299b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INCOME FIRST 5 SYMBOLS:\n",
      "['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACGL']\n",
      "\n",
      "INCOME LAST 5 SYMBOLS:\n",
      "['XYZ', 'YUM', 'ZBH', 'ZBRA', 'ZTS']\n",
      "\n",
      "BALANCE FIRST 5 SYMBOLS:\n",
      "['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACGL']\n",
      "\n",
      "BALANCE LAST 5 SYMBOLS:\n",
      "['XYZ', 'YUM', 'ZBH', 'ZBRA', 'ZTS']\n",
      "\n",
      "CASHFLOW FIRST 5 SYMBOLS:\n",
      "['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACGL']\n",
      "\n",
      "CASHFLOW LAST 5 SYMBOLS:\n",
      "['XYZ', 'YUM', 'ZBH', 'ZBRA', 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "for name, df in {\"income\": income_clean, \"balance\": balance_clean, \"cashflow\": cashflow_clean}.items():\n",
    "    print(f\"\\n{name.upper()} FIRST 5 SYMBOLS:\")\n",
    "    print(df[\"symbol\"].head(5).to_list())\n",
    "    print(f\"\\n{name.upper()} LAST 5 SYMBOLS:\")\n",
    "    print(df[\"symbol\"].tail(5).to_list())\n",
    "    \n",
    "# just a check that the symbols are the same and aligned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd4fef13-9c56-4801-86b3-1d7904fc3e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pca(df, selected_cols, n_components=5, scale=True):\n",
    "    \"\"\"\n",
    "    Run PCA on selected columns with optional scaling and visualize:\n",
    "      - Column variances\n",
    "      - Explained variance ratio of PCs\n",
    "    \"\"\"\n",
    "    sub_df = df[selected_cols].copy()\n",
    "    \n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        scaled = scaler.fit_transform(sub_df)\n",
    "        data_to_use = scaled\n",
    "        title_suffix = \" (post-standardization)\"\n",
    "    else:\n",
    "        data_to_use = sub_df.values\n",
    "        title_suffix = \" (raw)\"\n",
    "    \n",
    "    # Compute PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(data_to_use)\n",
    "    pcs_df = pd.DataFrame(pcs, index=sub_df.index, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "    \n",
    "    # Print some info\n",
    "    print(f\"PCA shape: {pcs_df.shape}\")\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "    return pcs_df, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512f12a1-643f-458b-ae19-f8fb82a8b909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA shape: (491, 3)\n",
      "Explained variance ratio: [0.34710423 0.34377133 0.30912445]\n",
      "PCA shape: (491, 3)\n",
      "Explained variance ratio: [0.646486  0.3168122 0.0367018]\n",
      "PCA shape: (491, 3)\n",
      "Explained variance ratio: [6.80308141e-01 3.19691688e-01 1.71405147e-07]\n"
     ]
    }
   ],
   "source": [
    "income_cols = [\"revenue_yoy\",\"netIncome_yoy\",\"operatingIncomeRatio_yoy\"]\n",
    "balance_cols = [\"totalAssets_yoy\",\"totalLiabilities_yoy\",\"totalStockholdersEquity_yoy\"]\n",
    "cashflow_cols = [\"operatingCashFlow_yoy\",\"freeCashFlow_yoy\",\"netCashProvidedByOperatingActivities_yoy\"]\n",
    "\n",
    "income_pcs, income_pca_model = run_pca(aligned[\"income\"], income_cols, n_components=3)\n",
    "balance_pcs, balance_pca_model = run_pca(aligned[\"balance\"], balance_cols, n_components=3)\n",
    "cashflow_pcs, cashflow_pca_model = run_pca(aligned[\"cashflow\"], cashflow_cols, n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61229223-2e01-45a1-be31-ff084145b6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491, 4)\n",
      "(491, 4)\n",
      "(491, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(480, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_pcs[\"symbol\"] = aligned[\"income\"][\"symbol\"].values\n",
    "balance_pcs[\"symbol\"] = aligned[\"balance\"][\"symbol\"].values\n",
    "cashflow_pcs[\"symbol\"] = aligned[\"cashflow\"][\"symbol\"].values\n",
    "\n",
    "print(cashflow_pcs.shape)\n",
    "print(balance_pcs.shape)\n",
    "print(income_pcs.shape)\n",
    "\n",
    "cashflow_pcs.columns\n",
    "pe_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d13adc9-7452-4516-93d4-c67b94e457e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (468, 5)\n"
     ]
    }
   ],
   "source": [
    "all_pcs = pd.concat(\n",
    "    [income_pcs, balance_pcs, cashflow_pcs], axis=1\n",
    ")\n",
    "\n",
    "all_pcs = all_pcs.loc[:, ~all_pcs.columns.duplicated()]\n",
    "\n",
    "\n",
    "merged = pd.merge(all_pcs, pe_data[[\"symbol\", \"log_PE\"]], on=\"symbol\")\n",
    "print(\"Merged shape:\", merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bc708f8-93bf-4075-a690-ff9a1380cfe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 log_PE   R-squared:                       0.044\n",
      "Model:                            OLS   Adj. R-squared:                  0.038\n",
      "Method:                 Least Squares   F-statistic:                     7.141\n",
      "Date:                Tue, 16 Sep 2025   Prob (F-statistic):           0.000107\n",
      "Time:                        09:09:51   Log-Likelihood:                -532.40\n",
      "No. Observations:                 468   AIC:                             1073.\n",
      "Df Residuals:                     464   BIC:                             1089.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.2993      0.035     94.068      0.000       3.230       3.368\n",
      "PC1            0.0792      0.035      2.256      0.025       0.010       0.148\n",
      "PC2            0.0117      0.040      0.293      0.769      -0.067       0.090\n",
      "PC3           -0.1522      0.039     -3.858      0.000      -0.230      -0.075\n",
      "==============================================================================\n",
      "Omnibus:                      153.127   Durbin-Watson:                   1.821\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              657.389\n",
      "Skew:                           1.399   Prob(JB):                    1.78e-143\n",
      "Kurtosis:                       8.088   Cond. No.                         1.30\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "X = merged.drop(columns=[\"symbol\", \"log_PE\"])\n",
    "y = merged[\"log_PE\"]\n",
    "\n",
    "X = sm.add_constant(X)  # add intercept\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b89cf58b-c450-4654-8fd2-a4fe31b3c4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Regression for PE quartile 1 ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 log_PE   R-squared:                       0.026\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                     1.018\n",
      "Date:                Tue, 16 Sep 2025   Prob (F-statistic):              0.388\n",
      "Time:                        09:09:51   Log-Likelihood:                -10.246\n",
      "No. Observations:                 117   AIC:                             28.49\n",
      "Df Residuals:                     113   BIC:                             39.54\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.5122      0.025    100.272      0.000       2.463       2.562\n",
      "PC1           -0.0173      0.017     -1.048      0.297      -0.050       0.015\n",
      "PC2           -0.0139      0.015     -0.953      0.343      -0.043       0.015\n",
      "PC3            0.0225      0.017      1.358      0.177      -0.010       0.055\n",
      "==============================================================================\n",
      "Omnibus:                       41.835   Durbin-Watson:                   2.087\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               90.317\n",
      "Skew:                          -1.458   Prob(JB):                     2.44e-20\n",
      "Kurtosis:                       6.166   Cond. No.                         2.06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "--- Regression for PE quartile 2 ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 log_PE   R-squared:                       0.010\n",
      "Model:                            OLS   Adj. R-squared:                 -0.016\n",
      "Method:                 Least Squares   F-statistic:                    0.3878\n",
      "Date:                Tue, 16 Sep 2025   Prob (F-statistic):              0.762\n",
      "Time:                        09:09:51   Log-Likelihood:                 95.618\n",
      "No. Observations:                 117   AIC:                            -183.2\n",
      "Df Residuals:                     113   BIC:                            -172.2\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.0427      0.012    250.528      0.000       3.019       3.067\n",
      "PC1           -0.0037      0.041     -0.090      0.928      -0.085       0.078\n",
      "PC2           -0.1275      0.155     -0.825      0.411      -0.434       0.179\n",
      "PC3           -0.0882      0.099     -0.896      0.372      -0.283       0.107\n",
      "==============================================================================\n",
      "Omnibus:                       48.989   Durbin-Watson:                   2.246\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):                7.782\n",
      "Skew:                          -0.150   Prob(JB):                       0.0204\n",
      "Kurtosis:                       1.773   Cond. No.                         18.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "--- Regression for PE quartile 3 ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 log_PE   R-squared:                       0.003\n",
      "Model:                            OLS   Adj. R-squared:                 -0.023\n",
      "Method:                 Least Squares   F-statistic:                    0.1242\n",
      "Date:                Tue, 16 Sep 2025   Prob (F-statistic):              0.946\n",
      "Time:                        09:09:51   Log-Likelihood:                 94.351\n",
      "No. Observations:                 117   AIC:                            -180.7\n",
      "Df Residuals:                     113   BIC:                            -169.7\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.3878      0.013    262.176      0.000       3.362       3.413\n",
      "PC1            0.0543      0.093      0.581      0.562      -0.131       0.239\n",
      "PC2            0.0226      0.099      0.228      0.820      -0.174       0.219\n",
      "PC3            0.0924      0.171      0.541      0.590      -0.246       0.431\n",
      "==============================================================================\n",
      "Omnibus:                       19.798   Durbin-Watson:                   1.853\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):                6.443\n",
      "Skew:                           0.271   Prob(JB):                       0.0399\n",
      "Kurtosis:                       1.986   Cond. No.                         20.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "--- Regression for PE quartile 4 ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 log_PE   R-squared:                       0.212\n",
      "Model:                            OLS   Adj. R-squared:                  0.191\n",
      "Method:                 Least Squares   F-statistic:                     10.14\n",
      "Date:                Tue, 16 Sep 2025   Prob (F-statistic):           5.76e-06\n",
      "Time:                        09:09:51   Log-Likelihood:                -126.73\n",
      "No. Observations:                 117   AIC:                             261.5\n",
      "Df Residuals:                     113   BIC:                             272.5\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.9423      0.092     42.825      0.000       3.760       4.125\n",
      "PC1           -2.5044      0.656     -3.820      0.000      -3.803      -1.206\n",
      "PC2           -0.9947      1.376     -0.723      0.471      -3.721       1.732\n",
      "PC3           -3.9890      0.825     -4.832      0.000      -5.624      -2.354\n",
      "==============================================================================\n",
      "Omnibus:                       42.760   Durbin-Watson:                   1.899\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               87.100\n",
      "Skew:                           1.537   Prob(JB):                     1.22e-19\n",
      "Kurtosis:                       5.901   Cond. No.                         25.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Assume merged has all the features and log_PE\n",
    "X = merged.drop(columns=[\"symbol\", \"log_PE\"])\n",
    "y = merged[\"log_PE\"]\n",
    "\n",
    "# Add a constant for intercept\n",
    "X_const = sm.add_constant(X)\n",
    "\n",
    "# Create quartiles based on log_PE\n",
    "merged[\"PE_quartile\"] = pd.qcut(merged[\"log_PE\"], 4, labels=False)  # 0=lowest, 3=highest\n",
    "\n",
    "# Loop through quartiles\n",
    "for q in range(4):\n",
    "    mask = merged[\"PE_quartile\"] == q\n",
    "    X_q = X_const[mask]\n",
    "    y_q = y[mask]\n",
    "    \n",
    "    model_q = sm.OLS(y_q, X_q).fit()\n",
    "    \n",
    "    print(f\"\\n--- Regression for PE quartile {q+1} ---\")\n",
    "    print(model_q.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a5538ec-a122-47a6-854a-868ff37d0512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       variable      coef   std_err  R_squared  quartile\n",
      "0         const  2.512168  0.025053   0.026304         1\n",
      "1           PC1 -0.017309  0.016511   0.026304         1\n",
      "2           PC2 -0.013928  0.014614   0.026304         1\n",
      "3           PC3  0.022537  0.016594   0.026304         1\n",
      "4   PE_quartile  0.000000  0.000000   0.026304         1\n",
      "5         const  1.521359  0.006073   0.010191         2\n",
      "6           PC1 -0.003696  0.041033   0.010191         2\n",
      "7           PC2 -0.127523  0.154594   0.010191         2\n",
      "8           PC3 -0.088242  0.098536   0.010191         2\n",
      "9   PE_quartile  1.521359  0.006073   0.010191         2\n",
      "10        const  0.677555  0.002584   0.003285         3\n",
      "11          PC1  0.054271  0.093405   0.003285         3\n",
      "12          PC2  0.022564  0.099015   0.003285         3\n",
      "13          PC3  0.092381  0.170751   0.003285         3\n",
      "14  PE_quartile  1.355111  0.005169   0.003285         3\n",
      "15        const  0.394228  0.009205   0.212034         4\n",
      "16          PC1 -2.504433  0.655548   0.212034         4\n",
      "17          PC2 -0.994669  1.376141   0.212034         4\n",
      "18          PC3 -3.989013  0.825462   0.212034         4\n",
      "19  PE_quartile  1.182684  0.027616   0.212034         4\n"
     ]
    }
   ],
   "source": [
    "# Assume merged has all the features and log_PE\n",
    "X = merged.drop(columns=[\"symbol\", \"log_PE\"])\n",
    "y = merged[\"log_PE\"]\n",
    "\n",
    "# Add a constant for intercept\n",
    "X_const = sm.add_constant(X)\n",
    "\n",
    "# Create quartiles based on log_PE\n",
    "merged[\"PE_quartile\"] = pd.qcut(merged[\"log_PE\"], 4, labels=False)  # 0=lowest, 3=highest\n",
    "\n",
    "# Prepare a DataFrame to store results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Loop through quartiles\n",
    "for q in range(4):\n",
    "    mask = merged[\"PE_quartile\"] == q\n",
    "    X_q = X_const[mask]\n",
    "    y_q = y[mask]\n",
    "    \n",
    "    model_q = sm.OLS(y_q, X_q).fit()\n",
    "    \n",
    "    # Store coefficients and standard errors\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"variable\": model_q.params.index,\n",
    "        \"coef\": model_q.params.values,\n",
    "        \"std_err\": model_q.bse.values,\n",
    "        \"R_squared\": model_q.rsquared,\n",
    "        \"quartile\": q+1  # human-friendly 1-4\n",
    "    })\n",
    "    \n",
    "    results_df = pd.concat([results_df, coef_df], ignore_index=True)\n",
    "\n",
    "# Show results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6841963a-74ea-4215-8852-bbd2b5cd669f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income 1, Balance 1, Cashflow 1, R: 0.011\n",
      "Income 1, Balance 1, Cashflow 2, R: 0.013\n",
      "Income 1, Balance 1, Cashflow 3, R: 0.044\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m y \u001b[38;5;241m=\u001b[39m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_PE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Fit OLS regression\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk_income\u001b[39m\u001b[38;5;124m\"\u001b[39m: k_income,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk_balance\u001b[39m\u001b[38;5;124m\"\u001b[39m: k_balance,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk_cashflow\u001b[39m\u001b[38;5;124m\"\u001b[39m: k_cashflow,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR_squared\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mrsquared\n\u001b[1;32m     33\u001b[0m })\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:922\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    921\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:748\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    751\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:202\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRegressionModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/statsmodels/base/model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/statsmodels/base/model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/statsmodels/base/model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/statsmodels/base/data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/statsmodels/base/data.py:84\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_endog \u001b[38;5;241m=\u001b[39m endog\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_exog \u001b[38;5;241m=\u001b[39m exog\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_endog_exog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/statsmodels/base/data.py:509\u001b[0m, in \u001b[0;36mPandasData._convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    507\u001b[0m exog \u001b[38;5;241m=\u001b[39m exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas data cast to numpy dtype of object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck input data with np.asarray(data).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(PandasData, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_convert_endog_exog(endog, exog)\n",
      "\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "# --- Define PCA column groups ---\n",
    "income_pcs_cols = [c for c in merged.columns if c in income_pcs.columns]\n",
    "balance_pcs_cols = [c for c in merged.columns if c in balance_pcs.columns]\n",
    "cashflow_pcs_cols = [c for c in merged.columns if c in cashflow_pcs.columns]\n",
    "\n",
    "# --- Hyperparameter loop ---\n",
    "results = []\n",
    "\n",
    "for k_income in range(1, len(income_pcs_cols)+1):\n",
    "    for k_balance in range(1, len(balance_pcs_cols)+1):\n",
    "        for k_cashflow in range(1, len(cashflow_pcs_cols)+1):\n",
    "            \n",
    "            # Subset first k PCs for each category\n",
    "            X_subset = pd.concat([\n",
    "                merged[income_pcs_cols[:k_income]],\n",
    "                merged[balance_pcs_cols[:k_balance]],\n",
    "                merged[cashflow_pcs_cols[:k_cashflow]]\n",
    "            ], axis=1)\n",
    "            \n",
    "            # Add constant for intercept\n",
    "            X_subset = sm.add_constant(X_subset)\n",
    "            y = merged[\"log_PE\"]\n",
    "            \n",
    "            # Fit OLS regression\n",
    "            model = sm.OLS(y, X_subset).fit()\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"k_income\": k_income,\n",
    "                \"k_balance\": k_balance,\n",
    "                \"k_cashflow\": k_cashflow,\n",
    "                \"R_squared\": model.rsquared\n",
    "            })\n",
    "            \n",
    "            print(f\"Income {k_income}, Balance {k_balance}, Cashflow {k_cashflow}, R: {model.rsquared:.3f}\")\n",
    "\n",
    "# Convert results to DataFrame for easy analysis\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4b15a-dc38-4987-98af-029cf9d16292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
