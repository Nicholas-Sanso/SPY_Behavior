{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9ed79e-112a-418f-b955-5c13dd46dd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import gaussian_kde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2982057a-32e3-43e6-b761-6240815efa25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tickers from CSV cache...\n",
      "(503, 8)\n",
      "Index(['symbol', 'name', 'sector', 'subSector', 'headQuarter',\n",
      "       'dateFirstAdded', 'cik', 'founded'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "\n",
    "# THIS pulls sector and subsector info either localy if it's cached or from the api\n",
    "\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "\n",
    "# --- LOAD FROM CACHE OR FETCH ---\n",
    "if os.path.exists(tickers_csv_file):\n",
    "    print(\"Loading tickers from CSV cache...\")\n",
    "    df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "else:\n",
    "    print(\"Fetching tickers from API...\")\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/sp500_constituent?apikey={API_KEY}\"\n",
    "    df_sp500 = pd.DataFrame(requests.get(url).json())\n",
    "\n",
    "    # Save to CSV\n",
    "    df_sp500.to_csv(tickers_csv_file, index=False)\n",
    "    print(f\"Saved {len(df_sp500)} tickers to CSV cache.\")\n",
    "\n",
    "    \n",
    "# --- PREVIEW ---\n",
    "print(df_sp500.shape)\n",
    "print(df_sp500.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d2a811-89dd-4a1d-b195-4f3af35f41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d8a247-31e3-486c-a090-35f4860c348f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/price_and_earnings.json\n",
      "Index(['symbol', 'price', 'price_date', 'eps', 'netIncome', 'date'], dtype='object')\n",
      "(503, 6)\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "# Load tickers\n",
    "df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()\n",
    "\n",
    "# Output file\n",
    "output_file = os.path.join(data_folder, \"price_and_earnings.json\")\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Get current price\n",
    "            quote_url = f\"https://financialmodelingprep.com/api/v3/quote/{ticker}?apikey={API_KEY}\"\n",
    "            price_data = requests.get(quote_url).json()\n",
    "            if not price_data:\n",
    "                continue\n",
    "            price = price_data[0][\"price\"]\n",
    "            price_date = price_data[0].get(\"date\")  # trading date\n",
    "\n",
    "            # Get latest annual income statement\n",
    "            income_url = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?limit=1&apikey={API_KEY}\"\n",
    "            income_data = requests.get(income_url).json()\n",
    "            if not income_data:\n",
    "                continue\n",
    "            eps = income_data[0].get(\"eps\")\n",
    "            net_income = income_data[0].get(\"netIncome\")\n",
    "            report_date = income_data[0].get(\"date\")  # fiscal period end date\n",
    "\n",
    "            records.append({\n",
    "                \"symbol\": ticker,\n",
    "                \"price\": price,\n",
    "                \"price_date\": price_date,\n",
    "                \"eps\": eps,\n",
    "                \"netIncome\": net_income,\n",
    "                \"date\": report_date\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "        time.sleep(0.2)  # polite rate limit\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "price_earnings_df = fetch_price_and_earnings(tickers, output_file)\n",
    "print(price_earnings_df.columns)\n",
    "print(price_earnings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb737c07-dc25-4514-858c-f5178907b5be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 7)\n"
     ]
    }
   ],
   "source": [
    "#CLEANS AND TAKES LOG OF THE PE DATA\n",
    "\n",
    "# Only keep rows with positive EPS\n",
    "pe_data = price_earnings_df[price_earnings_df[\"eps\"] > 0].copy()\n",
    "\n",
    "# Compute log(PE)\n",
    "pe_data[\"log_PE\"] = np.log(pe_data[\"price\"] / pe_data[\"eps\"])\n",
    "\n",
    "# Print row count for reference\n",
    "print(pe_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1844560d-77dc-4f2a-8993-96bb263662be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/income-statement_annual_limit2.json\n",
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/balance-sheet-statement_annual_limit2.json\n",
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/cash-flow-statement_annual_limit2.json\n",
      "Income shape: (1006, 38)\n",
      "Balance shape: (1006, 54)\n",
      "Cash flow shape: (1006, 40)\n"
     ]
    }
   ],
   "source": [
    "# FETCHES INCOME STATEMENT AND BS STATEMENT AND CF STATEMENT\n",
    "def fetch_statement(endpoint, tickers, period, limit, data_folder):\n",
    "    \"\"\"Fetch statements with unique JSON filename based on endpoint, period, limit.\"\"\"\n",
    "    output_file = os.path.join(\n",
    "        data_folder,\n",
    "        f\"{endpoint}_{period}_limit{limit}.json\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        with open(output_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        url = f\"https://financialmodelingprep.com/api/v3/{endpoint}/{ticker}?period={period}&limit={limit}&apikey={API_KEY}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                for row in data:\n",
    "                    row[\"symbol\"] = ticker\n",
    "                records.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker} ({endpoint}): {e}\")\n",
    "        time.sleep(.2)  # API polite rate limit\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "    # \"RECORDS HERE IS A LIST OF STRINGS NOT A DF\"\n",
    "    return records\n",
    "\n",
    "# AT THIS POINT INCOME_DATA_2_YEARS IS STILL A LIST OF STRINGS STORED AS A VAR\n",
    "income_data_2_years   = fetch_statement(\"income-statement\", tickers, \"annual\", 2, data_folder)\n",
    "balance_data_2_years  = fetch_statement(\"balance-sheet-statement\", tickers, \"annual\", 2, data_folder)\n",
    "cashflow_data_2_years = fetch_statement(\"cash-flow-statement\", tickers, \"annual\", 2, data_folder)\n",
    "\n",
    "# INCOME_DATA_2_YEARS IS CONVERTED TO A DF\n",
    "income_data_2_years   = pd.DataFrame(income_data_2_years)\n",
    "balance_data_2_years  = pd.DataFrame(balance_data_2_years)\n",
    "cashflow_data_2_years = pd.DataFrame(cashflow_data_2_years)\n",
    "\n",
    "print(\"Income shape:\", income_data_2_years.shape)\n",
    "print(\"Balance shape:\", balance_data_2_years.shape)\n",
    "print(\"Cash flow shape:\", cashflow_data_2_years.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031400b-6a30-4ff6-ab53-72963cfe5a96",
   "metadata": {},
   "source": [
    "# End of fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e321a26-3485-4841-aea9-0e069cd8dc23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_data_2_years.drop(columns=[\"netIncome\"], inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86b245b2-1b76-49e2-a37d-e3d08f3acc87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income_data_2_years: cleaned column names\n",
      "balance_data_2_years: cleaned column names\n",
      "cashflow_data_2_years: cleaned column names\n",
      "pe_data: cleaned column names\n"
     ]
    }
   ],
   "source": [
    "# --- Clean up column names (remove spaces, tabs, and backslashes) ---\n",
    "for df_name, df in {\n",
    "    \"income_data_2_years\": income_data_2_years,\n",
    "    \"balance_data_2_years\": balance_data_2_years,\n",
    "    \"cashflow_data_2_years\": cashflow_data_2_years,\n",
    "    \"pe_data\": pe_data\n",
    "}.items():\n",
    "    df.columns = df.columns.str.strip().str.replace(r'\\\\', '', regex=True)\n",
    "    print(f\"{df_name}: cleaned column names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b213f63-6b7f-4f36-822d-5d301e65400d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_problematic_entries(df, label):\n",
    "    print(f\"\\n🔍 Problematic value counts for {label} statement:\")\n",
    "    \n",
    "    def count_issues(col):\n",
    "        numeric_col = pd.to_numeric(col, errors='coerce')\n",
    "        return ((numeric_col.isna()) | (numeric_col == 0)).sum()\n",
    "\n",
    "    issue_counts = df.apply(count_issues)\n",
    "    filtered_counts = issue_counts[issue_counts > 0].sort_values()\n",
    "    print(filtered_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "391db554-2c54-4798-8835-6b83f817c3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Problematic value counts for Income statement:\n",
      "operatingExpenses                             8\n",
      "depreciationAndAmortization                   9\n",
      "incomeTaxExpense                             12\n",
      "costOfRevenue                                20\n",
      "interestExpense                              63\n",
      "totalOtherIncomeExpensesNet                  65\n",
      "sellingGeneralAndAdministrativeExpenses      79\n",
      "otherExpenses                               330\n",
      "interestIncome                              352\n",
      "generalAndAdministrativeExpenses            548\n",
      "researchAndDevelopmentExpenses              605\n",
      "sellingAndMarketingExpenses                 727\n",
      "date                                       1006\n",
      "period                                     1006\n",
      "acceptedDate                               1006\n",
      "fillingDate                                1006\n",
      "reportedCurrency                           1006\n",
      "symbol                                     1006\n",
      "link                                       1006\n",
      "finalLink                                  1006\n",
      "dtype: int64\n",
      "\n",
      "🔍 Problematic value counts for Balance Sheet statement:\n",
      "totalNonCurrentAssets                         2\n",
      "totalNonCurrentLiabilities                    4\n",
      "otherNonCurrentAssets                         5\n",
      "totalDebt                                     5\n",
      "totalCurrentAssets                            5\n",
      "longTermDebt                                  8\n",
      "totalCurrentLiabilities                      12\n",
      "otherNonCurrentLiabilities                   15\n",
      "retainedEarnings                             15\n",
      "netReceivables                               20\n",
      "propertyPlantEquipmentNet                    25\n",
      "otherCurrentLiabilities                      28\n",
      "accumulatedOtherComprehensiveIncomeLoss      32\n",
      "accountPayables                              62\n",
      "commonStock                                  70\n",
      "otherCurrentAssets                           74\n",
      "goodwillAndIntangibleAssets                  79\n",
      "shortTermDebt                                96\n",
      "othertotalStockholdersEquity                 97\n",
      "goodwill                                    106\n",
      "intangibleAssets                            184\n",
      "capitalLeaseObligations                     217\n",
      "totalInvestments                            232\n",
      "inventory                                   327\n",
      "longTermInvestments                         340\n",
      "deferredTaxLiabilitiesNonCurrent            349\n",
      "taxPayables                                 423\n",
      "minorityInterest                            440\n",
      "deferredRevenue                             479\n",
      "taxAssets                                   526\n",
      "shortTermInvestments                        581\n",
      "deferredRevenueNonCurrent                   754\n",
      "preferredStock                              898\n",
      "otherAssets                                 954\n",
      "otherLiabilities                            982\n",
      "date                                       1006\n",
      "period                                     1006\n",
      "acceptedDate                               1006\n",
      "fillingDate                                1006\n",
      "reportedCurrency                           1006\n",
      "symbol                                     1006\n",
      "link                                       1006\n",
      "finalLink                                  1006\n",
      "dtype: int64\n",
      "\n",
      "🔍 Problematic value counts for Cash Flow statement:\n",
      "cashAtBeginningOfPeriod                      1\n",
      "netChangeInCash                              1\n",
      "changeInWorkingCapital                       4\n",
      "otherNonCashItems                           12\n",
      "depreciationAndAmortization                 14\n",
      "otherWorkingCapital                         17\n",
      "otherFinancingActivites                     45\n",
      "capitalExpenditure                          48\n",
      "investmentsInPropertyPlantAndEquipment      71\n",
      "otherInvestingActivites                     90\n",
      "debtRepayment                               98\n",
      "accountsReceivables                        134\n",
      "deferredIncomeTax                          158\n",
      "accountsPayables                           173\n",
      "stockBasedCompensation                     182\n",
      "commonStockRepurchased                     197\n",
      "dividendsPaid                              202\n",
      "effectOfForexChangesOnCash                 276\n",
      "acquisitionsNet                            357\n",
      "inventory                                  364\n",
      "purchasesOfInvestments                     401\n",
      "salesMaturitiesOfInvestments               401\n",
      "commonStockIssued                          625\n",
      "date                                      1006\n",
      "period                                    1006\n",
      "acceptedDate                              1006\n",
      "fillingDate                               1006\n",
      "reportedCurrency                          1006\n",
      "symbol                                    1006\n",
      "link                                      1006\n",
      "finalLink                                 1006\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_problematic_entries(income_data_2_years, \"Income\")\n",
    "count_problematic_entries(balance_data_2_years, \"Balance Sheet\")\n",
    "count_problematic_entries(cashflow_data_2_years, \"Cash Flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9216e8e-cb6b-4db2-916f-8c3f5fd5e734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>price</th>\n",
       "      <th>price_date</th>\n",
       "      <th>eps</th>\n",
       "      <th>netIncome</th>\n",
       "      <th>date</th>\n",
       "      <th>log_PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>138.70</td>\n",
       "      <td>None</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1289000000</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>3.441659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>257.13</td>\n",
       "      <td>None</td>\n",
       "      <td>6.11</td>\n",
       "      <td>93736000000</td>\n",
       "      <td>2024-09-28</td>\n",
       "      <td>3.739655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>236.56</td>\n",
       "      <td>None</td>\n",
       "      <td>2.40</td>\n",
       "      <td>4278000000</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>4.590733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABNB</td>\n",
       "      <td>121.49</td>\n",
       "      <td>None</td>\n",
       "      <td>4.19</td>\n",
       "      <td>2648000000</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>3.367131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABT</td>\n",
       "      <td>132.99</td>\n",
       "      <td>None</td>\n",
       "      <td>7.67</td>\n",
       "      <td>13402000000</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>2.852957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ACGL</td>\n",
       "      <td>89.08</td>\n",
       "      <td>None</td>\n",
       "      <td>11.47</td>\n",
       "      <td>4312000000</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>2.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ACN</td>\n",
       "      <td>244.34</td>\n",
       "      <td>None</td>\n",
       "      <td>12.29</td>\n",
       "      <td>7678433000</td>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>2.989775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>351.48</td>\n",
       "      <td>None</td>\n",
       "      <td>12.44</td>\n",
       "      <td>5560000000</td>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>3.341236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol   price price_date    eps    netIncome        date    log_PE\n",
       "0      A  138.70       None   4.44   1289000000  2024-10-31  3.441659\n",
       "1   AAPL  257.13       None   6.11  93736000000  2024-09-28  3.739655\n",
       "2   ABBV  236.56       None   2.40   4278000000  2024-12-31  4.590733\n",
       "3   ABNB  121.49       None   4.19   2648000000  2024-12-31  3.367131\n",
       "4    ABT  132.99       None   7.67  13402000000  2024-12-31  2.852957\n",
       "5   ACGL   89.08       None  11.47   4312000000  2024-12-31  2.049800\n",
       "6    ACN  244.34       None  12.29   7678433000  2025-08-31  2.989775\n",
       "7   ADBE  351.48       None  12.44   5560000000  2024-11-29  3.341236"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_by_symbol_then_date(df):\n",
    "    # Sort ascending by symbol, then by date (oldest first)\n",
    "    return df.sort_values([\"symbol\", \"date\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "income_sorted = sort_by_symbol_then_date(income_data_2_years)\n",
    "balance_sorted = sort_by_symbol_then_date(balance_data_2_years)\n",
    "cashflow_sorted = sort_by_symbol_then_date(cashflow_data_2_years)\n",
    "pe_data_sorted = sort_by_symbol_then_date(pe_data)\n",
    "\n",
    "\n",
    "pe_data_sorted.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f0ff733-b5e3-462d-b9d4-28467fbe55c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DOUBLE CHECK THE ECISION TO ADD A CONSTANT... ELONGATING THE TAIL OF THE DISTRIBUTION\n",
    "\n",
    "def compute_log_change(df, constant=1e-3, drop_first=True):\n",
    "    \"\"\"\n",
    "    Compute log-differences for year-over-year growth of financial statement items.\n",
    "    Keeps 'symbol' and 'date' columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must include 'symbol' and 'date' columns, sorted by both.\n",
    "    constant : float\n",
    "        Small stabilizing constant for log transform.\n",
    "    drop_first : bool\n",
    "        Whether to drop the first row per symbol (NaN after diff).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Numeric part\n",
    "    num_df = df.select_dtypes(include=[np.number])\n",
    "    log_df = np.log(np.abs(num_df) + constant)\n",
    "    log_diff = log_df.diff()\n",
    "\n",
    "    # Rename to show log-change\n",
    "    log_diff.columns = [f\"{col}_logchg\" for col in log_diff.columns]\n",
    "\n",
    "    # Combine with non-numeric columns\n",
    "    result = pd.concat([df[[\"symbol\", \"date\"]], log_diff], axis=1)\n",
    "\n",
    "    if drop_first:\n",
    "        # Drop the first row per symbol (which has NaN diffs)\n",
    "        result = result.groupby(\"symbol\", group_keys=False).apply(lambda g: g.iloc[1:])\n",
    "\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- Apply grouped by symbol ---\n",
    "income_log_change = (\n",
    "    income_sorted.groupby(\"symbol\", group_keys=False)\n",
    "    .apply(lambda g: compute_log_change(g))\n",
    ")\n",
    "\n",
    "balance_log_change = (\n",
    "    balance_sorted.groupby(\"symbol\", group_keys=False)\n",
    "    .apply(lambda g: compute_log_change(g))\n",
    ")\n",
    "\n",
    "cashflow_log_change = (\n",
    "    cashflow_sorted.groupby(\"symbol\", group_keys=False)\n",
    "    .apply(lambda g: compute_log_change(g))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "130f17b0-b137-4386-b95c-58b08ed4a839",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income 0/NaN counts per YoY column:\n",
      "                                                zeros  nans\n",
      "revenue_logchg                                      0     0\n",
      "grossProfit_logchg                                  0     0\n",
      "costAndExpenses_logchg                              0     0\n",
      "ebitda_logchg                                       0     0\n",
      "ebitdaratio_logchg                                  0     0\n",
      "operatingIncome_logchg                              0     0\n",
      "operatingIncomeRatio_logchg                         0     0\n",
      "incomeBeforeTax_logchg                              0     0\n",
      "incomeBeforeTaxRatio_logchg                         0     0\n",
      "netIncomeRatio_logchg                               0     0\n",
      "eps_logchg                                          1     0\n",
      "epsdiluted_logchg                                   1     0\n",
      "operatingExpenses_logchg                            4     0\n",
      "depreciationAndAmortization_logchg                  5     0\n",
      "costOfRevenue_logchg                                7     0\n",
      "incomeTaxExpense_logchg                             7     0\n",
      "weightedAverageShsOut_logchg                        7     0\n",
      "grossProfitRatio_logchg                            10     0\n",
      "weightedAverageShsOutDil_logchg                    10     0\n",
      "totalOtherIncomeExpensesNet_logchg                 32     0\n",
      "sellingGeneralAndAdministrativeExpenses_logchg     34     0\n",
      "interestExpense_logchg                             35     0\n",
      "otherExpenses_logchg                              137     0\n",
      "interestIncome_logchg                             169     0\n",
      "generalAndAdministrativeExpenses_logchg           248     0\n",
      "researchAndDevelopmentExpenses_logchg             295     0\n",
      "sellingAndMarketingExpenses_logchg                349     0\n",
      "\n",
      "Balance 0/NaN counts per YoY column:\n",
      "                                                zeros  nans\n",
      "totalAssets_logchg                                  0     0\n",
      "totalLiabilities_logchg                             0     0\n",
      "totalStockholdersEquity_logchg                      0     0\n",
      "totalEquity_logchg                                  0     0\n",
      "totalLiabilitiesAndStockholdersEquity_logchg        0     0\n",
      "totalLiabilitiesAndTotalEquity_logchg               0     0\n",
      "cashAndCashEquivalents_logchg                       1     0\n",
      "cashAndShortTermInvestments_logchg                  1     0\n",
      "totalNonCurrentAssets_logchg                        1     0\n",
      "totalCurrentAssets_logchg                           2     0\n",
      "otherNonCurrentAssets_logchg                        2     0\n",
      "longTermDebt_logchg                                 2     0\n",
      "totalNonCurrentLiabilities_logchg                   2     0\n",
      "totalDebt_logchg                                    2     0\n",
      "netDebt_logchg                                      2     0\n",
      "retainedEarnings_logchg                             3     0\n",
      "totalCurrentLiabilities_logchg                      6     0\n",
      "otherNonCurrentLiabilities_logchg                   6     0\n",
      "netReceivables_logchg                               8     0\n",
      "propertyPlantEquipmentNet_logchg                   10     0\n",
      "otherCurrentLiabilities_logchg                     10     0\n",
      "accumulatedOtherComprehensiveIncomeLoss_logchg     16     0\n",
      "accountPayables_logchg                             28     0\n",
      "otherCurrentAssets_logchg                          33     0\n",
      "shortTermDebt_logchg                               41     0\n",
      "othertotalStockholdersEquity_logchg                47     0\n",
      "goodwillAndIntangibleAssets_logchg                 60     0\n",
      "intangibleAssets_logchg                            91     0\n",
      "totalInvestments_logchg                            95     0\n",
      "capitalLeaseObligations_logchg                    100     0\n",
      "goodwill_logchg                                   125     0\n",
      "longTermInvestments_logchg                        150     0\n",
      "deferredTaxLiabilitiesNonCurrent_logchg           162     0\n",
      "inventory_logchg                                  166     0\n",
      "taxPayables_logchg                                172     0\n",
      "deferredRevenue_logchg                            229     0\n",
      "minorityInterest_logchg                           229     0\n",
      "taxAssets_logchg                                  249     0\n",
      "shortTermInvestments_logchg                       274     0\n",
      "commonStock_logchg                                280     0\n",
      "deferredRevenueNonCurrent_logchg                  372     0\n",
      "otherAssets_logchg                                469     0\n",
      "preferredStock_logchg                             471     0\n",
      "otherLiabilities_logchg                           486     0\n",
      "\n",
      "Cashflow 0/NaN counts per YoY column:\n",
      "                                                 zeros  nans\n",
      "netCashProvidedByOperatingActivities_logchg          0     0\n",
      "netCashUsedForInvestingActivites_logchg              0     0\n",
      "netCashUsedProvidedByFinancingActivities_logchg      0     0\n",
      "cashAtBeginningOfPeriod_logchg                       0     0\n",
      "operatingCashFlow_logchg                             0     0\n",
      "freeCashFlow_logchg                                  0     0\n",
      "netIncome_logchg                                     1     0\n",
      "cashAtEndOfPeriod_logchg                             1     0\n",
      "netChangeInCash_logchg                               2     0\n",
      "changeInWorkingCapital_logchg                        3     0\n",
      "otherNonCashItems_logchg                             4     0\n",
      "depreciationAndAmortization_logchg                   6     0\n",
      "otherWorkingCapital_logchg                           6     0\n",
      "otherFinancingActivites_logchg                      12     0\n",
      "capitalExpenditure_logchg                           24     0\n",
      "investmentsInPropertyPlantAndEquipment_logchg       26     0\n",
      "debtRepayment_logchg                                35     0\n",
      "otherInvestingActivites_logchg                      36     0\n",
      "accountsReceivables_logchg                          55     0\n",
      "deferredIncomeTax_logchg                            65     0\n",
      "accountsPayables_logchg                             77     0\n",
      "commonStockRepurchased_logchg                       77     0\n",
      "stockBasedCompensation_logchg                       87     0\n",
      "dividendsPaid_logchg                                96     0\n",
      "acquisitionsNet_logchg                             116     0\n",
      "effectOfForexChangesOnCash_logchg                  137     0\n",
      "salesMaturitiesOfInvestments_logchg                162     0\n",
      "inventory_logchg                                   164     0\n",
      "purchasesOfInvestments_logchg                      171     0\n",
      "commonStockIssued_logchg                           282     0\n"
     ]
    }
   ],
   "source": [
    "def count_zeros_nans_logchg(df):\n",
    "    # Keep only numeric columns that end with \"_logchg\"\n",
    "    numeric_cols = [c for c in df.select_dtypes(include=[float, int]).columns if c.endswith(\"_logchg\")]\n",
    "    \n",
    "    # Count zeros and NaNs\n",
    "    zero_counts = (df[numeric_cols] == 0).sum()\n",
    "    nan_counts = df[numeric_cols].isna().sum()\n",
    "    \n",
    "    # Combine into a single DataFrame\n",
    "    summary = pd.DataFrame({\n",
    "        \"zeros\": zero_counts,\n",
    "        \"nans\": nan_counts\n",
    "    }).sort_values(by=[\"zeros\", \"nans\"], ascending=True)\n",
    "    \n",
    "    # Force full display\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(summary)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "print(\"Income 0/NaN counts per YoY column:\")\n",
    "income_summary = count_zeros_nans_logchg(income_log_change)\n",
    "\n",
    "print(\"\\nBalance 0/NaN counts per YoY column:\")\n",
    "balance_summary = count_zeros_nans_logchg(balance_log_change)\n",
    "\n",
    "print(\"\\nCashflow 0/NaN counts per YoY column:\")\n",
    "cashflow_summary = count_zeros_nans_logchg(cashflow_log_change)\n",
    "\n",
    "# the purpose here is to make it easy to identify which line items (columns) are fully filled out from our sample so that we are only grabbing columns (features)\n",
    "# that are likely to be filled out by the stock under consideration, cause ultimately after we find a regression that has explanatory power... we can still only apply it \n",
    "# to the stock under consideration if it has the same line items filled out \n",
    "# the 503 nans is a result of the .pct change method that we used which creates a nan on every other row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e50f924e-d7fd-4277-a941-208f6143627b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income_post_nans: (27, 2)\n",
      "Columns: ['zeros', 'nans']\n",
      "------------------------------------------------------------\n",
      "balance_post_nans: (44, 2)\n",
      "Columns: ['zeros', 'nans']\n",
      "------------------------------------------------------------\n",
      "cashflow_post_nans: (30, 2)\n",
      "Columns: ['zeros', 'nans']\n",
      "------------------------------------------------------------\n",
      "pe_data: (480, 7)\n",
      "Columns: ['symbol', 'price', 'price_date', 'eps', 'netIncome', 'date', 'log_PE']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04bb7dc9-acca-4d01-a334-c4898a4a747d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'symbol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'symbol'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m     19\u001b[0m dfs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincome_post_nans\u001b[39m\u001b[38;5;124m\"\u001b[39m: income_summary,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalance_post_nans\u001b[39m\u001b[38;5;124m\"\u001b[39m: balance_summary,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcashflow_post_nans\u001b[39m\u001b[38;5;124m\"\u001b[39m: cashflow_summary,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpe_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: pe_data\n\u001b[1;32m     24\u001b[0m }\n\u001b[0;32m---> 26\u001b[0m \u001b[43madd_symbol_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36madd_symbol_date\u001b[0;34m(df_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mAdds a 'symbol_date' column to each DataFrame in df_dict.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mPrints the columns and shape of each updated DataFrame.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mNone (updates DataFrames in place)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;129;01min\u001b[39;00m df_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 14\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msymbol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: columns =  shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'symbol'"
     ]
    }
   ],
   "source": [
    "def add_symbol_date(df_dict):\n",
    "    \"\"\"\n",
    "    Adds a 'symbol_date' column to each DataFrame in df_dict.\n",
    "    Prints the columns and shape of each updated DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df_dict : dict\n",
    "        Dictionary of DataFrames keyed by variable names (strings)\n",
    "    \n",
    "    Returns:\n",
    "    None (updates DataFrames in place)\n",
    "    \"\"\"\n",
    "    for name, df in df_dict.items():\n",
    "        df[\"symbol_date\"] = list(zip(df[\"symbol\"], df[\"date\"]))\n",
    "        print(f\"{name}: columns =  shape = {df.shape}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "dfs = {\n",
    "    \"income_post_nans\": income_summary,\n",
    "    \"balance_post_nans\": balance_summary,\n",
    "    \"cashflow_post_nans\": cashflow_summary,\n",
    "    \"pe_data\": pe_data\n",
    "}\n",
    "\n",
    "add_symbol_date(dfs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b96a7-2d78-40a1-a8d8-5de193100403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_pairs = (\n",
    "    set(income_after_feature_drop[\"symbol_date\"])\n",
    "    & set(balance_after_feature_drop[\"symbol_date\"])\n",
    "    & set(cashflow_after_feature_drop[\"symbol_date\"])\n",
    "    & set(pe_data[\"symbol_date\"])\n",
    ")\n",
    "\n",
    "def filter_by_common_pairs(df, common_pairs):\n",
    "    \"\"\"\n",
    "    Keep only rows where the 'symbol_date' is in common_pairs.\n",
    "    Returns a new DataFrame.\n",
    "    \"\"\"\n",
    "    return df[df[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "\n",
    "income_post_nans_overlapped = filter_by_common_pairs(income_after_feature_drop, common_pairs)\n",
    "balance_post_nans_overlapped = filter_by_common_pairs(balance_after_feature_drop, common_pairs)\n",
    "cashflow_post_nans_overlapped = filter_by_common_pairs(cashflow_after_feature_drop, common_pairs)\n",
    "pe_post_nans_overlapped = filter_by_common_pairs(pe_data, common_pairs)\n",
    "\n",
    "print(income_post_nans_overlapped.shape)\n",
    "print(balance_post_nans_overlapped.shape)\n",
    "print(cashflow_post_nans_overlapped.shape)\n",
    "print(pe_post_nans_overlapped.shape)\n",
    "\n",
    "# we only want to include a ticker if we have all the financial statement items for all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ff5cc-0343-463b-a89d-32c3f8b744a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Sort consistently\n",
    "income_post_nans_overlapped = income_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "balance_post_nans_overlapped = balance_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "cashflow_post_nans_overlapped = cashflow_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "pe_post_nans_overlapped = pe_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Cleaned and aligned shapes:\", \n",
    "    income_post_nans_overlapped.shape,   \n",
    "    balance_post_nans_overlapped.shape, \n",
    "    cashflow_post_nans_overlapped.shape,\n",
    "    pe_post_nans_overlapped.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175a1b1-dae1-4a4b-9378-700529b7646c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_alignment(dfs: dict):\n",
    "    \"\"\"\n",
    "    Check alignment of multiple DataFrames on 'symbol' and 'date'.\n",
    "    Expects each DataFrame to have the same row order and columns: 'symbol', 'date'.\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    None (prints summary of mismatches)\n",
    "    \"\"\"\n",
    "    # Ensure equal lengths\n",
    "    lengths = {name: len(df) for name, df in dfs.items()}\n",
    "    if len(set(lengths.values())) > 1:\n",
    "        print(\"⚠️ Row counts differ between DataFrames:\")\n",
    "        for name, length in lengths.items():\n",
    "            print(f\"  {name}: {length} rows\")\n",
    "    else:\n",
    "        print(f\"✅ All DataFrames have {list(lengths.values())[0]} rows\")\n",
    "\n",
    "    # Concatenate for comparison\n",
    "    merged = pd.concat(\n",
    "        {name: df[[\"symbol\", \"date\"]].reset_index(drop=True) for name, df in dfs.items()},\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Compare across DataFrames\n",
    "    base = list(dfs.keys())[0]  # pick first as reference\n",
    "    symbol_mismatches = 0\n",
    "    date_mismatches = 0\n",
    "\n",
    "    for i in range(len(dfs[base])):\n",
    "        symbols = [merged[(name, \"symbol\")][i] for name in dfs.keys()]\n",
    "        dates   = [merged[(name, \"date\")][i]   for name in dfs.keys()]\n",
    "        if len(set(symbols)) > 1:\n",
    "            symbol_mismatches += 1\n",
    "        if len(set(dates)) > 1:\n",
    "            date_mismatches += 1\n",
    "\n",
    "    print(f\"Symbol mismatches: {symbol_mismatches}\")\n",
    "    print(f\"Date mismatches:   {date_mismatches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd750a7-c30a-454c-9dad-c60417d95332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_alignment({\n",
    "    \"income\": income_post_nans_overlapped,\n",
    "    \"balance\": balance_post_nans_overlapped,\n",
    "    \"cashflow\": cashflow_post_nans_overlapped,\n",
    "    \"pe\": pe_post_nans_overlapped\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2766c-a364-4fce-8953-7b5dba753584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def outlier_check_1(df, title):\n",
    "    # Flatten all numeric columns into one long vector\n",
    "    numeric_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns\n",
    "    values = df[numeric_cols].values.flatten()\n",
    "    values = values[~np.isnan(values)]  # drop NaNs\n",
    "\n",
    "    # Scatter vs. index, colored by density\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(values, fill=True, color=\"lightblue\", alpha=0.3, linewidth=0)  # background density\n",
    "    plt.scatter(range(len(values)), values, \n",
    "                c=values, cmap=\"viridis\", s=5, alpha=0.6)\n",
    "\n",
    "    plt.title(f\"Density Scatterplot: {title}\", fontsize=14)\n",
    "    plt.xlabel(\"Index\", fontsize=12)\n",
    "    plt.ylabel(\"Value\", fontsize=12)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Call for each of your cleaned DataFrames\n",
    "outlier_check_1(income_log_change, \"Income\")\n",
    "outlier_check_1(balance_log_change, \"Balance\")\n",
    "outlier_check_1(cashflow_log_change, \"Cashflow\")\n",
    "\n",
    "#demonstrates the necessity for addressing outlier concerns \n",
    "# ONLY PURPOSE OF. THIS IS TO CONFIRM WE HAVE SERIOUS OUTLIERS THAT NEED TO BE ADDRESSED\n",
    "\n",
    "\n",
    "def drop_high_zero_columns(df, threshold=0.03):\n",
    "    \"\"\"\n",
    "    Drops columns with more than `threshold` proportion of zero values,\n",
    "    and reports how many columns were dropped.\n",
    "    \"\"\"\n",
    "    original_col_count = df.shape[1]\n",
    "\n",
    "    zero_proportion = (df == 0).sum() / len(df)\n",
    "    cols_to_drop = zero_proportion[zero_proportion > threshold].index\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    new_col_count = df.shape[1]\n",
    "\n",
    "    print(f\"Columns before: {original_col_count}\")\n",
    "    print(f\"Columns dropped: {len(cols_to_drop)} (>{threshold*100:.1f}% zeros)\")\n",
    "    print(f\"Columns after: {new_col_count}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81348b74-38eb-4ab3-b371-afa3c03389ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_after_feature_drop = drop_high_zero_columns(income_log_change, threshold=0.05)\n",
    "balance_after_feature_drop = drop_high_zero_columns(balance_log_change, threshold=0.05)\n",
    "cashflow_after_feature_drop = drop_high_zero_columns(cashflow_log_change, threshold=0.05)\n",
    "\n",
    "# Call for each of your cleaned DataFrames\n",
    "outlier_check_1(income_after_feature_drop, \"Income\")\n",
    "outlier_check_1(balance_after_feature_drop, \"Balance\")\n",
    "outlier_check_1(cashflow_after_feature_drop, \"Cashflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77731b1-1937-4252-9909-9bd8bbca7204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_outlier_rows(df, threshold=10):\n",
    "    \"\"\"\n",
    "    Drops rows where ANY numeric column has a value greater than `threshold`\n",
    "    or less than `-threshold`. Prints how many rows remain afterward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to filter.\n",
    "    threshold : float\n",
    "        Absolute cutoff for detecting outliers (e.g., 10 keeps rows within [-10, 10]).\n",
    "    \"\"\"\n",
    "    original_row_count = df.shape[0]\n",
    "\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "    # Create mask: keep rows where all numeric values are within [-threshold, threshold]\n",
    "    mask = (df[numeric_cols].abs() <= threshold).all(axis=1)\n",
    "    df_filtered = df[mask].copy()\n",
    "\n",
    "    new_row_count = df_filtered.shape[0]\n",
    "    dropped_rows = original_row_count - new_row_count\n",
    "\n",
    "    print(f\"Rows before: {original_row_count}\")\n",
    "    print(f\"Rows dropped: {dropped_rows} (>|{threshold}| in any numeric column)\")\n",
    "    print(f\"Rows after: {new_row_count}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# --- Drop columns with too many zeros ---\n",
    "income_after_feature_drop   = drop_high_zero_columns(income_log_change, threshold=0.05)\n",
    "balance_after_feature_drop  = drop_high_zero_columns(balance_log_change, threshold=0.05)\n",
    "cashflow_after_feature_drop = drop_high_zero_columns(cashflow_log_change, threshold=0.05)\n",
    "\n",
    "# --- Drop rows with extreme outliers (values beyond ±10) ---\n",
    "income_after_outlier_drop   = drop_outlier_rows(income_after_feature_drop, threshold=10)\n",
    "balance_after_outlier_drop  = drop_outlier_rows(balance_after_feature_drop, threshold=10)\n",
    "cashflow_after_outlier_drop = drop_outlier_rows(cashflow_after_feature_drop, threshold=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7067c-0a6b-4032-a6b1-249ac6e49ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def run_statement_univariate(statement_df, log_pe_df, label=\"\", plot=False):\n",
    "    merged = statement_df.merge(log_pe_df, on=\"symbol\", how=\"inner\")\n",
    "    X = merged.select_dtypes(include=[\"number\"]).drop(columns=[\"log_PE\"])\n",
    "    y = merged[\"log_PE\"]\n",
    "\n",
    "    results = []\n",
    "    for col in X.columns:\n",
    "        X_var = sm.add_constant(X[col])\n",
    "        model = sm.OLS(y, X_var, missing=\"drop\").fit()\n",
    "        residuals = model.resid\n",
    "        fitted = model.fittedvalues\n",
    "\n",
    "        # --- Tests ---\n",
    "        bp_test = het_breuschpagan(residuals, X_var)\n",
    "        bp_pvalue = bp_test[1]  # p-value for heteroskedasticity\n",
    "        shapiro_pvalue = shapiro(residuals)[1] if len(residuals) < 5000 else None  # Shapiro limited to <5000 obs\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": col,\n",
    "            \"coef\": model.params[col],\n",
    "            \"t_value\": model.tvalues[col],\n",
    "            \"p_value\": model.pvalues[col],\n",
    "            \"bp_pvalue\": bp_pvalue,\n",
    "            \"shapiro_pvalue\": shapiro_pvalue\n",
    "        })\n",
    "\n",
    "        # --- Optional diagnostic plots ---\n",
    "        if plot:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            fig.suptitle(f\"{label}: Residual Diagnostics for {col}\", fontsize=12)\n",
    "\n",
    "            sns.scatterplot(x=fitted, y=residuals, alpha=0.6, ax=axes[0])\n",
    "            axes[0].axhline(0, color=\"red\", linestyle=\"--\", lw=1)\n",
    "            axes[0].set_xlabel(\"Fitted Values\")\n",
    "            axes[0].set_ylabel(\"Residuals\")\n",
    "            axes[0].set_title(\"Residuals vs Fitted\")\n",
    "\n",
    "            sns.histplot(residuals, kde=True, ax=axes[1], color=\"teal\")\n",
    "            axes[1].set_title(\"Residual Distribution\")\n",
    "            axes[1].set_xlabel(\"Residuals\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"p_value\")\n",
    "\n",
    "    return merged, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69492bc9-8225-473d-8ef3-59497cb5e81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged, uni_results = run_statement_univariate(income_post_nans_overlapped, log_pe_df, label=\"Income\", plot=False)\n",
    "merged, uni_results = run_statement_univariate(balance_post_nans_overlapped, log_pe_df, label=\"Income\", plot=False)\n",
    "merged, uni_results = run_statement_univariate(cashflow_post_nans_overlapped, log_pe_df, label=\"Income\", plot=False)\n",
    "\n",
    "\n",
    "# Filter features that fail normality or homoskedasticity\n",
    "bad_features = uni_results[\n",
    "    (uni_results[\"bp_pvalue\"] < 0.05) | (uni_results[\"shapiro_pvalue\"] < 0.05)\n",
    "]\n",
    "print(\"Problematic features:\")\n",
    "print(bad_features[[\"feature\", \"bp_pvalue\", \"shapiro_pvalue\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e811299-ba72-48c4-9b50-fc90c8d29308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine for convenience, but we’ll split by statement type\n",
    "plot_df = pd.concat([\n",
    "    income_univariate.assign(statement_type='Income'),\n",
    "    balance_univariate.assign(statement_type='Balance'),\n",
    "    cashflow_univariate.assign(statement_type='Cashflow')\n",
    "], ignore_index=True)\n",
    "\n",
    "statement_order = ['Income', 'Balance', 'Cashflow']\n",
    "n_statements = len(statement_order)\n",
    "\n",
    "# Create subplots (one per statement type)\n",
    "fig, axes = plt.subplots(n_statements, 1, figsize=(12, 5*n_statements), sharex=True)\n",
    "\n",
    "for i, statement in enumerate(statement_order):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Subset for this statement\n",
    "    df = plot_df[plot_df['statement_type'] == statement].copy()\n",
    "    \n",
    "    # Sort by absolute t-value descending\n",
    "    df = df.sort_values('t_value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Normalize r² for color mapping\n",
    "    norm = mpl.colors.Normalize(vmin=df['r2'].min(), vmax=df['r2'].max())\n",
    "    cmap = mpl.cm.viridis\n",
    "    colors = [cmap(norm(val)) for val in df['r2']]\n",
    "    \n",
    "    # Y positions\n",
    "    y_pos = range(len(df))\n",
    "    \n",
    "    # Horizontal bar chart\n",
    "    ax.barh(y=y_pos, width=df['t_value'], color=colors, edgecolor='black')\n",
    "    \n",
    "    # Highlight significant features\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if row.reject_null:\n",
    "            ax.text(\n",
    "                x=row.t_value + (0.05 if row.t_value>0 else -0.05),\n",
    "                y=idx,\n",
    "                s='*',\n",
    "                va='center',\n",
    "                ha='left' if row.t_value>0 else 'right',\n",
    "                color='red',\n",
    "                fontsize=12\n",
    "            )\n",
    "    \n",
    "    # Y-axis labels\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(df['feature'], fontsize=10)\n",
    "    ax.axvline(0, color='black', linewidth=0.8)\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_title(f'{statement} Statement: t-values by Feature (Color = R²)')\n",
    "\n",
    "# Shared x-label\n",
    "axes[-1].set_xlabel('t-value')\n",
    "\n",
    "# Add a single colorbar\n",
    "sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=axes, orientation='vertical', fraction=0.02, pad=0.01)\n",
    "cbar.set_label('R²')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad612d8e-3a7f-4d6d-be69-32f8e60502cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_significant_features(univariate_results, merged_df, \n",
    "                                t_threshold=1.2, r2_threshold=0.005, p_threshold=0.2, \n",
    "                                label=\"\"):\n",
    "    \"\"\"\n",
    "    Select significant features from univariate regression results \n",
    "    and return the corresponding data from the merged dataframe.\n",
    "    \"\"\"\n",
    "    total_features = len(univariate_results)\n",
    "\n",
    "    # Filter by thresholds\n",
    "    signif = univariate_results[\n",
    "        (univariate_results[\"t_value\"].abs() > t_threshold) &\n",
    "        (univariate_results[\"r2\"] > r2_threshold) &\n",
    "        (univariate_results[\"pval\"] < p_threshold)\n",
    "    ]\n",
    "\n",
    "    features = signif[\"feature\"].tolist()\n",
    "    selected = merged_df[[\"symbol\",\"date\",\"symbol_date\", \"log_PE\"] + features]\n",
    "\n",
    "    if label:\n",
    "        n_selected = len(features)\n",
    "        pct = (n_selected / total_features * 100) if total_features > 0 else 0\n",
    "        print(f\"{label.title()}: {n_selected}/{total_features} features selected ({pct:.1f}%)\")\n",
    "\n",
    "    return signif, selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbab16b-dc57-4faf-9aa3-4d5fa629abbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "signif_income, income_selected = select_significant_features(income_univariate, income_merged, label=\"income\")\n",
    "\n",
    "signif_balance, balance_selected = select_significant_features(balance_univariate, balance_merged, label=\"balance\")\n",
    "\n",
    "signif_cashflow, cashflow_selected = select_significant_features(cashflow_univariate, cashflow_merged, label=\"cashflow\")\n",
    "\n",
    "\n",
    "print(income_selected.shape)\n",
    "print(balance_selected.shape)\n",
    "print(cashflow_selected.shape)\n",
    "print(income_selected.columns)\n",
    "print(balance_selected.columns)\n",
    "print(cashflow_selected.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ea8cf-dd9c-4eea-9fa8-55a1f1f1b386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pca(df, n_components=None, prefix=\"\", columns=None):\n",
    "    \"\"\"\n",
    "    Run PCA on a DataFrame with optional column selection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input data.\n",
    "    n_components : int or None\n",
    "        Number of PCA components. If None, use all available features.\n",
    "    prefix : str\n",
    "        Prefix for the PCA component column names.\n",
    "    columns : list of str or None\n",
    "        Subset of columns to run PCA on. If None, use all numeric columns\n",
    "        except common ID columns like symbol/date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with PCA component columns added.\n",
    "    PCA\n",
    "        The fitted PCA object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop common ID columns if present\n",
    "    exclude_cols = [\"symbol\", \"date\", \"symbol_date\", \"log_PE\"]\n",
    "\n",
    "    if columns is None:\n",
    "        feature_df = df.drop(columns=[c for c in exclude_cols if c in df.columns], errors=\"ignore\")\n",
    "    else:\n",
    "        feature_df = df[columns]\n",
    "\n",
    "    # Make sure it's numeric\n",
    "    feature_df = feature_df.select_dtypes(include=\"number\")\n",
    "\n",
    "    # Handle n_components safely\n",
    "    max_components = min(feature_df.shape[0], feature_df.shape[1])\n",
    "    if n_components is None:\n",
    "        n_components = max_components\n",
    "    else:\n",
    "        n_components = min(n_components, max_components)\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(feature_df)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = pca.fit_transform(scaled)\n",
    "\n",
    "    comp_df = pd.DataFrame(\n",
    "        components,\n",
    "        columns=[f\"{prefix}{i+1}\" for i in range(components.shape[1])],\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    result_df = pd.concat([df, comp_df], axis=1)\n",
    "\n",
    "    # Print shape\n",
    "    print(f\"PCA Result Shape ({prefix}): {result_df.shape}\")\n",
    "\n",
    "    return result_df, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fcdd7-57d2-4bb2-bf2a-f3c0355ae096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Income statement PCA\n",
    "income_df_with_pca, income_pca_model = run_pca(\n",
    "    income_selected,        # Pass full DataFrame; ID columns are preserved\n",
    "    n_components=3,\n",
    "    prefix=\"income_\"\n",
    ")\n",
    "\n",
    "# Balance sheet PCA\n",
    "balance_df_with_pca, balance_pca_model = run_pca(\n",
    "    balance_selected,\n",
    "    n_components=3,\n",
    "    prefix=\"balance_\"\n",
    ")\n",
    "\n",
    "# Cash flow PCA\n",
    "cashflow_df_with_pca, cashflow_pca_model = run_pca(\n",
    "    cashflow_selected,\n",
    "    n_components=3,\n",
    "    prefix=\"cashflow_\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21b6c7-c1f9-4852-ab71-01d31f700497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(income_df_with_pca.columns)\n",
    "print(balance_df_with_pca.columns)\n",
    "print(cashflow_df_with_pca.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4b15a-dc38-4987-98af-029cf9d16292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def regress_log_pe_on_pca(pca_df, prefix, label=\"\"):\n",
    "    \"\"\"\n",
    "    Run regression of log(PE) on PCA components and return both model + summary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pca_df : pd.DataFrame\n",
    "        DataFrame containing PCA components and 'log_PE' column.\n",
    "    prefix : str\n",
    "        Prefix for the PCA component names (e.g., 'income_', 'balance_', 'cashflow_').\n",
    "    label : str, optional\n",
    "        Label to display in printed output and summary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : statsmodels RegressionResults\n",
    "        The fitted regression model.\n",
    "    summary_df : pd.DataFrame\n",
    "        Compact summary of coefficients, t-values, and p-values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure log_PE exists\n",
    "    if \"log_PE\" not in pca_df.columns:\n",
    "        raise KeyError(\"'log_PE' column not found in provided DataFrame.\")\n",
    "\n",
    "    # Select PCA columns\n",
    "    pca_cols = [c for c in pca_df.columns if c.startswith(prefix)]\n",
    "    X = pca_df[pca_cols]\n",
    "    y = pca_df[\"log_PE\"]\n",
    "\n",
    "    # Add intercept\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Run regression\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"coef\": model.params,\n",
    "        \"t_value\": model.tvalues,\n",
    "        \"p_value\": model.pvalues\n",
    "    }).reset_index().rename(columns={\"index\": \"variable\"})\n",
    "\n",
    "    if label:\n",
    "        summary_df[\"label\"] = label\n",
    "        print(f\"{label.title()} PCA Regression:\")\n",
    "        print(f\"  Components: {len(pca_cols)}\")\n",
    "        print(f\"  Observations: {len(pca_df)}\")\n",
    "        print(f\"  R-squared: {model.rsquared:.4f}\\n\")\n",
    "\n",
    "    return model, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04f486-970a-4958-b919-0723c66180f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_model, income_summary = regress_log_pe_on_pca(income_df_with_pca, prefix=\"income_\", label=\"income\")\n",
    "balance_model, balance_summary = regress_log_pe_on_pca(balance_df_with_pca, prefix=\"balance_\", label=\"balance\")\n",
    "cashflow_model, cashflow_summary = regress_log_pe_on_pca(cashflow_df_with_pca, prefix=\"cashflow_\", label=\"cashflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0bae18-28dc-4965-9c5a-2e0b142925db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_pca_summaries = pd.concat([income_summary, balance_summary, cashflow_summary], ignore_index=True)\n",
    "display(all_pca_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9c3d3-3020-4885-a398-0a080c86cda3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
