{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9ed79e-112a-418f-b955-5c13dd46dd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import gaussian_kde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2982057a-32e3-43e6-b761-6240815efa25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tickers from CSV cache...\n",
      "(503, 8)\n",
      "Index(['symbol', 'name', 'sector', 'subSector', 'headQuarter',\n",
      "       'dateFirstAdded', 'cik', 'founded'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "\n",
    "# THIS pulls sector and subsector info either localy if it's cached or from the api\n",
    "\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "\n",
    "# --- LOAD FROM CACHE OR FETCH ---\n",
    "if os.path.exists(tickers_csv_file):\n",
    "    print(\"Loading tickers from CSV cache...\")\n",
    "    df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "else:\n",
    "    print(\"Fetching tickers from API...\")\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/sp500_constituent?apikey={API_KEY}\"\n",
    "    df_sp500 = pd.DataFrame(requests.get(url).json())\n",
    "\n",
    "    # Save to CSV\n",
    "    df_sp500.to_csv(tickers_csv_file, index=False)\n",
    "    print(f\"Saved {len(df_sp500)} tickers to CSV cache.\")\n",
    "\n",
    "    \n",
    "# --- PREVIEW ---\n",
    "print(df_sp500.shape)\n",
    "print(df_sp500.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d2a811-89dd-4a1d-b195-4f3af35f41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d8a247-31e3-486c-a090-35f4860c348f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/price_and_earnings.json\n",
      "Index(['symbol', 'price', 'price_date', 'eps', 'netIncome', 'date'], dtype='object')\n",
      "(503, 6)\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "# Load tickers\n",
    "df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()\n",
    "\n",
    "# Output file\n",
    "output_file = os.path.join(data_folder, \"price_and_earnings.json\")\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Get current price\n",
    "            quote_url = f\"https://financialmodelingprep.com/api/v3/quote/{ticker}?apikey={API_KEY}\"\n",
    "            price_data = requests.get(quote_url).json()\n",
    "            if not price_data:\n",
    "                continue\n",
    "            price = price_data[0][\"price\"]\n",
    "            price_date = price_data[0].get(\"date\")  # trading date\n",
    "\n",
    "            # Get latest annual income statement\n",
    "            income_url = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?limit=1&apikey={API_KEY}\"\n",
    "            income_data = requests.get(income_url).json()\n",
    "            if not income_data:\n",
    "                continue\n",
    "            eps = income_data[0].get(\"eps\")\n",
    "            net_income = income_data[0].get(\"netIncome\")\n",
    "            report_date = income_data[0].get(\"date\")  # fiscal period end date\n",
    "\n",
    "            records.append({\n",
    "                \"symbol\": ticker,\n",
    "                \"price\": price,\n",
    "                \"price_date\": price_date,\n",
    "                \"eps\": eps,\n",
    "                \"netIncome\": net_income,\n",
    "                \"date\": report_date\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "        time.sleep(0.2)  # polite rate limit\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "price_earnings_df = fetch_price_and_earnings(tickers, output_file)\n",
    "print(price_earnings_df.columns)\n",
    "print(price_earnings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb737c07-dc25-4514-858c-f5178907b5be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 7)\n"
     ]
    }
   ],
   "source": [
    "#CLEANS AND TAKES LOG OF THE PE DATA\n",
    "\n",
    "# Only keep rows with positive EPS\n",
    "pe_data = price_earnings_df[price_earnings_df[\"eps\"] > 0].copy()\n",
    "\n",
    "# Compute log(PE)\n",
    "pe_data[\"log_PE\"] = np.log(pe_data[\"price\"] / pe_data[\"eps\"])\n",
    "\n",
    "# Print row count for reference\n",
    "print(pe_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1844560d-77dc-4f2a-8993-96bb263662be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/income-statement_annual_limit2.json\n",
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/balance-sheet-statement_annual_limit2.json\n",
      "Loading from cache: /Users/nicholassanso/Desktop/Trading/Data/cash-flow-statement_annual_limit2.json\n",
      "Income shape: (1006, 38)\n",
      "Balance shape: (1006, 54)\n",
      "Cash flow shape: (1006, 40)\n"
     ]
    }
   ],
   "source": [
    "# FETCHES INCOME STATEMENT AND BS STATEMENT AND CF STATEMENT\n",
    "def fetch_statement(endpoint, tickers, period, limit, data_folder):\n",
    "    \"\"\"Fetch statements with unique JSON filename based on endpoint, period, limit.\"\"\"\n",
    "    output_file = os.path.join(\n",
    "        data_folder,\n",
    "        f\"{endpoint}_{period}_limit{limit}.json\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        with open(output_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        url = f\"https://financialmodelingprep.com/api/v3/{endpoint}/{ticker}?period={period}&limit={limit}&apikey={API_KEY}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                for row in data:\n",
    "                    row[\"symbol\"] = ticker\n",
    "                records.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker} ({endpoint}): {e}\")\n",
    "        time.sleep(.2)  # API polite rate limit\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "    # \"RECORDS HERE IS A LIST OF STRINGS NOT A DF\"\n",
    "    return records\n",
    "\n",
    "# AT THIS POINT INCOME_DATA_2_YEARS IS STILL A LIST OF STRINGS STORED AS A VAR\n",
    "income_data_2_years   = fetch_statement(\"income-statement\", tickers, \"annual\", 2, data_folder)\n",
    "balance_data_2_years  = fetch_statement(\"balance-sheet-statement\", tickers, \"annual\", 2, data_folder)\n",
    "cashflow_data_2_years = fetch_statement(\"cash-flow-statement\", tickers, \"annual\", 2, data_folder)\n",
    "\n",
    "# INCOME_DATA_2_YEARS IS CONVERTED TO A DF\n",
    "income_data_2_years   = pd.DataFrame(income_data_2_years)\n",
    "balance_data_2_years  = pd.DataFrame(balance_data_2_years)\n",
    "cashflow_data_2_years = pd.DataFrame(cashflow_data_2_years)\n",
    "\n",
    "print(\"Income shape:\", income_data_2_years.shape)\n",
    "print(\"Balance shape:\", balance_data_2_years.shape)\n",
    "print(\"Cash flow shape:\", cashflow_data_2_years.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031400b-6a30-4ff6-ab53-72963cfe5a96",
   "metadata": {},
   "source": [
    "# End of fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e321a26-3485-4841-aea9-0e069cd8dc23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_data_2_years = income_data_2_years.drop(columns=[\"netIncome\"], errors=\"ignore\")\n",
    "income_data_2_years = income_data_2_years.drop(columns=[\"eps\"], errors=\"ignore\")\n",
    "balance_data_2_years = balance_data_2_years.drop(columns=[\"eps\"], errors=\"ignore\")\n",
    "price_earnings_df = price_earnings_df.drop(columns=[\"eps\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b245b2-1b76-49e2-a37d-e3d08f3acc87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income_data_2_years: cleaned column names\n",
      "balance_data_2_years: cleaned column names\n",
      "cashflow_data_2_years: cleaned column names\n",
      "pe_data: cleaned column names\n"
     ]
    }
   ],
   "source": [
    "# --- Clean up column names (remove spaces, tabs, and backslashes) ---\n",
    "for df_name, df in {\n",
    "    \"income_data_2_years\": income_data_2_years,\n",
    "    \"balance_data_2_years\": balance_data_2_years,\n",
    "    \"cashflow_data_2_years\": cashflow_data_2_years,\n",
    "    \"pe_data\": pe_data\n",
    "}.items():\n",
    "    df.columns = df.columns.str.strip().str.replace(r'\\\\', '', regex=True)\n",
    "    print(f\"{df_name}: cleaned column names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b213f63-6b7f-4f36-822d-5d301e65400d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_problematic_entries(df, label):\n",
    "    print(f\"\\nüîç Problematic value counts for {label} statement:\")\n",
    "    \n",
    "    def count_issues(col):\n",
    "        numeric_col = pd.to_numeric(col, errors='coerce')\n",
    "        return ((numeric_col.isna()) | (numeric_col == 0)).sum()\n",
    "\n",
    "    issue_counts = df.apply(count_issues)\n",
    "    filtered_counts = issue_counts[issue_counts > 0].sort_values()\n",
    "    print(filtered_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "391db554-2c54-4798-8835-6b83f817c3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Problematic value counts for Income statement:\n",
      "operatingExpenses                             8\n",
      "depreciationAndAmortization                   9\n",
      "incomeTaxExpense                             12\n",
      "costOfRevenue                                20\n",
      "interestExpense                              63\n",
      "totalOtherIncomeExpensesNet                  65\n",
      "sellingGeneralAndAdministrativeExpenses      79\n",
      "otherExpenses                               330\n",
      "interestIncome                              352\n",
      "generalAndAdministrativeExpenses            548\n",
      "researchAndDevelopmentExpenses              605\n",
      "sellingAndMarketingExpenses                 727\n",
      "date                                       1006\n",
      "period                                     1006\n",
      "acceptedDate                               1006\n",
      "fillingDate                                1006\n",
      "reportedCurrency                           1006\n",
      "symbol                                     1006\n",
      "link                                       1006\n",
      "finalLink                                  1006\n",
      "dtype: int64\n",
      "\n",
      "üîç Problematic value counts for Balance Sheet statement:\n",
      "totalNonCurrentAssets                         2\n",
      "totalNonCurrentLiabilities                    4\n",
      "otherNonCurrentAssets                         5\n",
      "totalDebt                                     5\n",
      "totalCurrentAssets                            5\n",
      "longTermDebt                                  8\n",
      "totalCurrentLiabilities                      12\n",
      "otherNonCurrentLiabilities                   15\n",
      "retainedEarnings                             15\n",
      "netReceivables                               20\n",
      "propertyPlantEquipmentNet                    25\n",
      "otherCurrentLiabilities                      28\n",
      "accumulatedOtherComprehensiveIncomeLoss      32\n",
      "accountPayables                              62\n",
      "commonStock                                  70\n",
      "otherCurrentAssets                           74\n",
      "goodwillAndIntangibleAssets                  79\n",
      "shortTermDebt                                96\n",
      "othertotalStockholdersEquity                 97\n",
      "goodwill                                    106\n",
      "intangibleAssets                            184\n",
      "capitalLeaseObligations                     217\n",
      "totalInvestments                            232\n",
      "inventory                                   327\n",
      "longTermInvestments                         340\n",
      "deferredTaxLiabilitiesNonCurrent            349\n",
      "taxPayables                                 423\n",
      "minorityInterest                            440\n",
      "deferredRevenue                             479\n",
      "taxAssets                                   526\n",
      "shortTermInvestments                        581\n",
      "deferredRevenueNonCurrent                   754\n",
      "preferredStock                              898\n",
      "otherAssets                                 954\n",
      "otherLiabilities                            982\n",
      "date                                       1006\n",
      "period                                     1006\n",
      "acceptedDate                               1006\n",
      "fillingDate                                1006\n",
      "reportedCurrency                           1006\n",
      "symbol                                     1006\n",
      "link                                       1006\n",
      "finalLink                                  1006\n",
      "dtype: int64\n",
      "\n",
      "üîç Problematic value counts for Cash Flow statement:\n",
      "cashAtBeginningOfPeriod                      1\n",
      "netChangeInCash                              1\n",
      "changeInWorkingCapital                       4\n",
      "otherNonCashItems                           12\n",
      "depreciationAndAmortization                 14\n",
      "otherWorkingCapital                         17\n",
      "otherFinancingActivites                     45\n",
      "capitalExpenditure                          48\n",
      "investmentsInPropertyPlantAndEquipment      71\n",
      "otherInvestingActivites                     90\n",
      "debtRepayment                               98\n",
      "accountsReceivables                        134\n",
      "deferredIncomeTax                          158\n",
      "accountsPayables                           173\n",
      "stockBasedCompensation                     182\n",
      "commonStockRepurchased                     197\n",
      "dividendsPaid                              202\n",
      "effectOfForexChangesOnCash                 276\n",
      "acquisitionsNet                            357\n",
      "inventory                                  364\n",
      "purchasesOfInvestments                     401\n",
      "salesMaturitiesOfInvestments               401\n",
      "commonStockIssued                          625\n",
      "date                                      1006\n",
      "period                                    1006\n",
      "acceptedDate                              1006\n",
      "fillingDate                               1006\n",
      "reportedCurrency                          1006\n",
      "symbol                                    1006\n",
      "link                                      1006\n",
      "finalLink                                 1006\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_problematic_entries(income_data_2_years, \"Income\")\n",
    "count_problematic_entries(balance_data_2_years, \"Balance Sheet\")\n",
    "count_problematic_entries(cashflow_data_2_years, \"Cash Flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9216e8e-cb6b-4db2-916f-8c3f5fd5e734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>price</th>\n",
       "      <th>price_date</th>\n",
       "      <th>eps</th>\n",
       "      <th>netIncome</th>\n",
       "      <th>date</th>\n",
       "      <th>log_PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>138.70</td>\n",
       "      <td>None</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1289000000</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>3.441659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>257.13</td>\n",
       "      <td>None</td>\n",
       "      <td>6.11</td>\n",
       "      <td>93736000000</td>\n",
       "      <td>2024-09-28</td>\n",
       "      <td>3.739655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>236.56</td>\n",
       "      <td>None</td>\n",
       "      <td>2.40</td>\n",
       "      <td>4278000000</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>4.590733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABNB</td>\n",
       "      <td>121.49</td>\n",
       "      <td>None</td>\n",
       "      <td>4.19</td>\n",
       "      <td>2648000000</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>3.367131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABT</td>\n",
       "      <td>132.99</td>\n",
       "      <td>None</td>\n",
       "      <td>7.67</td>\n",
       "      <td>13402000000</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>2.852957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ACGL</td>\n",
       "      <td>89.08</td>\n",
       "      <td>None</td>\n",
       "      <td>11.47</td>\n",
       "      <td>4312000000</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>2.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ACN</td>\n",
       "      <td>244.34</td>\n",
       "      <td>None</td>\n",
       "      <td>12.29</td>\n",
       "      <td>7678433000</td>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>2.989775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>351.48</td>\n",
       "      <td>None</td>\n",
       "      <td>12.44</td>\n",
       "      <td>5560000000</td>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>3.341236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol   price price_date    eps    netIncome        date    log_PE\n",
       "0      A  138.70       None   4.44   1289000000  2024-10-31  3.441659\n",
       "1   AAPL  257.13       None   6.11  93736000000  2024-09-28  3.739655\n",
       "2   ABBV  236.56       None   2.40   4278000000  2024-12-31  4.590733\n",
       "3   ABNB  121.49       None   4.19   2648000000  2024-12-31  3.367131\n",
       "4    ABT  132.99       None   7.67  13402000000  2024-12-31  2.852957\n",
       "5   ACGL   89.08       None  11.47   4312000000  2024-12-31  2.049800\n",
       "6    ACN  244.34       None  12.29   7678433000  2025-08-31  2.989775\n",
       "7   ADBE  351.48       None  12.44   5560000000  2024-11-29  3.341236"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_by_symbol_then_date(df):\n",
    "    # Sort ascending by symbol, then by date (oldest first)\n",
    "    return df.sort_values([\"symbol\", \"date\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "income_sorted = sort_by_symbol_then_date(income_data_2_years)\n",
    "balance_sorted = sort_by_symbol_then_date(balance_data_2_years)\n",
    "cashflow_sorted = sort_by_symbol_then_date(cashflow_data_2_years)\n",
    "pe_data_sorted = sort_by_symbol_then_date(pe_data)\n",
    "\n",
    "\n",
    "pe_data_sorted.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f0ff733-b5e3-462d-b9d4-28467fbe55c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DOUBLE CHECK THE ECISION TO ADD A CONSTANT... ELONGATING THE TAIL OF THE DISTRIBUTION\n",
    "# DATA PROVIDER PUTS \"0\" FOR ANY LINE ITEM THAT DOESN'T EXIST IN THE FINANCIAL STATEMENT... ADDING THE CONSTANT THEN RETURNS A SMALL VALUE WHICH IN TURN \n",
    "#ALLOWS FOR A HUGE LOG CHANGE OUTLIER IF THAT LINE ITEM IN THE NEXT FINANCIALS STATEMENT IS ANYHTING OTHER THAN 0?\n",
    "\n",
    "def compute_log_change(df, constant=1e-3, drop_first=True):\n",
    "    \"\"\"\n",
    "    Compute log-differences for year-over-year growth of financial statement items.\n",
    "    retains 'symbol' and 'date' columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must include 'symbol' and 'date' columns, sorted by both.\n",
    "    constant : float\n",
    "        Small stabilizing constant for log transform.\n",
    "    drop_first : bool\n",
    "        Whether to drop the first row per symbol (NaN after diff).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Numeric part\n",
    "    num_df = df.select_dtypes(include=[np.number])\n",
    "    log_df = np.log(np.abs(num_df) + constant)\n",
    "    log_diff = log_df.diff()\n",
    "\n",
    "    # Rename to show log-change\n",
    "    log_diff.columns = [f\"{col}_logchg\" for col in log_diff.columns]\n",
    "\n",
    "    # Combine with non-numeric columns\n",
    "    result = pd.concat([df[[\"symbol\", \"date\"]], log_diff], axis=1)\n",
    "\n",
    "    if drop_first:\n",
    "        # Drop the first row per symbol (which has NaN diffs)\n",
    "        result = result.groupby(\"symbol\", group_keys=False).apply(lambda g: g.iloc[1:])\n",
    "\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- Apply grouped by symbol ---\n",
    "income_log_change = (\n",
    "    income_sorted.groupby(\"symbol\", group_keys=False)\n",
    "    .apply(lambda g: compute_log_change(g))\n",
    ")\n",
    "\n",
    "balance_log_change = (\n",
    "    balance_sorted.groupby(\"symbol\", group_keys=False)\n",
    "    .apply(lambda g: compute_log_change(g))\n",
    ")\n",
    "\n",
    "cashflow_log_change = (\n",
    "    cashflow_sorted.groupby(\"symbol\", group_keys=False)\n",
    "    .apply(lambda g: compute_log_change(g))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "130f17b0-b137-4386-b95c-58b08ed4a839",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income 0/NaN counts per YoY column:\n",
      "                                                zeros  nans\n",
      "revenue_logchg                                      0     0\n",
      "grossProfit_logchg                                  0     0\n",
      "costAndExpenses_logchg                              0     0\n",
      "ebitda_logchg                                       0     0\n",
      "ebitdaratio_logchg                                  0     0\n",
      "operatingIncome_logchg                              0     0\n",
      "operatingIncomeRatio_logchg                         0     0\n",
      "incomeBeforeTax_logchg                              0     0\n",
      "incomeBeforeTaxRatio_logchg                         0     0\n",
      "netIncomeRatio_logchg                               0     0\n",
      "epsdiluted_logchg                                   1     0\n",
      "operatingExpenses_logchg                            4     0\n",
      "depreciationAndAmortization_logchg                  5     0\n",
      "costOfRevenue_logchg                                7     0\n",
      "incomeTaxExpense_logchg                             7     0\n",
      "weightedAverageShsOut_logchg                        7     0\n",
      "grossProfitRatio_logchg                            10     0\n",
      "weightedAverageShsOutDil_logchg                    10     0\n",
      "totalOtherIncomeExpensesNet_logchg                 32     0\n",
      "sellingGeneralAndAdministrativeExpenses_logchg     34     0\n",
      "interestExpense_logchg                             35     0\n",
      "otherExpenses_logchg                              137     0\n",
      "interestIncome_logchg                             169     0\n",
      "generalAndAdministrativeExpenses_logchg           248     0\n",
      "researchAndDevelopmentExpenses_logchg             295     0\n",
      "sellingAndMarketingExpenses_logchg                349     0\n",
      "\n",
      "Balance 0/NaN counts per YoY column:\n",
      "                                                zeros  nans\n",
      "totalAssets_logchg                                  0     0\n",
      "totalLiabilities_logchg                             0     0\n",
      "totalStockholdersEquity_logchg                      0     0\n",
      "totalEquity_logchg                                  0     0\n",
      "totalLiabilitiesAndStockholdersEquity_logchg        0     0\n",
      "totalLiabilitiesAndTotalEquity_logchg               0     0\n",
      "cashAndCashEquivalents_logchg                       1     0\n",
      "cashAndShortTermInvestments_logchg                  1     0\n",
      "totalNonCurrentAssets_logchg                        1     0\n",
      "totalCurrentAssets_logchg                           2     0\n",
      "otherNonCurrentAssets_logchg                        2     0\n",
      "longTermDebt_logchg                                 2     0\n",
      "totalNonCurrentLiabilities_logchg                   2     0\n",
      "totalDebt_logchg                                    2     0\n",
      "netDebt_logchg                                      2     0\n",
      "retainedEarnings_logchg                             3     0\n",
      "totalCurrentLiabilities_logchg                      6     0\n",
      "otherNonCurrentLiabilities_logchg                   6     0\n",
      "netReceivables_logchg                               8     0\n",
      "propertyPlantEquipmentNet_logchg                   10     0\n",
      "otherCurrentLiabilities_logchg                     10     0\n",
      "accumulatedOtherComprehensiveIncomeLoss_logchg     16     0\n",
      "accountPayables_logchg                             28     0\n",
      "otherCurrentAssets_logchg                          33     0\n",
      "shortTermDebt_logchg                               41     0\n",
      "othertotalStockholdersEquity_logchg                47     0\n",
      "goodwillAndIntangibleAssets_logchg                 60     0\n",
      "intangibleAssets_logchg                            91     0\n",
      "totalInvestments_logchg                            95     0\n",
      "capitalLeaseObligations_logchg                    100     0\n",
      "goodwill_logchg                                   125     0\n",
      "longTermInvestments_logchg                        150     0\n",
      "deferredTaxLiabilitiesNonCurrent_logchg           162     0\n",
      "inventory_logchg                                  166     0\n",
      "taxPayables_logchg                                172     0\n",
      "deferredRevenue_logchg                            229     0\n",
      "minorityInterest_logchg                           229     0\n",
      "taxAssets_logchg                                  249     0\n",
      "shortTermInvestments_logchg                       274     0\n",
      "commonStock_logchg                                280     0\n",
      "deferredRevenueNonCurrent_logchg                  372     0\n",
      "otherAssets_logchg                                469     0\n",
      "preferredStock_logchg                             471     0\n",
      "otherLiabilities_logchg                           486     0\n",
      "\n",
      "Cashflow 0/NaN counts per YoY column:\n",
      "                                                 zeros  nans\n",
      "netCashProvidedByOperatingActivities_logchg          0     0\n",
      "netCashUsedForInvestingActivites_logchg              0     0\n",
      "netCashUsedProvidedByFinancingActivities_logchg      0     0\n",
      "cashAtBeginningOfPeriod_logchg                       0     0\n",
      "operatingCashFlow_logchg                             0     0\n",
      "freeCashFlow_logchg                                  0     0\n",
      "netIncome_logchg                                     1     0\n",
      "cashAtEndOfPeriod_logchg                             1     0\n",
      "netChangeInCash_logchg                               2     0\n",
      "changeInWorkingCapital_logchg                        3     0\n",
      "otherNonCashItems_logchg                             4     0\n",
      "depreciationAndAmortization_logchg                   6     0\n",
      "otherWorkingCapital_logchg                           6     0\n",
      "otherFinancingActivites_logchg                      12     0\n",
      "capitalExpenditure_logchg                           24     0\n",
      "investmentsInPropertyPlantAndEquipment_logchg       26     0\n",
      "debtRepayment_logchg                                35     0\n",
      "otherInvestingActivites_logchg                      36     0\n",
      "accountsReceivables_logchg                          55     0\n",
      "deferredIncomeTax_logchg                            65     0\n",
      "accountsPayables_logchg                             77     0\n",
      "commonStockRepurchased_logchg                       77     0\n",
      "stockBasedCompensation_logchg                       87     0\n",
      "dividendsPaid_logchg                                96     0\n",
      "acquisitionsNet_logchg                             116     0\n",
      "effectOfForexChangesOnCash_logchg                  137     0\n",
      "salesMaturitiesOfInvestments_logchg                162     0\n",
      "inventory_logchg                                   164     0\n",
      "purchasesOfInvestments_logchg                      171     0\n",
      "commonStockIssued_logchg                           282     0\n"
     ]
    }
   ],
   "source": [
    "def count_zeros_nans_logchg(df):\n",
    "    # Keep only numeric columns that end with \"_logchg\"\n",
    "    numeric_cols = [c for c in df.select_dtypes(include=[float, int]).columns if c.endswith(\"_logchg\")]\n",
    "    \n",
    "    # Count zeros and NaNs\n",
    "    zero_counts = (df[numeric_cols] == 0).sum()\n",
    "    nan_counts = df[numeric_cols].isna().sum()\n",
    "    \n",
    "    # Combine into a single DataFrame\n",
    "    summary = pd.DataFrame({\n",
    "        \"zeros\": zero_counts,\n",
    "        \"nans\": nan_counts\n",
    "    }).sort_values(by=[\"zeros\", \"nans\"], ascending=True)\n",
    "    \n",
    "    # Force full display\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(summary)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "print(\"Income 0/NaN counts per YoY column:\")\n",
    "income_summary = count_zeros_nans_logchg(income_log_change)\n",
    "\n",
    "print(\"\\nBalance 0/NaN counts per YoY column:\")\n",
    "balance_summary = count_zeros_nans_logchg(balance_log_change)\n",
    "\n",
    "print(\"\\nCashflow 0/NaN counts per YoY column:\")\n",
    "cashflow_summary = count_zeros_nans_logchg(cashflow_log_change)\n",
    "\n",
    "# the purpose here is to make it easy to identify which line items (columns) are fully filled out from our sample so that we are only grabbing columns (features)\n",
    "# that are likely to be filled out by the stock under consideration, cause ultimately after we find a regression that has explanatory power... we can still only apply it \n",
    "# to the stock under consideration if it has the same line items filled out \n",
    "# the 503 nans is a result of the .pct change method that we used which creates a nan on every other row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04bb7dc9-acca-4d01-a334-c4898a4a747d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income_post_nans: columns =  shape = (503, 29)\n",
      "balance_post_nans: columns =  shape = (503, 47)\n",
      "cashflow_post_nans: columns =  shape = (503, 33)\n",
      "pe_data_sorted: columns =  shape = (480, 8)\n"
     ]
    }
   ],
   "source": [
    "def add_symbol_date(df_dict):\n",
    "    \"\"\"\n",
    "    Adds a 'symbol_date' column to each DataFrame in df_dict.\n",
    "    Prints the columns and shape of each updated DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df_dict : dict\n",
    "        Dictionary of DataFrames keyed by variable names (strings)\n",
    "    \n",
    "    Returns:\n",
    "    None (updates DataFrames in place)\n",
    "    \"\"\"\n",
    "    for name, df in df_dict.items():\n",
    "        df[\"symbol_date\"] = list(zip(df[\"symbol\"], df[\"date\"]))\n",
    "        print(f\"{name}: columns =  shape = {df.shape}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "dfs = {\n",
    "    \"income_post_nans\": income_log_change,\n",
    "    \"balance_post_nans\": balance_log_change,\n",
    "    \"cashflow_post_nans\": cashflow_log_change,\n",
    "    \"pe_data_sorted\": pe_data_sorted\n",
    "}\n",
    "\n",
    "add_symbol_date(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "604b96a7-2d78-40a1-a8d8-5de193100403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_pairs = (\n",
    "    set(income_log_change[\"symbol_date\"])\n",
    "    & set(balance_log_change[\"symbol_date\"])\n",
    "    & set(cashflow_log_change[\"symbol_date\"])\n",
    "    & set(pe_data_sorted[\"symbol_date\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1edabd0-44ff-44c2-a835-1a878a87bb28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(479, 29)\n",
      "(479, 47)\n",
      "(479, 33)\n",
      "(479, 8)\n"
     ]
    }
   ],
   "source": [
    "def filter_by_common_pairs(df, common_pairs):\n",
    "    \"\"\"\n",
    "    Keep only rows where the 'symbol_date' is in common_pairs.\n",
    "    Returns a new DataFrame.\n",
    "    \"\"\"\n",
    "    return df[df[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "\n",
    "income_post_nans_overlapped = filter_by_common_pairs(income_log_change, common_pairs)\n",
    "balance_post_nans_overlapped = filter_by_common_pairs(balance_log_change, common_pairs)\n",
    "cashflow_post_nans_overlapped = filter_by_common_pairs(cashflow_log_change, common_pairs)\n",
    "pe_post_nans_overlapped = filter_by_common_pairs(pe_data_sorted, common_pairs)\n",
    "\n",
    "print(income_post_nans_overlapped.shape)\n",
    "print(balance_post_nans_overlapped.shape)\n",
    "print(cashflow_post_nans_overlapped.shape)\n",
    "print(pe_post_nans_overlapped.shape)\n",
    "\n",
    "# we only want to include a ticker if we have all the financial statement items for all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ff5cc-0343-463b-a89d-32c3f8b744a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Sort consistently\n",
    "income_post_nans_overlapped = income_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "balance_post_nans_overlapped = balance_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "cashflow_post_nans_overlapped = cashflow_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "pe_post_nans_overlapped = pe_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Cleaned and aligned shapes:\", \n",
    "    income_post_nans_overlapped.shape,   \n",
    "    balance_post_nans_overlapped.shape, \n",
    "    cashflow_post_nans_overlapped.shape,\n",
    "    pe_post_nans_overlapped.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175a1b1-dae1-4a4b-9378-700529b7646c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_alignment(dfs: dict):\n",
    "    \"\"\"\n",
    "    Check alignment of multiple DataFrames on 'symbol' and 'date'.\n",
    "    Expects each DataFrame to have the same row order and columns: 'symbol', 'date'.\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    None (prints summary of mismatches)\n",
    "    \"\"\"\n",
    "    # Ensure equal lengths\n",
    "    lengths = {name: len(df) for name, df in dfs.items()}\n",
    "    if len(set(lengths.values())) > 1:\n",
    "        print(\"‚ö†Ô∏è Row counts differ between DataFrames:\")\n",
    "        for name, length in lengths.items():\n",
    "            print(f\"  {name}: {length} rows\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All DataFrames have {list(lengths.values())[0]} rows\")\n",
    "\n",
    "    # Concatenate for comparison\n",
    "    merged = pd.concat(\n",
    "        {name: df[[\"symbol\", \"date\"]].reset_index(drop=True) for name, df in dfs.items()},\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Compare across DataFrames\n",
    "    base = list(dfs.keys())[0]  # pick first as reference\n",
    "    symbol_mismatches = 0\n",
    "    date_mismatches = 0\n",
    "\n",
    "    for i in range(len(dfs[base])):\n",
    "        symbols = [merged[(name, \"symbol\")][i] for name in dfs.keys()]\n",
    "        dates   = [merged[(name, \"date\")][i]   for name in dfs.keys()]\n",
    "        if len(set(symbols)) > 1:\n",
    "            symbol_mismatches += 1\n",
    "        if len(set(dates)) > 1:\n",
    "            date_mismatches += 1\n",
    "\n",
    "    print(f\"Symbol mismatches: {symbol_mismatches}\")\n",
    "    print(f\"Date mismatches:   {date_mismatches}\")\n",
    "\n",
    "    \n",
    "check_alignment({\n",
    "    \"income\": income_post_nans_overlapped,\n",
    "    \"balance\": balance_post_nans_overlapped,\n",
    "    \"cashflow\": cashflow_post_nans_overlapped,\n",
    "    \"pe\": pe_post_nans_overlapped\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2766c-a364-4fce-8953-7b5dba753584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def outlier_check_1(df, title):\n",
    "    # Flatten all numeric columns into one long vector\n",
    "    numeric_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns\n",
    "    values = df[numeric_cols].values.flatten()\n",
    "    values = values[~np.isnan(values)]  # drop NaNs\n",
    "\n",
    "    # Scatter vs. index, colored by density\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(values, fill=True, color=\"lightblue\", alpha=0.3, linewidth=0)  # background density\n",
    "    plt.scatter(range(len(values)), values, \n",
    "                c=values, cmap=\"viridis\", s=5, alpha=0.6)\n",
    "\n",
    "    plt.title(f\"Density Scatterplot: {title}\", fontsize=14)\n",
    "    plt.xlabel(\"Index\", fontsize=12)\n",
    "    plt.ylabel(\"Value\", fontsize=12)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Call for each of your cleaned DataFrames\n",
    "outlier_check_1(income_log_change, \"Income\")\n",
    "outlier_check_1(balance_log_change, \"Balance\")\n",
    "outlier_check_1(cashflow_log_change, \"Cashflow\")\n",
    "\n",
    "#demonstrates the necessity for addressing outlier concerns \n",
    "# ONLY PURPOSE OF. THIS IS TO CONFIRM WE HAVE SERIOUS OUTLIERS THAT NEED TO BE ADDRESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f80e4-afca-467c-81f9-cbbd35602332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_high_zero_columns(df, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Drops columns with more than `threshold` proportion of zero values,\n",
    "    and reports how many columns were dropped.\n",
    "    \"\"\"\n",
    "    original_col_count = df.shape[1]\n",
    "\n",
    "    zero_proportion = (df == 0).sum() / len(df)\n",
    "    cols_to_drop = zero_proportion[zero_proportion > threshold].index\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    new_col_count = df.shape[1]\n",
    "\n",
    "    print(f\"Columns before: {original_col_count}\")\n",
    "    print(f\"Columns dropped: {len(cols_to_drop)} (>{threshold*100:.1f}% zeros)\")\n",
    "    print(f\"Columns after: {new_col_count}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81348b74-38eb-4ab3-b371-afa3c03389ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_after_feature_drop = drop_high_zero_columns(income_log_change, threshold=0.05)\n",
    "balance_after_feature_drop = drop_high_zero_columns(balance_log_change, threshold=0.05)\n",
    "cashflow_after_feature_drop = drop_high_zero_columns(cashflow_log_change, threshold=0.05)\n",
    "\n",
    "# Call for each of your cleaned DataFrames\n",
    "outlier_check_1(income_after_feature_drop, \"Income\")\n",
    "outlier_check_1(balance_after_feature_drop, \"Balance\")\n",
    "outlier_check_1(cashflow_after_feature_drop, \"Cashflow\")\n",
    "\n",
    "print(income_after_feature_drop.shape)\n",
    "print(balance_after_feature_drop.shape)\n",
    "print(cashflow_after_feature_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77731b1-1937-4252-9909-9bd8bbca7204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_outlier_rows(df, threshold=10):\n",
    "    \"\"\"\n",
    "    Drops rows where ANY numeric column has a value greater than `threshold`\n",
    "    or less than `-threshold`. Prints how many rows remain afterward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to filter.\n",
    "    threshold : float\n",
    "        Absolute cutoff for detecting outliers (e.g., 10 retains rows within [-10, 10]).\n",
    "    \"\"\"\n",
    "    original_row_count = df.shape[0]\n",
    "\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "    # Create mask: keep rows where all numeric values are within [-threshold, threshold]\n",
    "    mask = (df[numeric_cols].abs() <= threshold).all(axis=1)\n",
    "    df_filtered = df[mask].copy()\n",
    "\n",
    "    new_row_count = df_filtered.shape[0]\n",
    "    dropped_rows = original_row_count - new_row_count\n",
    "\n",
    "    print(f\"Rows before: {original_row_count}\")\n",
    "    print(f\"Rows dropped: {dropped_rows} (>|{threshold}| in any numeric column)\")\n",
    "    print(f\"Rows after: {new_row_count}\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# --- Drop columns with too many zeros ---\n",
    "income_after_feature_drop   = drop_high_zero_columns(income_log_change, threshold=0.05)\n",
    "balance_after_feature_drop  = drop_high_zero_columns(balance_log_change, threshold=0.05)\n",
    "cashflow_after_feature_drop = drop_high_zero_columns(cashflow_log_change, threshold=0.05)\n",
    "\n",
    "# --- Drop rows with extreme outliers (values beyond ¬±10) ---\n",
    "income_after_outlier_drop   = drop_outlier_rows(income_after_feature_drop, threshold=10)\n",
    "balance_after_outlier_drop  = drop_outlier_rows(balance_after_feature_drop, threshold=10)\n",
    "cashflow_after_outlier_drop = drop_outlier_rows(cashflow_after_feature_drop, threshold=10)\n",
    "\n",
    "\n",
    "print(income_after_outlier_drop.shape)\n",
    "print(balance_after_outlier_drop.shape)\n",
    "print(cashflow_after_outlier_drop.shape)\n",
    "pe_data_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9800a-6272-4ce7-a6c3-dda6f1fa9c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### YOU MUST REALIGN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204d1d8-430d-43bf-b2f3-8ecf091021aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_dfs_on_symbol_date(dfs,\n",
    "                             symbol_col=\"symbol\",\n",
    "                             date_col=\"date\",\n",
    "                             key_col=\"symbol_date\",\n",
    "                             drop_duplicates=True,\n",
    "                             verbose=True):\n",
    "    \"\"\"\n",
    "    Align multiple DataFrames on the intersection of (symbol, date) pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : dict or list\n",
    "        If dict: mapping name -> DataFrame. If list: list of DataFrames (names will be auto-generated).\n",
    "    symbol_col : str\n",
    "        Column name holding the ticker/symbol.\n",
    "    date_col : str\n",
    "        Column name holding the date.\n",
    "    key_col : str\n",
    "        Name to use for the temporary composite key column (string).\n",
    "    drop_duplicates : bool\n",
    "        If True, drop duplicate key values in each DataFrame (keep first).\n",
    "    verbose : bool\n",
    "        Print progress and shapes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aligned : dict\n",
    "        Mapping name -> aligned DataFrame (index set to key_col, sorted).\n",
    "    common_keys : list\n",
    "        Sorted list of common key strings present in every DataFrame.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError if no common keys are found or if required columns are missing.\n",
    "    \"\"\"\n",
    "    # Normalize input to a dict\n",
    "    if isinstance(dfs, dict):\n",
    "        df_map = dict(dfs)\n",
    "    else:\n",
    "        df_map = {f\"df_{i}\": df for i, df in enumerate(dfs, 1)}\n",
    "\n",
    "    # Ensure required columns exist and build standardized key strings\n",
    "    for name, df in df_map.items():\n",
    "        if symbol_col not in df.columns or date_col not in df.columns:\n",
    "            raise ValueError(f\"{name} is missing required columns '{symbol_col}' or '{date_col}'\")\n",
    "        # Create a reproducible string key: \"<symbol>_<date_str>\"\n",
    "        # Use astype(str) to avoid tuple/dtype mismatch issues.\n",
    "        df_map[name] = df.copy()\n",
    "        df_map[name][key_col] = df_map[name][symbol_col].astype(str) + \"_\" + df_map[name][date_col].astype(str)\n",
    "        if drop_duplicates:\n",
    "            dup_count = df_map[name][key_col].duplicated().sum()\n",
    "            if dup_count and verbose:\n",
    "                print(f\"{name}: dropping {dup_count} duplicate {key_col} rows (keeping first)\")\n",
    "            df_map[name] = df_map[name][~df_map[name][key_col].duplicated(keep=\"first\")]\n",
    "\n",
    "    # Compute intersection of keys\n",
    "    key_sets = [set(df[key_col]) for df in df_map.values()]\n",
    "    common = set.intersection(*key_sets) if key_sets else set()\n",
    "\n",
    "    if not common:\n",
    "        raise ValueError(\"No common symbol_date keys found across all DataFrames.\")\n",
    "\n",
    "    common_sorted = sorted(common)  # stable order\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Common observation count:\", len(common_sorted))\n",
    "        for name, df in df_map.items():\n",
    "            print(f\"  {name}: before={df.shape[0]}  after_intersection={df[df[key_col].isin(common)].shape[0]}\")\n",
    "\n",
    "    # Filter each df to the common keys, set index, and sort for perfect alignment\n",
    "    aligned = {}\n",
    "    for name, df in df_map.items():\n",
    "        df_f = df[df[key_col].isin(common)].copy()\n",
    "        df_f = df_f.set_index(key_col).loc[common_sorted]   # preserve the same order for all\n",
    "        aligned[name] = df_f\n",
    "\n",
    "    # Final sanity check: all indexes should be equal\n",
    "    indexes = [df.index for df in aligned.values()]\n",
    "    first_index = indexes[0]\n",
    "    if not all(idx.equals(first_index) for idx in indexes[1:]):\n",
    "        raise RuntimeError(\"Failed to align indexes exactly ‚Äî unexpected mismatch.\")\n",
    "\n",
    "    return aligned, common_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7c0f5-c2ea-4ff1-b27b-25bc0cb94570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    \"income\": income_after_outlier_drop,\n",
    "    \"balance\": balance_after_outlier_drop,\n",
    "    \"cashflow\": cashflow_after_outlier_drop,\n",
    "    \"pe\": pe_data_sorted\n",
    "}\n",
    "\n",
    "aligned_dict, common_keys = align_dfs_on_symbol_date(dfs,\n",
    "                                                     symbol_col=\"symbol\",\n",
    "                                                     date_col=\"date\",\n",
    "                                                     key_col=\"symbol_date\",\n",
    "                                                     drop_duplicates=True,\n",
    "                                                     verbose=True)\n",
    "\n",
    "# Access aligned frames:\n",
    "income_aligned = aligned_dict[\"income\"]\n",
    "balance_aligned = aligned_dict[\"balance\"]\n",
    "cashflow_aligned = aligned_dict[\"cashflow\"]\n",
    "pe_aligned = aligned_dict[\"pe\"]\n",
    "\n",
    "# Confirm shapes are identical:\n",
    "print(income_aligned.shape, balance_aligned.shape, cashflow_aligned.shape, pe_aligned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7067c-0a6b-4032-a6b1-249ac6e49ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def run_statement_univariate(statement_df, pe_data_sorted, label=\"\", plot=False):\n",
    "    merged = statement_df.merge(pe_data_sorted, on=\"symbol\", how=\"inner\")\n",
    "    X = merged.select_dtypes(include=[\"number\"]).drop(columns=[\"log_PE\"])\n",
    "    y = merged[\"log_PE\"]\n",
    "\n",
    "    results = []\n",
    "    for col in X.columns:\n",
    "        X_var = sm.add_constant(X[col])\n",
    "        model = sm.OLS(y, X_var, missing=\"drop\").fit()\n",
    "        residuals = model.resid\n",
    "        fitted = model.fittedvalues\n",
    "\n",
    "        # --- Tests ---\n",
    "        bp_test = het_breuschpagan(residuals, X_var)\n",
    "        bp_pvalue = bp_test[1]  # p-value for heteroskedasticity\n",
    "        shapiro_pvalue = shapiro(residuals)[1] if len(residuals) < 5000 else None  # Shapiro limited to <5000 obs\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": col,\n",
    "            \"coef\": model.params[col],\n",
    "            \"t_value\": model.tvalues[col],\n",
    "            \"p_value\": model.pvalues[col],\n",
    "            \"bp_pvalue\": bp_pvalue,\n",
    "            \"r2\":      model.rsquared,\n",
    "            \"shapiro_pvalue\": shapiro_pvalue,\n",
    "            \"reject_null\": model.pvalues[col]< 0.05\n",
    "        })\n",
    "\n",
    "        # --- Optional diagnostic plots ---\n",
    "        if plot:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            fig.suptitle(f\"{label}: Residual Diagnostics for {col}\", fontsize=12)\n",
    "\n",
    "            sns.scatterplot(x=fitted, y=residuals, alpha=0.6, ax=axes[0])\n",
    "            axes[0].axhline(0, color=\"red\", linestyle=\"--\", lw=1)\n",
    "            axes[0].set_xlabel(\"Fitted Values\")\n",
    "            axes[0].set_ylabel(\"Residuals\")\n",
    "            axes[0].set_title(\"Residuals vs Fitted\")\n",
    "\n",
    "            sns.histplot(residuals, kde=True, ax=axes[1], color=\"teal\")\n",
    "            axes[1].set_title(\"Residual Distribution\")\n",
    "            axes[1].set_xlabel(\"Residuals\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"p_value\")\n",
    "\n",
    "    return merged, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69492bc9-8225-473d-8ef3-59497cb5e81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run diagnostics separately for each statement\n",
    "merged_income, uni_results_income = run_statement_univariate(income_aligned, pe_aligned, label=\"Income\", plot=False)\n",
    "merged_balance, uni_results_balance = run_statement_univariate(balance_aligned, pe_aligned, label=\"Balance\", plot=False)\n",
    "merged_cashflow, uni_results_cashflow = run_statement_univariate(cashflow_aligned, pe_aligned, label=\"Cashflow\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d5309-f729-40a3-b35c-2f227874e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter problematic features for each statement\n",
    "bad_income = uni_results_income[\n",
    "    (uni_results_income[\"bp_pvalue\"] < 0.05) | (uni_results_income[\"shapiro_pvalue\"] < 0.05)\n",
    "]\n",
    "\n",
    "bad_balance = uni_results_balance[\n",
    "    (uni_results_balance[\"bp_pvalue\"] < 0.05) | (uni_results_balance[\"shapiro_pvalue\"] < 0.05)\n",
    "]\n",
    "\n",
    "bad_cashflow = uni_results_cashflow[\n",
    "    (uni_results_cashflow[\"bp_pvalue\"] < 0.05) | (uni_results_cashflow[\"shapiro_pvalue\"] < 0.05)\n",
    "]\n",
    "\n",
    "# Print results separately\n",
    "print(\"\\nüö® Problematic Income Features:\")\n",
    "print(bad_income[[\"feature\", \"bp_pvalue\", \"shapiro_pvalue\"]])\n",
    "\n",
    "print(\"\\nüö® Problematic Balance Features:\")\n",
    "print(bad_balance[[\"feature\", \"bp_pvalue\", \"shapiro_pvalue\"]])\n",
    "\n",
    "print(\"\\nüö® Problematic Cashflow Features:\")\n",
    "print(bad_cashflow[[\"feature\", \"bp_pvalue\", \"shapiro_pvalue\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e811299-ba72-48c4-9b50-fc90c8d29308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine for convenience, but we‚Äôll split by statement type\n",
    "plot_df = pd.concat([\n",
    "    uni_results_income.assign(statement_type='Income'),\n",
    "    uni_results_balance.assign(statement_type='Balance'),\n",
    "    uni_results_cashflow.assign(statement_type='Cashflow')\n",
    "], ignore_index=True)\n",
    "\n",
    "statement_order = ['Income', 'Balance', 'Cashflow']\n",
    "n_statements = len(statement_order)\n",
    "\n",
    "# Create subplots (one per statement type)\n",
    "fig, axes = plt.subplots(n_statements, 1, figsize=(12, 5*n_statements), sharex=True)\n",
    "\n",
    "for i, statement in enumerate(statement_order):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Subset for this statement\n",
    "    df = plot_df[plot_df['statement_type'] == statement].copy()\n",
    "    \n",
    "    # Sort by absolute t-value descending\n",
    "    df = df.sort_values('t_value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Normalize r¬≤ for color mapping\n",
    "    norm = mpl.colors.Normalize(vmin=df['r2'].min(), vmax=df['r2'].max())\n",
    "    cmap = mpl.cm.viridis\n",
    "    colors = [cmap(norm(val)) for val in df['r2']]\n",
    "    \n",
    "    # Y positions\n",
    "    y_pos = range(len(df))\n",
    "    \n",
    "    # Horizontal bar chart\n",
    "    ax.barh(y=y_pos, width=df['t_value'], color=colors, edgecolor='black')\n",
    "    \n",
    "    # Highlight significant features\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if row.reject_null:\n",
    "            ax.text(\n",
    "                x=row.t_value + (0.05 if row.t_value>0 else -0.05),\n",
    "                y=idx,\n",
    "                s='*',\n",
    "                va='center',\n",
    "                ha='left' if row.t_value>0 else 'right',\n",
    "                color='red',\n",
    "                fontsize=12\n",
    "            )\n",
    "    \n",
    "    # Y-axis labels\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(df['feature'], fontsize=10)\n",
    "    ax.axvline(0, color='black', linewidth=0.8)\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_title(f'{statement} Statement: t-values by Feature (Color = R¬≤)')\n",
    "\n",
    "# Shared x-label\n",
    "axes[-1].set_xlabel('t-value')\n",
    "\n",
    "# Add a single colorbar\n",
    "scalar_map = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "scalar_map.set_array([])\n",
    "cbar = fig.colorbar(scalar_map, ax=axes, orientation='vertical', fraction=0.02, pad=0.01)\n",
    "cbar.set_label('R¬≤')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad612d8e-3a7f-4d6d-be69-32f8e60502cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_significant_features(statement_df, uni_results, \n",
    "                                t_threshold=1.2, r2_threshold=0.005, p_threshold=0.2, \n",
    "                                label=\"\"):\n",
    "    \"\"\"\n",
    "    Select significant features from a financial statement dataframe\n",
    "    based on univariate regression results. \n",
    "    Returns only the subset of statement_df with those features.\n",
    "    \"\"\"\n",
    "    # Filter by thresholds\n",
    "    signif = uni_results[\n",
    "        (uni_results[\"t_value\"].abs() > t_threshold) &\n",
    "        (uni_results[\"r2\"] > r2_threshold) &\n",
    "        (uni_results[\"p_value\"] < p_threshold)\n",
    "    ]\n",
    "\n",
    "    features = signif[\"feature\"].tolist()\n",
    "    \n",
    "    # Only use statement data ‚Äî no merge\n",
    "    selected = statement_df[[\"symbol\", \"date\", \"symbol_date\"] + features]\n",
    "\n",
    "    if label:\n",
    "        total_features = len(uni_results)\n",
    "        n_selected = len(features)\n",
    "        pct = (n_selected / total_features * 100) if total_features > 0 else 0\n",
    "        print(f\"{label.title()}: {n_selected}/{total_features} features selected ({pct:.1f}%)\")\n",
    "\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55240f42-237c-4b74-bdfb-41c04c2dfe1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('eps' in df.columns)\n",
    "print(income_after_outlier_drop.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfae16f-bfed-4756-9364-8879d1852e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uni_results_income = uni_results_income[uni_results_income['feature'] != 'eps']\n",
    "uni_results_balance = uni_results_balance[uni_results_balance['feature'] != 'eps']\n",
    "uni_results_cashflow = uni_results_cashflow[uni_results_cashflow['feature'] != 'eps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbab16b-dc57-4faf-9aa3-4d5fa629abbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select significant features from each statement type\n",
    "selected_income = select_significant_features(\n",
    "    income_after_outlier_drop, uni_results_income, label=\"Income\"\n",
    ")\n",
    "\n",
    "selected_balance = select_significant_features(\n",
    "    balance_after_outlier_drop, uni_results_balance, label=\"Balance\"\n",
    ")\n",
    "\n",
    "selected_cashflow = select_significant_features(\n",
    "    cashflow_after_outlier_drop, uni_results_cashflow, label=\"Cashflow\"\n",
    ")\n",
    "\n",
    "print(income_after_outlier_drop.shape)\n",
    "print(balance_after_outlier_drop.shape)\n",
    "print(cashflow_after_outlier_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a200b7-5e79-42bc-b715-a57e94d502ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pca(df, n_components=None, prefix=\"PC\"):\n",
    "    \"\"\"\n",
    "    Run PCA on the numeric feature columns of a dataframe.\n",
    "    Returns a separate dataframe containing only the PCA components,\n",
    "    plus 'symbol' and 'date' as identifiers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with ['symbol', 'date', 'symbol_date'] and features.\n",
    "    n_components : int or None\n",
    "        Number of PCA components. If None, use all numeric features.\n",
    "    prefix : str\n",
    "        Prefix for PCA component columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pca_components_df : pd.DataFrame\n",
    "        Dataframe with symbol, date, and PCA components.\n",
    "    pca_model : PCA object\n",
    "        Fitted PCA object.\n",
    "    \"\"\"\n",
    "    id_cols = [\"symbol\", \"date\", \"symbol_date\"]\n",
    "    feature_cols = [c for c in df.columns if c not in id_cols]\n",
    "\n",
    "    X = df[feature_cols].fillna(0)\n",
    "\n",
    "    if n_components is None:\n",
    "        n_components = min(len(feature_cols), len(df))\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = pca.fit_transform(X)\n",
    "\n",
    "    comp_cols = [f\"{prefix}{i+1}\" for i in range(n_components)]\n",
    "    pca_df = pd.DataFrame(components, columns=comp_cols, index=df.index)\n",
    "\n",
    "    # Return only symbol/date/symbol_date and components\n",
    "    pca_components_df = pd.concat([df[id_cols], pca_df], axis=1)\n",
    "\n",
    "    return pca_components_df, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fcdd7-57d2-4bb2-bf2a-f3c0355ae096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Income statement PCA\n",
    "income_df_pca, income_pca_model = run_pca(\n",
    "    selected_income,        # Pass full DataFrame; ID columns are preserved\n",
    "    n_components=3,\n",
    "    prefix=\"income_\"\n",
    ")\n",
    "\n",
    "# Balance sheet PCA\n",
    "balance_df_pca, balance_pca_model = run_pca(\n",
    "    selected_balance,\n",
    "    n_components=3,\n",
    "    prefix=\"balance_\"\n",
    ")\n",
    "\n",
    "# Cash flow PCA\n",
    "cashflow_df_pca, cashflow_pca_model = run_pca(\n",
    "    selected_cashflow,\n",
    "    n_components=3,\n",
    "    prefix=\"cashflow_\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8f02d-314a-40c7-8bc7-91d8559d0233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "balance_df_pca.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21b6c7-c1f9-4852-ab71-01d31f700497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(income_df_pca.columns)\n",
    "print(balance_df_pca.columns)\n",
    "print(cashflow_df_pca.columns)\n",
    "\n",
    "print(income_df_pca.shape)\n",
    "print(balance_df_pca.shape)\n",
    "print(cashflow_df_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4b15a-dc38-4987-98af-029cf9d16292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def regress_log_pe_on_pca(income_df_pca, balance_df_pca, cashflow_df_pca, pe_data_sorted, log_pe_col=\"log_PE\"):\n",
    "    \"\"\"\n",
    "    Run a multivariable linear regression using PCA components from income, balance, and cashflow dataframes\n",
    "    as independent variables, and the precomputed log_PE as the dependent variable.\n",
    "\n",
    "    Assumes all inputs already share the same index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    income_df_pca : pd.DataFrame\n",
    "        PCA components from the income statement.\n",
    "    balance_df_pca : pd.DataFrame\n",
    "        PCA components from the balance sheet.\n",
    "    cashflow_df_pca : pd.DataFrame\n",
    "        PCA components from the cash flow statement.\n",
    "    pe_data_sorted : pd.DataFrame\n",
    "        DataFrame containing a 'log_PE' column.\n",
    "    log_pe_col : str, default \"log_PE\"\n",
    "        Name of the logged PE column in `pe_data_sorted`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "        Fitted OLS model.\n",
    "    combined_df : pd.DataFrame\n",
    "        Full dataframe used in regression (includes all PCA components and log_PE).\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine PCA components and target variable\n",
    "    combined_df = pd.concat([income_df_pca, balance_df_pca, cashflow_df_pca, pe_data_sorted[[log_pe_col]]], axis=1)\n",
    "\n",
    "    # Split dependent and independent variables\n",
    "    y = combined_df[log_pe_col]\n",
    "    X = combined_df.drop(columns=[log_pe_col]).select_dtypes(include=[\"number\"])\n",
    "\n",
    "    # Add constant\n",
    "    X_const = sm.add_constant(X)\n",
    "\n",
    "    # Fit OLS regression\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "\n",
    "    print(model.summary())\n",
    "    return model, combined_df\n",
    "\n",
    "\n",
    "model, combined_df = regress_log_pe_on_pca(\n",
    "    income_df_pca,\n",
    "    balance_df_pca,\n",
    "    cashflow_df_pca,\n",
    "    pe_data_sorted,\n",
    "    log_pe_col=\"log_PE\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04f486-970a-4958-b919-0723c66180f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in [\n",
    "    (\"income_df_pca\", income_df_pca),\n",
    "    (\"balance_df_pca\", balance_df_pca),\n",
    "    (\"cashflow_df_pca\", cashflow_df_pca),\n",
    "    (\"pe_data_sorted\", pe_data_sorted)\n",
    "]:\n",
    "    if not df.index.is_unique:\n",
    "        print(f\"{name} has duplicate indexes!\")\n",
    "        print(df.index[df.index.duplicated()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e520b-c400-4ae3-b049-e86832c6657e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
