{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ed79e-112a-418f-b955-5c13dd46dd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import gaussian_kde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982057a-32e3-43e6-b761-6240815efa25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "\n",
    "# THIS pulls sector and subsector info either localy if it's cached or from the api\n",
    "\n",
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "\n",
    "# --- LOAD FROM CACHE OR FETCH ---\n",
    "if os.path.exists(tickers_csv_file):\n",
    "    print(\"Loading tickers from CSV cache...\")\n",
    "    df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "else:\n",
    "    print(\"Fetching tickers from API...\")\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/sp500_constituent?apikey={API_KEY}\"\n",
    "    df_sp500 = pd.DataFrame(requests.get(url).json())\n",
    "\n",
    "    # Save to CSV\n",
    "    df_sp500.to_csv(tickers_csv_file, index=False)\n",
    "    print(f\"Saved {len(df_sp500)} tickers to CSV cache.\")\n",
    "\n",
    "    \n",
    "# --- PREVIEW ---\n",
    "print(df_sp500.shape)\n",
    "print(df_sp500.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2a811-89dd-4a1d-b195-4f3af35f41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8a247-31e3-486c-a090-35f4860c348f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "API_KEY = \"YwnbHRjcJvf6Md2OPoKbSRGHlzZ7hjR6\"\n",
    "data_folder = os.path.join(os.path.expanduser(\"~/Desktop/Trading\"), \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "tickers_csv_file = os.path.join(data_folder, \"sp500_tickers.csv\")\n",
    "\n",
    "# Load tickers\n",
    "df_sp500 = pd.read_csv(tickers_csv_file)\n",
    "tickers = df_sp500[\"symbol\"].dropna().unique().tolist()\n",
    "\n",
    "# Output file\n",
    "output_file = os.path.join(data_folder, \"price_and_earnings.json\")\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "\n",
    "def fetch_price_and_earnings(tickers, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        return pd.DataFrame(json.load(open(output_file)))\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Get current price\n",
    "            quote_url = f\"https://financialmodelingprep.com/api/v3/quote/{ticker}?apikey={API_KEY}\"\n",
    "            price_data = requests.get(quote_url).json()\n",
    "            if not price_data:\n",
    "                continue\n",
    "            price = price_data[0][\"price\"]\n",
    "            price_date = price_data[0].get(\"date\")  # trading date\n",
    "\n",
    "            # Get latest annual income statement\n",
    "            income_url = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?limit=1&apikey={API_KEY}\"\n",
    "            income_data = requests.get(income_url).json()\n",
    "            if not income_data:\n",
    "                continue\n",
    "            eps = income_data[0].get(\"eps\")\n",
    "            net_income = income_data[0].get(\"netIncome\")\n",
    "            report_date = income_data[0].get(\"date\")  # fiscal period end date\n",
    "\n",
    "            records.append({\n",
    "                \"symbol\": ticker,\n",
    "                \"price\": price,\n",
    "                \"price_date\": price_date,\n",
    "                \"eps\": eps,\n",
    "                \"date\": report_date\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker}: {e}\")\n",
    "        time.sleep(0.2)  # polite rate limit\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "price_earnings_df = fetch_price_and_earnings(tickers, output_file)\n",
    "print(price_earnings_df.columns)\n",
    "print(price_earnings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb737c07-dc25-4514-858c-f5178907b5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CLEANS AND TAKES LOG OF THE PE DATA\n",
    "\n",
    "# Only keep rows with positive EPS\n",
    "pe_data = price_earnings_df[price_earnings_df[\"eps\"] > 0].copy()\n",
    "\n",
    "# Compute log(PE)\n",
    "pe_data[\"log_PE\"] = np.log(pe_data[\"price\"] / pe_data[\"eps\"])\n",
    "\n",
    "# Print row count for reference\n",
    "print(pe_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1844560d-77dc-4f2a-8993-96bb263662be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FETCHES INCOME STATEMENT AND BS STATEMENT AND CF STATEMENT\n",
    "def fetch_statement(endpoint, tickers, period, limit, data_folder):\n",
    "    \"\"\"Fetch statements with unique JSON filename based on endpoint, period, limit.\"\"\"\n",
    "    output_file = os.path.join(\n",
    "        data_folder,\n",
    "        f\"{endpoint}_{period}_limit{limit}.json\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Loading from cache: {output_file}\")\n",
    "        with open(output_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for ticker in tickers:\n",
    "        url = f\"https://financialmodelingprep.com/api/v3/{endpoint}/{ticker}?period={period}&limit={limit}&apikey={API_KEY}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                for row in data:\n",
    "                    row[\"symbol\"] = ticker\n",
    "                records.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {ticker} ({endpoint}): {e}\")\n",
    "        time.sleep(.2)  # API polite rate limit\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    print(f\"Saved {len(records)} records to {output_file}\")\n",
    "    # \"RECORDS HERE IS A LIST OF STRINGS NOT A DF\"\n",
    "    return records\n",
    "\n",
    "# AT THIS POINT INCOME_DATA_2_YEARS IS STILL A LIST OF STRINGS STORED AS A VAR\n",
    "income_data_2_years   = fetch_statement(\"income-statement\", tickers, \"annual\", 2, data_folder)\n",
    "balance_data_2_years  = fetch_statement(\"balance-sheet-statement\", tickers, \"annual\", 2, data_folder)\n",
    "cashflow_data_2_years = fetch_statement(\"cash-flow-statement\", tickers, \"annual\", 2, data_folder)\n",
    "\n",
    "# INCOME_DATA_2_YEARS IS CONVERTED TO A DF\n",
    "income_data_2_years   = pd.DataFrame(income_data_2_years)\n",
    "balance_data_2_years  = pd.DataFrame(balance_data_2_years)\n",
    "cashflow_data_2_years = pd.DataFrame(cashflow_data_2_years)\n",
    "\n",
    "print(\"Income shape:\", income_data_2_years.shape)\n",
    "print(\"Balance shape:\", balance_data_2_years.shape)\n",
    "print(\"Cash flow shape:\", cashflow_data_2_years.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031400b-6a30-4ff6-ab53-72963cfe5a96",
   "metadata": {},
   "source": [
    "# End of fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e321a26-3485-4841-aea9-0e069cd8dc23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_to_drop_income = [\n",
    "    \"netIncome\", \"netIncomeRatio\", \"epsdiluted\", \"eps\",\n",
    "    \"operatingIncomeRatio\", \"incomeBeforeTaxRatio\",\n",
    "    \"incomeBeforeTax\", \"operatingIncome\",\"ebitda\",\"ebitdaratio\", \"link\", \"finalLink\",\"fillingDate\",\"cik\",\"acceptedDate\",\"reportedCurrency\",\"period\",\"calendarYear\"]\n",
    "\n",
    "cols_to_drop_balance = [\n",
    "    \"eps\",\"retainedEarnings\", \"totalStockholdersEquity\",\"othertotalStockholdersEquity\", \"totalEquity\", \"link\", \"finalLink\",\"fillingDate\",\"cik\",\"acceptedDate\",\"reportedCurrency\",\"period\",\"calendarYear\"]\n",
    "\n",
    "cols_to_drop_cashflow = [\n",
    "    \"eps\", \"link\", \"finalLink\",\"fillingDate\",\"cik\",\"acceptedDate\",\"netIncome\",\"reportedCurrency\",\"period\",\"calendarYear\"]\n",
    "\n",
    "cols_to_drop_pe = [\"eps\",\"netIncome\"]\n",
    "\n",
    "income_data_2_years = income_data_2_years.drop(columns=cols_to_drop_income, errors=\"ignore\")\n",
    "balance_data_2_years = balance_data_2_years.drop(columns=cols_to_drop_balance, errors=\"ignore\")\n",
    "cashflow_data_2_years = cashflow_data_2_years.drop(columns=cols_to_drop_cashflow, errors=\"ignore\")\n",
    "pe_data = pe_data.drop(columns=cols_to_drop_pe, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c028481-e14a-4756-9818-384955c12ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(income_data_2_years.columns.tolist)\n",
    "print(balance_data_2_years.columns.tolist)\n",
    "print(cashflow_data_2_years.columns.tolist)\n",
    "print(pe_data.columns.tolist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b245b2-1b76-49e2-a37d-e3d08f3acc87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Clean up column names (remove spaces, tabs, and backslashes) ---\n",
    "for df_name, df in {\n",
    "    \"income_data_2_years\": income_data_2_years,\n",
    "    \"balance_data_2_years\": balance_data_2_years,\n",
    "    \"cashflow_data_2_years\": cashflow_data_2_years,\n",
    "    \"pe_data\": pe_data\n",
    "}.items():\n",
    "    df.columns = df.columns.str.strip().str.replace(r'\\\\', '', regex=True)\n",
    "    print(f\"{df_name}: cleaned column names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b213f63-6b7f-4f36-822d-5d301e65400d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_problematic_entries(df, label):\n",
    "    print(f\"\\nðŸ” Problematic value counts for {label} statement:\")\n",
    "    \n",
    "    def count_issues(col):\n",
    "        numeric_col = pd.to_numeric(col, errors='coerce')\n",
    "        return ((numeric_col.isna()) | (numeric_col == 0)).sum()\n",
    "\n",
    "    issue_counts = df.apply(count_issues)\n",
    "    filtered_counts = issue_counts[issue_counts > 0].sort_values()\n",
    "    print(filtered_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391db554-2c54-4798-8835-6b83f817c3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_problematic_entries(income_data_2_years, \"Income\")\n",
    "count_problematic_entries(balance_data_2_years, \"Balance Sheet\")\n",
    "count_problematic_entries(cashflow_data_2_years, \"Cash Flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9216e8e-cb6b-4db2-916f-8c3f5fd5e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_symbol_then_date(df):\n",
    "    # Sort ascending by symbol, then by date (oldest first)\n",
    "    return df.sort_values([\"symbol\", \"date\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "income_sorted = sort_by_symbol_then_date(income_data_2_years)\n",
    "balance_sorted = sort_by_symbol_then_date(balance_data_2_years)\n",
    "cashflow_sorted = sort_by_symbol_then_date(cashflow_data_2_years)\n",
    "pe_data_sorted = sort_by_symbol_then_date(pe_data)\n",
    "\n",
    "\n",
    "pe_data_sorted.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ff733-b5e3-462d-b9d4-28467fbe55c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DOUBLE CHECK THE ECISION TO ADD A CONSTANT... ELONGATING THE TAIL OF THE DISTRIBUTION\n",
    "# DATA PROVIDER PUTS \"0\" FOR ANY LINE ITEM THAT DOESN'T EXIST IN THE FINANCIAL STATEMENT... ADDING THE CONSTANT THEN RETURNS A SMALL VALUE WHICH IN TURN \n",
    "#ALLOWS FOR A HUGE LOG CHANGE OUTLIER IF THAT LINE ITEM IN THE NEXT FINANCIALS STATEMENT IS ANYHTING OTHER THAN 0?\n",
    "\n",
    "def compute_log_change(df, constant=1e-3, drop_first=True):\n",
    "    \"\"\"\n",
    "    Compute log-differences for year-over-year growth of financial statement items.\n",
    "    retains 'symbol' and 'date' columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must include 'symbol' and 'date' columns, sorted by both.\n",
    "    constant : float\n",
    "        Small stabilizing constant for log transform.\n",
    "    drop_first : bool\n",
    "        Whether to drop the first row per symbol (NaN after diff).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Numeric part\n",
    "    num_df = df.select_dtypes(include=[np.number])\n",
    "    log_df = np.log(np.abs(num_df) + constant)\n",
    "    log_diff = log_df.diff()\n",
    "\n",
    "    # Rename to show log-change\n",
    "    log_diff.columns = [f\"{col}_logchg\" for col in log_diff.columns]\n",
    "\n",
    "    # Combine with non-numeric columns\n",
    "    result = pd.concat([df[[\"symbol\", \"date\"]], log_diff], axis=1)\n",
    "\n",
    "    if drop_first:\n",
    "        # Drop the first row per symbol (which has NaN diffs)\n",
    "        result = result.groupby(\"symbol\", group_keys=False).apply(lambda g: g.iloc[1:])\n",
    "\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- Apply grouped by symbol ---\n",
    "income_log_change = (\n",
    "    income_sorted.groupby(\"symbol\", group_keys=False)\n",
    "    .apply(lambda g: compute_log_change(g))\n",
    ")\n",
    "\n",
    "balance_log_change = (\n",
    "    balance_sorted.groupby(\"symbol\", group_keys=False)\n",
    "    .apply(lambda g: compute_log_change(g))\n",
    ")\n",
    "\n",
    "cashflow_log_change = (\n",
    "    cashflow_sorted.groupby(\"symbol\", group_keys=False)\n",
    "    .apply(lambda g: compute_log_change(g))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f17b0-b137-4386-b95c-58b08ed4a839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_zeros_nans_logchg(df):\n",
    "    # Keep only numeric columns that end with \"_logchg\"\n",
    "    numeric_cols = [c for c in df.select_dtypes(include=[float, int]).columns if c.endswith(\"_logchg\")]\n",
    "    \n",
    "    # Count zeros and NaNs\n",
    "    zero_counts = (df[numeric_cols] == 0).sum()\n",
    "    nan_counts = df[numeric_cols].isna().sum()\n",
    "    \n",
    "    # Combine into a single DataFrame\n",
    "    summary = pd.DataFrame({\n",
    "        \"zeros\": zero_counts,\n",
    "        \"nans\": nan_counts\n",
    "    }).sort_values(by=[\"zeros\", \"nans\"], ascending=True)\n",
    "    \n",
    "    # Force full display\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(summary)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "print(\"Income 0/NaN counts per YoY column:\")\n",
    "income_summary = count_zeros_nans_logchg(income_log_change)\n",
    "\n",
    "print(\"\\nBalance 0/NaN counts per YoY column:\")\n",
    "balance_summary = count_zeros_nans_logchg(balance_log_change)\n",
    "\n",
    "print(\"\\nCashflow 0/NaN counts per YoY column:\")\n",
    "cashflow_summary = count_zeros_nans_logchg(cashflow_log_change)\n",
    "\n",
    "# the purpose here is to make it easy to identify which line items (columns) are fully filled out from our sample so that we are only grabbing columns (features)\n",
    "# that are likely to be filled out by the stock under consideration, cause ultimately after we find a regression that has explanatory power... we can still only apply it \n",
    "# to the stock under consideration if it has the same line items filled out \n",
    "# the 503 nans is a result of the .pct change method that we used which creates a nan on every other row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb7dc9-acca-4d01-a334-c4898a4a747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_symbol_date(df_dict):\n",
    "    \"\"\"\n",
    "    Adds a 'symbol_date' column to each DataFrame in df_dict.\n",
    "    Prints the columns and shape of each updated DataFrame.\n",
    "    \n",
    "    Parameters:-->\n",
    "    df_dict : dict\n",
    "        Dictionary of DataFrames keyed by variable names (strings)\n",
    "    \n",
    "    Returns:\n",
    "    None (updates DataFrames in place)\n",
    "    \"\"\"\n",
    "    for name, df in df_dict.items():\n",
    "        df[\"symbol_date\"] = list(zip(df[\"symbol\"], df[\"date\"]))\n",
    "        print(f\"{name}: columns =  shape = {df.shape}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "dfs = {\n",
    "    \"income_post_nans\": income_log_change,\n",
    "    \"balance_post_nans\": balance_log_change,\n",
    "    \"cashflow_post_nans\": cashflow_log_change,\n",
    "    \"pe_data_sorted\": pe_data_sorted\n",
    "}\n",
    "\n",
    "add_symbol_date(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b96a7-2d78-40a1-a8d8-5de193100403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_pairs = (\n",
    "    set(income_log_change[\"symbol_date\"])\n",
    "    & set(balance_log_change[\"symbol_date\"])\n",
    "    & set(cashflow_log_change[\"symbol_date\"])\n",
    "    & set(pe_data_sorted[\"symbol_date\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edabd0-44ff-44c2-a835-1a878a87bb28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_by_common_pairs(df, common_pairs):\n",
    "    \"\"\"\n",
    "    Keep only rows where the 'symbol_date' is in common_pairs.\n",
    "    Returns a new DataFrame.\n",
    "    \"\"\"\n",
    "    return df[df[\"symbol_date\"].isin(common_pairs)].copy()\n",
    "\n",
    "income_post_nans_overlapped = filter_by_common_pairs(income_log_change, common_pairs)\n",
    "balance_post_nans_overlapped = filter_by_common_pairs(balance_log_change, common_pairs)\n",
    "cashflow_post_nans_overlapped = filter_by_common_pairs(cashflow_log_change, common_pairs)\n",
    "pe_post_nans_overlapped = filter_by_common_pairs(pe_data_sorted, common_pairs)\n",
    "\n",
    "print(income_post_nans_overlapped.shape)\n",
    "print(balance_post_nans_overlapped.shape)\n",
    "print(cashflow_post_nans_overlapped.shape)\n",
    "print(pe_post_nans_overlapped.shape)\n",
    "\n",
    "# we only want to include a ticker if we have all the financial statement items for all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ff5cc-0343-463b-a89d-32c3f8b744a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Sort consistently\n",
    "income_post_nans_overlapped = income_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "balance_post_nans_overlapped = balance_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "cashflow_post_nans_overlapped = cashflow_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "pe_post_nans_overlapped = pe_post_nans_overlapped.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Cleaned and aligned shapes:\", \n",
    "    income_post_nans_overlapped.shape,   \n",
    "    balance_post_nans_overlapped.shape, \n",
    "    cashflow_post_nans_overlapped.shape,\n",
    "    pe_post_nans_overlapped.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175a1b1-dae1-4a4b-9378-700529b7646c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_alignment(dfs: dict):\n",
    "    \"\"\"\n",
    "    Check alignment of multiple DataFrames on 'symbol' and 'date'.\n",
    "    Expects each DataFrame to have the same row order and columns: 'symbol', 'date'.\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    None (prints summary of mismatches)\n",
    "    \"\"\"\n",
    "    # Ensure equal lengths\n",
    "    lengths = {name: len(df) for name, df in dfs.items()}\n",
    "    if len(set(lengths.values())) > 1:\n",
    "        print(\"âš ï¸ Row counts differ between DataFrames:\")\n",
    "        for name, length in lengths.items():\n",
    "            print(f\"  {name}: {length} rows\")\n",
    "    else:\n",
    "        print(f\"âœ… All DataFrames have {list(lengths.values())[0]} rows\")\n",
    "\n",
    "    # Concatenate for comparison\n",
    "    merged = pd.concat(\n",
    "        {name: df[[\"symbol\", \"date\"]].reset_index(drop=True) for name, df in dfs.items()},\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Compare across DataFrames\n",
    "    base = list(dfs.keys())[0]  # pick first as reference\n",
    "    symbol_mismatches = 0\n",
    "    date_mismatches = 0\n",
    "\n",
    "    for i in range(len(dfs[base])):\n",
    "        symbols = [merged[(name, \"symbol\")][i] for name in dfs.keys()]\n",
    "        dates   = [merged[(name, \"date\")][i]   for name in dfs.keys()]\n",
    "        if len(set(symbols)) > 1:\n",
    "            symbol_mismatches += 1\n",
    "        if len(set(dates)) > 1:\n",
    "            date_mismatches += 1\n",
    "\n",
    "    print(f\"Symbol mismatches: {symbol_mismatches}\")\n",
    "    print(f\"Date mismatches:   {date_mismatches}\")\n",
    "\n",
    "    \n",
    "check_alignment({\n",
    "    \"income\": income_post_nans_overlapped,\n",
    "    \"balance\": balance_post_nans_overlapped,\n",
    "    \"cashflow\": cashflow_post_nans_overlapped,\n",
    "    \"pe\": pe_post_nans_overlapped\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2766c-a364-4fce-8953-7b5dba753584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def outlier_check_1(df, title):\n",
    "    # Flatten all numeric columns into one long vector\n",
    "    numeric_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns\n",
    "    values = df[numeric_cols].values.flatten()\n",
    "    values = values[~np.isnan(values)]  # drop NaNs\n",
    "\n",
    "    # Scatter vs. index, colored by density\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(values, fill=True, color=\"lightblue\", alpha=0.3, linewidth=0)  # background density\n",
    "    plt.scatter(range(len(values)), values, \n",
    "                c=values, cmap=\"viridis\", s=5, alpha=0.6)\n",
    "\n",
    "    plt.title(f\"Density Scatterplot: {title}\", fontsize=14)\n",
    "    plt.xlabel(\"Index\", fontsize=12)\n",
    "    plt.ylabel(\"Value\", fontsize=12)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Call for each of your cleaned DataFrames\n",
    "outlier_check_1(income_post_nans_overlapped, \"Income\")\n",
    "outlier_check_1(balance_post_nans_overlapped, \"Balance\")\n",
    "outlier_check_1(cashflow_post_nans_overlapped, \"Cashflow\")\n",
    "\n",
    "#demonstrates the necessity for addressing outlier concerns \n",
    "# ONLY PURPOSE OF. THIS IS TO CONFIRM WE HAVE SERIOUS OUTLIERS THAT NEED TO BE ADDRESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f80e4-afca-467c-81f9-cbbd35602332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_high_zero_columns(df, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Drops columns with more than `threshold` proportion of zero values,\n",
    "    and reports how many columns were dropped.\n",
    "    \"\"\"\n",
    "    original_col_count = df.shape[1]\n",
    "\n",
    "    zero_proportion = (df == 0).sum() / len(df)\n",
    "    cols_to_drop = zero_proportion[zero_proportion > threshold].index\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    new_col_count = df.shape[1]\n",
    "\n",
    "    print(f\"Columns before: {original_col_count}\")\n",
    "    print(f\"Columns dropped: {len(cols_to_drop)} (>{threshold*100:.1f}% zeros)\")\n",
    "    print(f\"Columns after: {new_col_count}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81348b74-38eb-4ab3-b371-afa3c03389ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_after_feature_drop = drop_high_zero_columns(income_log_change, threshold=0.05)\n",
    "balance_after_feature_drop = drop_high_zero_columns(balance_log_change, threshold=0.05)\n",
    "cashflow_after_feature_drop = drop_high_zero_columns(cashflow_log_change, threshold=0.05)\n",
    "\n",
    "# Call for each of your cleaned DataFrames\n",
    "outlier_check_1(income_after_feature_drop, \"Income\")\n",
    "outlier_check_1(balance_after_feature_drop, \"Balance\")\n",
    "outlier_check_1(cashflow_after_feature_drop, \"Cashflow\")\n",
    "\n",
    "print(income_after_feature_drop.shape)\n",
    "print(balance_after_feature_drop.shape)\n",
    "print(cashflow_after_feature_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77731b1-1937-4252-9909-9bd8bbca7204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_outlier_rows(df, threshold=10):\n",
    "    \"\"\"\n",
    "    Drops rows where ANY numeric column has a value greater than `threshold`\n",
    "    or less than `-threshold`. Prints how many rows remain afterward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to filter.\n",
    "    threshold : float\n",
    "        Absolute cutoff for detecting outliers (e.g., 10 retains rows within [-10, 10]).\n",
    "    \"\"\"\n",
    "    original_row_count = df.shape[0]\n",
    "\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "    # Create mask: keep rows where all numeric values are within [-threshold, threshold]\n",
    "    mask = (df[numeric_cols].abs() <= threshold).all(axis=1)\n",
    "    df_filtered = df[mask].copy()\n",
    "\n",
    "    new_row_count = df_filtered.shape[0]\n",
    "    dropped_rows = original_row_count - new_row_count\n",
    "\n",
    "    print(f\"Rows before: {original_row_count}\")\n",
    "    print(f\"Rows dropped: {dropped_rows} (>|{threshold}| in any numeric column)\")\n",
    "    print(f\"Rows after: {new_row_count}\")\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176fcbf2-ec9d-4806-9c20-9aef77af5e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Drop rows with extreme outliers (values beyond Â±10) ---\n",
    "income_after_outlier_drop   = drop_outlier_rows(income_after_feature_drop, threshold=10)\n",
    "balance_after_outlier_drop  = drop_outlier_rows(balance_after_feature_drop, threshold=10)\n",
    "cashflow_after_outlier_drop = drop_outlier_rows(cashflow_after_feature_drop, threshold=10)\n",
    "\n",
    "outlier_check_1(income_after_outlier_drop, \"Income\")\n",
    "outlier_check_1(balance_after_outlier_drop, \"Balance\")\n",
    "outlier_check_1(cashflow_after_outlier_drop, \"Cashflow\")\n",
    "\n",
    "\n",
    "print(income_after_outlier_drop.shape)\n",
    "print(balance_after_outlier_drop.shape)\n",
    "print(cashflow_after_outlier_drop.shape)\n",
    "pe_data_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9800a-6272-4ce7-a6c3-dda6f1fa9c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### YOU MUST REALIGN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204d1d8-430d-43bf-b2f3-8ecf091021aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def align_dfs_on_symbol_date(\n",
    "    dfs,\n",
    "    symbol_col=\"symbol\",\n",
    "    date_col=\"date\",\n",
    "    key_col=\"symbol_date\",\n",
    "    drop_duplicates=True,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Align multiple DataFrames on the intersection of (symbol, date) pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : dict or list\n",
    "        If dict: mapping name -> DataFrame. \n",
    "        If list: list of DataFrames (names will be auto-generated).\n",
    "    symbol_col : str\n",
    "        Column name holding the ticker/symbol.\n",
    "    date_col : str\n",
    "        Column name holding the date.\n",
    "    key_col : str\n",
    "        Name to use for the composite key column.\n",
    "    drop_duplicates : bool\n",
    "        If True, drop duplicate key values in each DataFrame (keep first).\n",
    "    verbose : bool\n",
    "        Print progress and shapes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aligned : dict\n",
    "        Mapping name -> aligned DataFrame (index set to key_col, but also keeps it as a column).\n",
    "    common_keys : list\n",
    "        Sorted list of common key strings present in every DataFrame.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError if no common keys are found or if required columns are missing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize input to a dict\n",
    "    if isinstance(dfs, dict):\n",
    "        df_map = dict(dfs)\n",
    "    else:\n",
    "        df_map = {f\"df_{i}\": df for i, df in enumerate(dfs, 1)}\n",
    "\n",
    "    # --- 1. Ensure required columns exist and build standardized key strings ---\n",
    "    for name, df in df_map.items():\n",
    "        if symbol_col not in df.columns or date_col not in df.columns:\n",
    "            raise ValueError(f\"{name} is missing required columns '{symbol_col}' or '{date_col}'\")\n",
    "\n",
    "        df = df.copy()\n",
    "        df[key_col] = df[symbol_col].astype(str) + \"_\" + df[date_col].astype(str)\n",
    "\n",
    "        if drop_duplicates:\n",
    "            dup_count = df[key_col].duplicated().sum()\n",
    "            if dup_count and verbose:\n",
    "                print(f\"{name}: dropping {dup_count} duplicate {key_col} rows (keeping first)\")\n",
    "            df = df[~df[key_col].duplicated(keep=\"first\")]\n",
    "\n",
    "        df_map[name] = df\n",
    "\n",
    "    # --- 2. Compute intersection of keys ---\n",
    "    key_sets = [set(df[key_col]) for df in df_map.values()]\n",
    "    common = set.intersection(*key_sets) if key_sets else set()\n",
    "\n",
    "    if not common:\n",
    "        raise ValueError(\"No common symbol_date keys found across all DataFrames.\")\n",
    "\n",
    "    common_sorted = sorted(common)  # stable order\n",
    "\n",
    "    # --- 3. Filter each df to the common keys and align them ---\n",
    "    aligned = {}\n",
    "    if verbose:\n",
    "        print(f\"\\nCommon observation count: {len(common_sorted)}\")\n",
    "\n",
    "    for name, df in df_map.items():\n",
    "        before = df.shape[0]\n",
    "        df_f = df[df[key_col].isin(common)].copy()\n",
    "        df_f = df_f.set_index(key_col).loc[common_sorted].reset_index()  # <-- keeps symbol_date as column\n",
    "        after = df_f.shape[0]\n",
    "        aligned[name] = df_f\n",
    "        if verbose:\n",
    "            print(f\"  {name}: before={before}, after={after}\")\n",
    "\n",
    "    # --- 4. Sanity check: all have same ordering ---\n",
    "    first_index = aligned[list(aligned.keys())[0]][\"symbol_date\"].tolist()\n",
    "    for name, df in aligned.items():\n",
    "        assert df[\"symbol_date\"].tolist() == first_index, f\"{name} misaligned!\"\n",
    "\n",
    "    return aligned, common_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7c0f5-c2ea-4ff1-b27b-25bc0cb94570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    \"income\": income_after_outlier_drop,\n",
    "    \"balance\": balance_after_outlier_drop,\n",
    "    \"cashflow\": cashflow_after_outlier_drop,\n",
    "    \"pe\": pe_data_sorted\n",
    "}\n",
    "\n",
    "aligned_dict, common_keys = align_dfs_on_symbol_date(dfs,\n",
    "                                                     symbol_col=\"symbol\",\n",
    "                                                     date_col=\"date\",\n",
    "                                                     key_col=\"symbol_date\",\n",
    "                                                     drop_duplicates=True,\n",
    "                                                     verbose=True)\n",
    "\n",
    "# Access aligned frames:\n",
    "income_aligned = aligned_dict[\"income\"]\n",
    "balance_aligned = aligned_dict[\"balance\"]\n",
    "cashflow_aligned = aligned_dict[\"cashflow\"]\n",
    "pe_aligned = aligned_dict[\"pe\"]\n",
    "\n",
    "# Confirm shapes are identical:\n",
    "print(income_aligned.shape, balance_aligned.shape, cashflow_aligned.shape, pe_aligned.shape)\n",
    "income_aligned.columns.tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7067c-0a6b-4032-a6b1-249ac6e49ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "def run_statement_univariate(statement_df, pe_data_sorted, label=\"\", plot=False):\n",
    "    # Merge on all three identifier columns to preserve them\n",
    "    merged = statement_df.merge(\n",
    "        pe_data_sorted, \n",
    "        on=[\"symbol\", \"date\", \"symbol_date\"], \n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    X = merged.select_dtypes(include=[\"number\"]).drop(columns=[\"log_PE\"])\n",
    "    y = merged[\"log_PE\"]\n",
    "\n",
    "    results = []\n",
    "    for col in X.columns:\n",
    "        X_var = sm.add_constant(X[col])\n",
    "        model = sm.OLS(y, X_var, missing=\"drop\").fit()\n",
    "        residuals = model.resid\n",
    "        fitted = model.fittedvalues\n",
    "\n",
    "        bp_test = het_breuschpagan(residuals, X_var)\n",
    "        bp_pvalue = bp_test[1]\n",
    "        shapiro_pvalue = shapiro(residuals)[1] if len(residuals) < 5000 else None\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": col,\n",
    "            \"coef\": model.params[col],\n",
    "            \"t_value\": model.tvalues[col],\n",
    "            \"p_value\": model.pvalues[col],\n",
    "            \"bp_pvalue\": bp_pvalue,\n",
    "            \"r2\": model.rsquared,\n",
    "            \"shapiro_pvalue\": shapiro_pvalue,\n",
    "            \"reject_null\": model.pvalues[col] < 0.05\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"p_value\")\n",
    "    return merged, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69492bc9-8225-473d-8ef3-59497cb5e81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run diagnostics separately for each statement\n",
    "merged_income, uni_results_income = run_statement_univariate(income_aligned, pe_aligned, label=\"Income\", plot=False)\n",
    "merged_balance, uni_results_balance = run_statement_univariate(balance_aligned, pe_aligned, label=\"Balance\", plot=False)\n",
    "merged_cashflow, uni_results_cashflow = run_statement_univariate(cashflow_aligned, pe_aligned, label=\"Cashflow\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8519f-7fe1-4d9f-b5e2-151c42225b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_income.columns.tolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ed44b-d279-45a3-81a9-a4c83b3a37a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def report_problematic_features(results_dict, bp_thresh=0.05, shapiro_thresh=0.05):\n",
    "    \"\"\"\n",
    "    Filters and prints problematic features from univariate test results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_dict : dict\n",
    "        Dictionary mapping statement name -> DataFrame with 'feature', 'bp_pvalue', 'shapiro_pvalue'.\n",
    "    bp_thresh : float\n",
    "        Threshold for Breuschâ€“Pagan p-value.\n",
    "    shapiro_thresh : float\n",
    "        Threshold for Shapiroâ€“Wilk p-value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filtered_dict : dict\n",
    "        Dictionary mapping statement name -> filtered DataFrame of problematic features.\n",
    "    \"\"\"\n",
    "    filtered_dict = {}\n",
    "\n",
    "    for name, df in results_dict.items():\n",
    "        bad = df[\n",
    "            (df[\"bp_pvalue\"] < bp_thresh) | (df[\"shapiro_pvalue\"] < shapiro_thresh)\n",
    "        ]\n",
    "        filtered_dict[name] = bad\n",
    "\n",
    "        print(f\"\\nðŸš¨ Problematic {name.capitalize()} Features:\")\n",
    "        print(bad[[\"feature\", \"bp_pvalue\", \"shapiro_pvalue\"]])\n",
    "\n",
    "    return filtered_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160ae5a-696c-483b-8082-b4088e56aea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    \"income\": uni_results_income,\n",
    "    \"balance\": uni_results_balance,\n",
    "    \"cashflow\": uni_results_cashflow\n",
    "}\n",
    "\n",
    "problematic_features = report_problematic_features(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e811299-ba72-48c4-9b50-fc90c8d29308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine for convenience, but weâ€™ll split by statement type\n",
    "plot_df = pd.concat([\n",
    "    uni_results_income.assign(statement_type='Income'),\n",
    "    uni_results_balance.assign(statement_type='Balance'),\n",
    "    uni_results_cashflow.assign(statement_type='Cashflow')\n",
    "], ignore_index=True)\n",
    "\n",
    "statement_order = ['Income', 'Balance', 'Cashflow']\n",
    "n_statements = len(statement_order)\n",
    "\n",
    "# Create subplots (one per statement type)\n",
    "fig, axes = plt.subplots(n_statements, 1, figsize=(12, 5*n_statements), sharex=True)\n",
    "\n",
    "for i, statement in enumerate(statement_order):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Subset for this statement\n",
    "    df = plot_df[plot_df['statement_type'] == statement].copy()\n",
    "    \n",
    "    # Sort by absolute t-value descending\n",
    "    df = df.sort_values('t_value', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Normalize rÂ² for color mapping\n",
    "    norm = mpl.colors.Normalize(vmin=df['r2'].min(), vmax=df['r2'].max())\n",
    "    cmap = mpl.cm.viridis\n",
    "    colors = [cmap(norm(val)) for val in df['r2']]\n",
    "    \n",
    "    # Y positions\n",
    "    y_pos = range(len(df))\n",
    "    \n",
    "    # Horizontal bar chart\n",
    "    ax.barh(y=y_pos, width=df['t_value'], color=colors, edgecolor='black')\n",
    "    \n",
    "    # Highlight significant features\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if row.reject_null:\n",
    "            ax.text(\n",
    "                x=row.t_value + (0.05 if row.t_value>0 else -0.05),\n",
    "                y=idx,\n",
    "                s='*',\n",
    "                va='center',\n",
    "                ha='left' if row.t_value>0 else 'right',\n",
    "                color='red',\n",
    "                fontsize=12\n",
    "            )\n",
    "    \n",
    "    # Y-axis labels\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(df['feature'], fontsize=10)\n",
    "    ax.axvline(0, color='black', linewidth=0.8)\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_title(f'{statement} Statement: t-values by Feature (Color = RÂ²)')\n",
    "\n",
    "# Shared x-label\n",
    "axes[-1].set_xlabel('t-value')\n",
    "\n",
    "# Add a single colorbar\n",
    "scalar_map = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "scalar_map.set_array([])\n",
    "cbar = fig.colorbar(scalar_map, ax=axes, orientation='vertical', fraction=0.02, pad=0.01)\n",
    "cbar.set_label('RÂ²')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad612d8e-3a7f-4d6d-be69-32f8e60502cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_significant_features(statement_df, uni_results, \n",
    "                                t_threshold=1.2, r2_threshold=0.005, p_threshold=0.2, \n",
    "                                label=\"\"):\n",
    "    \"\"\"\n",
    "    Select significant features from a financial statement dataframe\n",
    "    based on univariate regression results. \n",
    "    Returns only the subset of statement_df with those features.\n",
    "    \"\"\"\n",
    "    # Filter by thresholds\n",
    "    signif = uni_results[\n",
    "        (uni_results[\"t_value\"].abs() > t_threshold) &\n",
    "        (uni_results[\"r2\"] > r2_threshold) &\n",
    "        (uni_results[\"p_value\"] < p_threshold)\n",
    "    ]\n",
    "\n",
    "    features = signif[\"feature\"].tolist()\n",
    "    \n",
    "    # Only use statement data â€” no merge\n",
    "    selected = statement_df[[\"symbol\", \"date\", \"symbol_date\"] + features]\n",
    "\n",
    "    if label:\n",
    "        total_features = len(uni_results)\n",
    "        n_selected = len(features)\n",
    "        pct = (n_selected / total_features * 100) if total_features > 0 else 0\n",
    "        print(f\"{label.title()}: {n_selected}/{total_features} features selected ({pct:.1f}%)\")\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55240f42-237c-4b74-bdfb-41c04c2dfe1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(income_after_outlier_drop.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23205b77-9b25-4679-9adf-1d6fbcd4d533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uni_results_income = uni_results_income[uni_results_income['feature'] != 'eps']\n",
    "uni_results_balance = uni_results_balance[uni_results_balance['feature'] != 'eps']\n",
    "uni_results_cashflow = uni_results_cashflow[uni_results_cashflow['feature'] != 'eps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d124a-c17a-4e2e-b525-dab02ed1b1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(income_after_outlier_drop.shape)\n",
    "print(balance_after_outlier_drop.shape)\n",
    "print(cashflow_after_outlier_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbab16b-dc57-4faf-9aa3-4d5fa629abbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select significant features from each statement type\n",
    "selected_income = select_significant_features(\n",
    "    merged_income, uni_results_income, label=\"Income\"\n",
    ")\n",
    "\n",
    "selected_balance = select_significant_features(\n",
    "    merged_balance, uni_results_balance, label=\"Balance\"\n",
    ")\n",
    "\n",
    "selected_cashflow = select_significant_features(\n",
    "    merged_cashflow, uni_results_cashflow, label=\"Cashflow\"\n",
    ")\n",
    "\n",
    "print(selected_income.shape)\n",
    "print(selected_balance.shape)\n",
    "print(selected_cashflow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126713a-a471-4f88-8158-c529c9a08c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_selected_features(selected_dfs, labels=None, add_prefix=True):\n",
    "    \"\"\"\n",
    "    Combine multiple selected feature DataFrames (e.g., income, balance, cashflow)\n",
    "    into one aligned DataFrame suitable for PCA.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selected_dfs : list of pd.DataFrame\n",
    "        List of selected DataFrames (each with ['symbol', 'date', 'symbol_date']).\n",
    "    labels : list of str or None\n",
    "        Optional list of labels corresponding to each DataFrame \n",
    "        (e.g., ['income', 'balance', 'cashflow']).\n",
    "    add_prefix : bool\n",
    "        Whether to prefix column names with their label for clarity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined_df : pd.DataFrame\n",
    "        Merged DataFrame with identifiers and all features aligned.\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = [f\"df{i+1}\" for i in range(len(selected_dfs))]\n",
    "\n",
    "    dfs_prefixed = []\n",
    "    for df, label in zip(selected_dfs, labels):\n",
    "        id_cols = [\"symbol\", \"date\", \"symbol_date\"]\n",
    "        feature_cols = [c for c in df.columns if c not in id_cols]\n",
    "        df_copy = df.copy()\n",
    "        if add_prefix:\n",
    "            df_copy = df_copy.rename(columns={c: f\"{label}_{c}\" for c in feature_cols})\n",
    "        dfs_prefixed.append(df_copy)\n",
    "\n",
    "    # Merge all on common keys\n",
    "    combined_df = dfs_prefixed[0]\n",
    "    for df in dfs_prefixed[1:]:\n",
    "        combined_df = combined_df.merge(df, on=[\"symbol\", \"date\", \"symbol_date\"], how=\"inner\")\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793d1a4-f9e0-4528-9231-b25b9f3d15e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_selected_line_items = combine_selected_features(\n",
    "    [selected_income, selected_balance, selected_cashflow],\n",
    "    labels=[\"income\", \"balance\", \"cashflow\"],\n",
    "    add_prefix=True\n",
    ")\n",
    "\n",
    "print(combined_selected_line_items.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a200b7-5e79-42bc-b715-a57e94d502ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pca(df, n_components=None, prefix=\"PC\"):\n",
    "    \"\"\"\n",
    "    Run PCA on the numeric feature columns of a dataframe.\n",
    "    Returns a separate dataframe containing only the PCA components,\n",
    "    plus 'symbol' and 'date' as identifiers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with ['symbol', 'date', 'symbol_date'] and features.\n",
    "    n_components : int or None\n",
    "        Number of PCA components. If None, use all numeric features.\n",
    "    prefix : str\n",
    "        Prefix for PCA component columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pca_components_df : pd.DataFrame\n",
    "        Dataframe with symbol, date, and PCA components.\n",
    "    pca_model : PCA object\n",
    "        Fitted PCA object.\n",
    "    \"\"\"\n",
    "    id_cols = [\"symbol\", \"date\", \"symbol_date\"]\n",
    "    feature_cols = [c for c in df.columns if c not in id_cols]\n",
    "\n",
    "    X = df[feature_cols].fillna(0)\n",
    "\n",
    "    if n_components is None:\n",
    "        n_components = min(len(feature_cols), len(df))\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = pca.fit_transform(X)\n",
    "\n",
    "    comp_cols = [f\"{prefix}{i+1}\" for i in range(n_components)]\n",
    "    pca_df = pd.DataFrame(components, columns=comp_cols, index=df.index)\n",
    "\n",
    "    # Return only symbol/date/symbol_date and components\n",
    "    pca_components_df = pd.concat([df[id_cols], pca_df], axis=1)\n",
    "\n",
    "    return pca_components_df, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cd8a1-2bd1-49bb-8b93-6adbb0eb35fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Income statement PCA\n",
    "income_df_pca, income_pca_model = run_pca(\n",
    "    selected_income,        \n",
    "    n_components=3,\n",
    "    prefix=\"income_\"\n",
    ")\n",
    "\n",
    "# Balance sheet PCA\n",
    "balance_df_pca, balance_pca_model = run_pca(\n",
    "    selected_balance,\n",
    "    n_components=3,\n",
    "    prefix=\"balance_\"\n",
    ")\n",
    "\n",
    "# Cash flow PCA\n",
    "cashflow_df_pca, cashflow_pca_model = run_pca(\n",
    "    selected_cashflow,\n",
    "    n_components=3,\n",
    "    prefix=\"cashflow_\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d40728a-d092-45be-ad7f-1c3b751ef151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_pca_dfs(income_df_pca, balance_df_pca, cashflow_df_pca):\n",
    "\n",
    "    # Identify ID columns and feature columns\n",
    "    id_cols = [\"symbol\", \"date\", \"symbol_date\"]\n",
    "\n",
    "    # Keep IDs from the first dataframe (should be identical across all)\n",
    "    combined_pca_df = income_df_pca[id_cols].copy()\n",
    "\n",
    "    # Concatenate numeric PCA features from all three\n",
    "    combined_features = pd.concat([\n",
    "        income_df_pca.drop(columns=id_cols),\n",
    "        balance_df_pca.drop(columns=id_cols),\n",
    "        cashflow_df_pca.drop(columns=id_cols)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Combine identifiers with features\n",
    "    combined_pca_df = pd.concat([combined_pca_df, combined_features], axis=1)\n",
    "\n",
    "    return combined_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab202be5-e723-4eb3-90a1-62f8899be357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_pca_df = combine_pca_dfs(\n",
    "    income_df_pca,\n",
    "    balance_df_pca,\n",
    "    cashflow_df_pca\n",
    ")\n",
    "\n",
    "print(combined_pca_df.shape)\n",
    "print(combined_pca_df.columns[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea98255-789f-4fef-96de-37028d6b8336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(income_df_pca.columns)\n",
    "print(balance_df_pca.columns)\n",
    "print(cashflow_df_pca.columns)\n",
    "\n",
    "print(income_df_pca.shape)\n",
    "print(balance_df_pca.shape)\n",
    "print(cashflow_df_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547c703-b776-4831-a636-761a5d8c9605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regress_log_pe_on_pca(combined_df, pe_data_sorted, log_pe_col=\"log_PE\"):\n",
    "    \"\"\"\n",
    "    Run a multivariable linear regression using PCA components (already combined in `combined_df`)\n",
    "    as independent variables, and the precomputed log_PE from `pe_data_sorted` as the dependent variable.\n",
    "\n",
    "    Assumes both dataframes are already aligned on index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    combined_df : pd.DataFrame\n",
    "        DataFrame containing all PCA components (independent variables).\n",
    "    pe_data_sorted : pd.DataFrame\n",
    "        DataFrame containing the log_PE column.\n",
    "    log_pe_col : str, default \"log_PE\"\n",
    "        Column name for the logged PE ratio in `pe_data_sorted`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "        Fitted OLS model.\n",
    "    reg_df : pd.DataFrame\n",
    "        DataFrame containing all features and the dependent variable (used in regression).\n",
    "    \"\"\"\n",
    "\n",
    "    # Build regression dataframe\n",
    "    reg_df = combined_df.copy()\n",
    "    reg_df[log_pe_col] = pe_data_sorted[log_pe_col]\n",
    "\n",
    "    # Replace inf/-inf with NaN and drop missing\n",
    "    reg_df = reg_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # Split dependent and independent variables\n",
    "    y = reg_df[log_pe_col]\n",
    "    X = reg_df.drop(columns=[log_pe_col]).select_dtypes(include=[\"number\"])\n",
    "\n",
    "    # Add constant\n",
    "    X_const = sm.add_constant(X)\n",
    "\n",
    "    # Fit regression\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "\n",
    "    print(model.summary())\n",
    "    return model, reg_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde787ad-a5f4-42a3-a196-9b92608bd535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, reg_df = regress_log_pe_on_pca(\n",
    "    combined_pca_df,   \n",
    "    pe_data_sorted,\n",
    "    log_pe_col=\"log_PE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fcdd7-57d2-4bb2-bf2a-f3c0355ae096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, reg_df = regress_log_pe_on_pca(\n",
    "    combined_selected_line_items,   # your already concatenated PCA dataframe\n",
    "    pe_data_sorted,\n",
    "    log_pe_col=\"log_PE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e520b-c400-4ae3-b049-e86832c6657e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb41f4-7b10-4e84-8448-59994d8faf45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
