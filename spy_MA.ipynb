{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea626d07-3875-4253-87da-add434ac9253",
   "metadata": {},
   "source": [
    "# <a id=\"table-of-contents\"></a>Table of Contents\n",
    "\n",
    "1. [Intraday Mean Reversion](#section1)\n",
    "2. [Moving Average](#section2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ee9c6-fd09-4569-a7fa-313c5b20494e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay,  roc_curve, roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from arch import arch_model\n",
    "from statsmodels.stats.stattools import durbin_watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30bab6e-c803-4fd8-a833-27325f710e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the Downloads folder and the file\n",
    "downloads_folder = os.path.expanduser(\"~/Desktop\")\n",
    "file_name = \"aaHistoricalData_1726248252859.csv\" \n",
    "file_path = os.path.join(downloads_folder, file_name)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8b98c-fa53-47b5-8885-5e42008222d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Sort the DataFrame by date in ascending order\n",
    "data = data.sort_values(by='Date', ascending=True)\n",
    "\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032afa92-76f2-436b-a8e6-e878e7a2c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your existing DataFrame\n",
    "data['Original_Index'] = data.index  # Add the Original_Index column to capture current index\n",
    "print('''It will record the INDEX to make sure that future data manipulations don't force us to lose the order.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d241e92-c549-4b97-acd3-ae52338e7573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add new columns with the given calculations\n",
    "data['High_minus_Open'] = (data['High'] - data['Open']) / ((data['Open'] + data['Close/Last']) / 2)\n",
    "data['day_change'] = (data['Close/Last'] - data['Close/Last'].shift(1)) / data['Close/Last'].shift(1)\n",
    "\n",
    "data['opening_gaps'] = (data['Open'] - data['Close/Last'].shift(1)) / ((data['Open'] + data['Close/Last']) / 2)\n",
    "\n",
    "#YOU DEFINED OPENNING GAPS AS TODAY'S OPEN TO YESTERDAY'S CLOSE INSTEAD OF TODAY'S OPEN TO YESTERDAY'S HIGH/LOW..\n",
    "\n",
    "\n",
    "data['net_Intraday_Movement_Around_Open_Ratio'] = ((data['High'] - data['Open']) / ((data['Open'] + data['Close/Last']) / 2)) - ((data['Open'] - data['Low']) / ((data['Open'] + data['Close/Last']) / 2))\n",
    "data['Intraday_Volatility'] = ((data['High'] - data['Open']) / ((data['Open'] + data['Close/Last']) / 2)) + ((data['Open'] - data['Low']) / ((data['Open'] + data['Close/Last']) / 2))\n",
    "\n",
    "# Calculate day change adjusted for opening gap\n",
    "data['day_change_adjusted_for_openning_gap'] = data['day_change'] - data['opening_gaps']\n",
    "\n",
    "\n",
    "# Create moving average columns\n",
    "data['MA_5'] = data['Open'].rolling(window=5).mean()\n",
    "data['MA_14'] = data['Open'].rolling(window=14).mean()\n",
    "data['MA_50'] = data['Open'].rolling(window=50).mean()\n",
    "data['MA_200'] = data['Open'].rolling(window=200).mean()\n",
    "\n",
    "\n",
    "# Create lagged features comparing today's open to these moving averages\n",
    "data['open_above_MA_5'] = (data['Open'] > data['MA_5']).astype(int)\n",
    "data['open_above_MA_14'] = (data['Open'] > data['MA_14']).astype(int)\n",
    "data['open_above_MA_50'] = (data['Open'] > data['MA_50']).astype(int)\n",
    "\n",
    "# Create a crossover signal column\n",
    "data['MA_Crossover_Signal_5_14'] = ((data['MA_5'] > data['MA_14']) & (data['MA_5'].shift(1) <= data['MA_5'].shift(1))).astype(int)\n",
    "data['MA_Crossover_Signal_today_5'] = ((data['Close/Last'] > data['MA_14']) & (data['Close/Last'].shift(1) <= data['MA_5'].shift(1))).astype(int)\n",
    "\n",
    "\n",
    "# Create a binary indicator for whether yesterday's close was above or below the 5-day MA\n",
    "data['yesterday_close_above_MA_5'] = (data['Close/Last'].shift(1) > data['MA_14']).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23aaeb-46f7-47d0-9c97-9ed854454be9",
   "metadata": {},
   "source": [
    "## <a id=\"section2\"></a> Moving Averages: Today's Price Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d57516-5a59-42b7-ae51-206bbd0970fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create lagged feature comparing today's open to the 200-day moving average\n",
    "data['open_above_MA_200'] = (data['Open'] > data['MA_200']).astype(int)\n",
    "\n",
    "# Drop rows with NaN values in the columns of interest and explicitly make a copy\n",
    "clean_data = data.dropna(subset=['open_above_MA_5', 'open_above_MA_14', 'open_above_MA_50', 'open_above_MA_200', 'day_change']).copy()\n",
    "\n",
    "# Logistic regression preparation: Convert 'day_change' to binary (0 for Down, 1 for Up)\n",
    "clean_data['day_change'] = clean_data['day_change'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Independent variables (including the new 200-day moving average) and dependent variable (day_change)\n",
    "X = clean_data[['open_above_MA_5', 'open_above_MA_14', 'open_above_MA_50', 'open_above_MA_200']]\n",
    "y = clean_data['day_change']\n",
    "\n",
    "# Add a constant term to the predictor\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "# Print the logistic regression results, including confidence intervals\n",
    "print(result.summary())\n",
    "\n",
    "# Extract confidence intervals and rename columns for clarity\n",
    "conf = result.conf_int()\n",
    "conf.columns = ['Lower Bound for 95% Interval', 'Upper Bound for 95% Interval']\n",
    "\n",
    "# Exponentiate coefficients to get odds ratios\n",
    "conf['Odds Ratio'] = np.exp(result.params)\n",
    "\n",
    "# Calculate the lower and upper bounds for the odds ratios\n",
    "conf['Odds Ratio Lower Bound'] = np.exp(conf['Lower Bound for 95% Interval'])\n",
    "conf['Odds Ratio Upper Bound'] = np.exp(conf['Upper Bound for 95% Interval'])\n",
    "\n",
    "# Display the confidence intervals and odds ratios\n",
    "print(conf)\n",
    "\n",
    "# Calculate probabilities from odds ratios\n",
    "odds_ratios = np.exp(result.params)\n",
    "probabilities = odds_ratios / (1 + odds_ratios)\n",
    "\n",
    "# Calculate the lower and upper bounds for the probabilities\n",
    "prob_lower_bound = conf['Odds Ratio Lower Bound'] / (1 + conf['Odds Ratio Lower Bound'])\n",
    "prob_upper_bound = conf['Odds Ratio Upper Bound'] / (1 + conf['Odds Ratio Upper Bound'])\n",
    "\n",
    "# Display the probabilities and their confidence intervals\n",
    "prob_conf = pd.DataFrame({\n",
    "    'Probability': probabilities,\n",
    "    'Lower Bound for 95% Interval': prob_lower_bound,\n",
    "    'Upper Bound for 95% Interval': prob_upper_bound\n",
    "})\n",
    "\n",
    "print(\"\\nProbabilities with Confidence Intervals:\")\n",
    "print(prob_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d97814-067e-483c-855a-1b3db7f9631e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot ACF and PACF for the dependent variable 'day_change'\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_acf(clean_data['day_change'], lags=30, ax=plt.gca())\n",
    "plt.title('ACF of Day Change')\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_pacf(clean_data['day_change'], lags=30, ax=plt.gca())\n",
    "plt.title('PACF of Day Change')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('''\n",
    "The below charts show that there are no temporal dependencies for the dependent variables with itself.\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a930c58-03ea-463c-8bba-7704475a3c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_prob = result.predict(X)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y, y_pred_prob)\n",
    "auc_score = roc_auc_score(y, y_pred_prob)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y, y_pred_prob)\n",
    "average_precision = average_precision_score(y, y_pred_prob)\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f'Precision-Recall curve (AP = {average_precision:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890891be-f14a-438c-bed9-9026d365c1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new columns for visualization\n",
    "clean_data['combined_label'] = clean_data.apply(\n",
    "    lambda row: f\"5MA: {'Above' if row['open_above_MA_5'] else 'Below'} / \"\n",
    "                f\"14MA: {'Above' if row['open_above_MA_14'] else 'Below'} / \"\n",
    "                f\"50MA: {'Above' if row['open_above_MA_50'] else 'Below'} / \"\n",
    "                f\"200MA: {'Above' if row['open_above_MA_200'] else 'Below'}\", axis=1)\n",
    "clean_data['day_change_label'] = clean_data['day_change'].apply(lambda x: 'Up' if x == 1 else 'Down')\n",
    "\n",
    "# Count the occurrences of each combination of combined_label and 'day_change_label'\n",
    "counts = clean_data.groupby(['combined_label', 'day_change_label']).size().unstack().fillna(0)\n",
    "\n",
    "# Normalize the counts to get proportions\n",
    "proportions = counts.div(counts.sum(axis=1), axis=0)\n",
    "\n",
    "# Calculate the absolute difference between 'Up' and 'Down' proportions for sorting\n",
    "proportions['Difference'] = abs(proportions['Up'] - proportions['Down'])\n",
    "proportions = proportions.sort_values('Difference', ascending=False).drop(columns='Difference')\n",
    "\n",
    "# Create the stacked bar chart\n",
    "ax = proportions.plot(kind='bar', stacked=True, color=['red', 'green'], figsize=(12, 8))\n",
    "\n",
    "# Add percentages on top of the bars\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    ax.annotate(f'{height:.0%}', (x + width / 2, y + height / 2), ha='center', va='center', color='white', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Open Status Relative to Moving Averages')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Proportion of Day Change (Up/Down) vs Open Status Relative to Moving Averages')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title='Day Change')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736b977-0108-4455-ab32-38d2bf2b9ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop over each moving average period from 1 to 50 days\n",
    "for i in range(1, 51):\n",
    "    ma_column = f'open_above_MA_{i}'\n",
    "    data[ma_column] = data['Open'].rolling(window=i).mean()\n",
    "\n",
    "# Calculate the binary variables to check if the opening price is above the moving averages\n",
    "for i in range(1, 51):\n",
    "    ma_column = f'open_above_MA_{i}'\n",
    "    data[ma_column] = (data['Open'] > data[ma_column]).astype(int)\n",
    "\n",
    "# Drop rows with NaN values in the newly created columns\n",
    "columns_of_interest = [f'open_above_MA_{i}' for i in range(1, 51)]\n",
    "data.dropna(subset=columns_of_interest, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb52344-6bed-44fb-af0e-e5e1a1f0acb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the columns of interest and explicitly make a copy\n",
    "clean_data = data.copy()\n",
    "\n",
    "# Logistic regression preparation: Convert 'day_change' to binary (0 for Down, 1 for Up)\n",
    "clean_data['day_change'] = clean_data['day_change'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "clean_data = clean_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "for i in range(2, 51):\n",
    "    ma_column = f'open_above_MA_{i}'\n",
    "    if ma_column in clean_data.columns:\n",
    "        clean_data[ma_column] = clean_data[ma_column].astype(int)\n",
    "\n",
    "# Drop rows with NaN values in the 'day_change' column and moving average columns\n",
    "columns_of_interest = ['day_change'] + [f'open_above_MA_{i}' for i in range(2, 51)]\n",
    "clean_data = clean_data.dropna(subset=columns_of_interest)\n",
    "\n",
    "# Prepare a list to collect results\n",
    "results_list = []\n",
    "\n",
    "# Loop over each moving average from 2 to 50 days\n",
    "for i in range(2, 51):\n",
    "    ma_column = f'open_above_MA_{i}'\n",
    "    if ma_column in clean_data.columns:\n",
    "        X = clean_data[[ma_column]]\n",
    "        y = clean_data['day_change']\n",
    "        \n",
    "        # Add a constant term to the predictor\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        # Fit the logistic regression model\n",
    "        model = sm.Logit(y, X)\n",
    "        result = model.fit(disp=0)  # disp=0 suppresses the output\n",
    "        \n",
    "        # Extract coefficient, p-value, and Durbin-Watson statistic\n",
    "        coef = result.params[ma_column]\n",
    "        p_value = result.pvalues[ma_column]\n",
    "        dw_stat = durbin_watson(result.resid_generalized)\n",
    "        \n",
    "        # Only collect significant results with beta > 0.4\n",
    "        if p_value < 0.05 and coef > 0.4:\n",
    "            odds_ratio = np.exp(coef)\n",
    "            prob = odds_ratio / (1 + odds_ratio)\n",
    "            \n",
    "            # Collect the results\n",
    "            results_list.append({\n",
    "                'Moving Average': i,\n",
    "                'Coefficient': coef,\n",
    "                'P-Value': p_value,\n",
    "                'Odds Ratio': odds_ratio,\n",
    "                'Probability': prob,\n",
    "                'Durbin-Watson': dw_stat\n",
    "            })\n",
    "\n",
    "# Convert results list to a DataFrame\n",
    "results_summary = pd.DataFrame(results_list)\n",
    "\n",
    "# Calculate confidence intervals for odds ratios and probabilities\n",
    "results_summary['Odds Ratio Lower Bound'] = np.exp(results_summary['Coefficient'] - 1.96 * results_summary['Coefficient'].std())\n",
    "results_summary['Odds Ratio Upper Bound'] = np.exp(results_summary['Coefficient'] + 1.96 * results_summary['Coefficient'].std())\n",
    "results_summary['Probability Lower Bound'] = results_summary['Odds Ratio Lower Bound'] / (1 + results_summary['Odds Ratio Lower Bound'])\n",
    "results_summary['Probability Upper Bound'] = results_summary['Odds Ratio Upper Bound'] / (1 + results_summary['Odds Ratio Upper Bound'])\n",
    "\n",
    "# Sort results by coefficient for better readability\n",
    "results_summary_sorted = results_summary.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nSignificant Moving Averages with Coefficients > 0.4:\")\n",
    "print(results_summary_sorted)\n",
    "\n",
    "# Plot stacked bar graph of probabilities with confidence intervals\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(results_summary_sorted['Moving Average'], results_summary_sorted['Probability'], yerr=[results_summary_sorted['Probability'] - results_summary_sorted['Probability Lower Bound'], results_summary_sorted['Probability Upper Bound'] - results_summary_sorted['Probability']], capsize=5, color='skyblue')\n",
    "plt.xlabel('Moving Average Period (Days)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Probability of Next Day\\'s Price Movement Being Positive')\n",
    "plt.xticks(ticks=np.arange(0, 20, 2), labels=np.arange(0, 20, 2))\n",
    "plt.axhline(y=0.5, color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print('''\n",
    "The logistic regression analysis confirms that shorter-term moving averages are significant predictors of the next day's price movement. Next steps would be\n",
    "to determine if there is any information in the later moving averages.. say 14 that is not included or already encapsulated in the 5 day.\n",
    "''')\n",
    "\n",
    "print(''' \n",
    "You're correct that multicollinearity is not a concern when using a single moving average as the predictor. Your approach to isolate and select the most \n",
    "predictive moving average is sound.\n",
    "''')\n",
    "\n",
    "print('''\n",
    "Durbin-Watson confirms that there is no autocorrelation\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadc13d-dbd9-46bd-baa4-9c710de02936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot ACF and PACF for the dependent variable\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_acf(clean_data['day_change'], lags=30, ax=plt.gca())\n",
    "plt.title('ACF of Day Change')\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_pacf(clean_data['day_change'], lags=30, ax=plt.gca())\n",
    "plt.title('PACF of Day Change')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('''\n",
    "the below charts show that there is no temporal dependencies for the dependent vairables with itself\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4730157-f1d8-4170-bcd5-914d9e6dde1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the columns of interest and explicitly make a copy\n",
    "clean_data = data.copy()\n",
    "\n",
    "# Logistic regression preparation: Convert 'day_change' to binary (0 for Down, 1 for Up)\n",
    "clean_data['day_change'] = clean_data['day_change'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "clean_data = clean_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "for i in range(2, 51):\n",
    "    ma_column = f'open_above_MA_{i}'\n",
    "    if ma_column in clean_data.columns:\n",
    "        clean_data[ma_column] = clean_data[ma_column].astype(int)\n",
    "\n",
    "# Drop rows with NaN values in the 'day_change' column and moving average columns\n",
    "columns_of_interest = ['day_change'] + [f'open_above_MA_{i}' for i in range(2, 51)]\n",
    "clean_data = clean_data.dropna(subset=columns_of_interest)\n",
    "\n",
    "# Ensure the dependent variable is binary\n",
    "assert clean_data['day_change'].isin([0, 1]).all(), \"Dependent variable must be binary.\"\n",
    "\n",
    "# Function to fit logistic regression and plot evaluation metrics\n",
    "def evaluate_moving_average(ma_column):\n",
    "    X = clean_data[[ma_column]]\n",
    "    y = clean_data['day_change']\n",
    "    \n",
    "    # Add a constant term to the predictor\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Fit the logistic regression model\n",
    "    model = sm.Logit(y, X)\n",
    "    result = model.fit(disp=0)  # disp=0 suppresses the output\n",
    "    \n",
    "    # Predict probabilities and classes\n",
    "    y_pred_prob = result.predict(X)\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for {ma_column}')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y, y_pred_prob)\n",
    "    auc_score = roc_auc_score(y, y_pred_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f}) - {ma_column}')\n",
    "    plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {ma_column}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y, y_pred_prob)\n",
    "    average_precision = average_precision_score(y, y_pred_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'Precision-Recall curve (AP = {average_precision:.2f}) - {ma_column}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve for {ma_column}')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate for MA_2 and MA_3\n",
    "evaluate_moving_average('open_above_MA_2')\n",
    "evaluate_moving_average('open_above_MA_3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b408d-cf2f-4294-a563-f782e7cdbc3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the columns of interest\n",
    "clean_data = data.dropna(subset=['open_above_MA_5', 'open_above_MA_14', 'open_above_MA_50', 'day_change'])\n",
    "\n",
    "# Independent variables\n",
    "X = clean_data[['open_above_MA_5', 'open_above_MA_14', 'open_above_MA_50']]\n",
    "\n",
    "# Add a constant term to the predictor\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Initialize a DataFrame to hold VIF and R^2 values\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "\n",
    "# Calculate VIF and R^2 for each variable\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif_data[\"R_squared\"] = [\n",
    "    sm.OLS(X.iloc[:, i], X.iloc[:, [j for j in range(X.shape[1]) if j != i]]).fit().rsquared for i in range(X.shape[1])\n",
    "]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2f5bd-d5b5-435d-96ef-dbee30b576bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('''\n",
    "\n",
    "Exactly. The low VIF values indicate that these moving averages are not strongly related to each other in predicting daily changes. This reinforces the \n",
    "idea that the data lacks a significant trend within the 5, 14, and 50-day windows. It suggests a level of randomness, with no clear pattern or momentum \n",
    "over these periods. So, the moving averages are not =trending indicators in this context. What's your next data adventure?\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe8f5b-e9ad-4b42-98df-47ff31f412c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the column to store the sum of moving averages\n",
    "data['Sum_MA_1_50'] = 0\n",
    "\n",
    "# Calculate and sum the moving averages from 1 to 50 days\n",
    "for i in range(1, 51):\n",
    "    # Calculate the moving average for each window size 'i'\n",
    "    ma = data['Close/Last'].rolling(window=i).mean()\n",
    "    # Add the calculated moving average to 'Sum_MA_1_50'\n",
    "    data['Sum_MA_1_50'] += ma\n",
    "\n",
    "# Divide by 50 to get the average\n",
    "data['Sum_MA_1_50_Avg'] = data['Sum_MA_1_50'] / 50\n",
    "\n",
    "# Ensure 'day_change' is defined\n",
    "data['day_change'] = (data['Close/Last'] - data['Close/Last'].shift(1)) / data['Close/Last'].shift(1)\n",
    "\n",
    "# Drop rows with NaN values in the columns of interest\n",
    "clean_data = data.dropna(subset=['Sum_MA_1_50_Avg', 'day_change'])\n",
    "\n",
    "# Independent variable and dependent variable\n",
    "X = clean_data[['Sum_MA_1_50_Avg']]\n",
    "y = clean_data['day_change']\n",
    "\n",
    "# Add a constant term to the predictor\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the OLS results\n",
    "print(model.summary())\n",
    "\n",
    "# Predict values\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(clean_data['Sum_MA_1_50_Avg'], clean_data['day_change'], color='blue', alpha=0.1, label='Data points')\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(clean_data['Sum_MA_1_50_Avg'], predictions, color='black', linewidth=2, label='Regression Line')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sum of Moving Averages (1-50 days) Averaged')\n",
    "plt.ylabel('Day Change')\n",
    "plt.title('Regression of Day Change on Sum of Moving Averages (1-50 days) Averaged')\n",
    "\n",
    "# Set the x-axis limit to fit the range of 'Sum_MA_1_50_Avg'\n",
    "plt.xlim(clean_data['Sum_MA_1_50_Avg'].min(), clean_data['Sum_MA_1_50_Avg'].max())\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show and save the plot\n",
    "plt.show()\n",
    "\n",
    "print('''\n",
    "\n",
    "Exactly. The OLS results show that the average of the MAs isn't predictive for day-to-day price changes. The small coefficient, \n",
    "high p-value, and low R-squared all suggest the sum of moving averages isn't a useful predictor.\n",
    "\n",
    "This finding aligns with your earlier insights about the lack of significance in individual moving averages. So, it seems like moving\n",
    "averages, at least in these configurations, don't offer predictive power for daily changes.\n",
    "\n",
    "''')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
